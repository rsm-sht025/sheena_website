---
title: "Project 3"
editor: visual

---
This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans.


## 1. Estimating Yogurt Preferences

### Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 4 products, then either $y=3$ or $y=(0,0,1,0)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, size, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 4 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta} + e^{x_4'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=\delta_{i4}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 \times \mathbb{P}_i(4)^0 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$


### Yogurt Dataset

We will use the `yogurt_data` dataset, which provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc.

_todo: import the data, maybe show the first few rows, and describe the data a bit._

Let the vector of product features include brand dummy variables for yogurts 1-3 (we'll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts' prices:  

$$ x_j' = [\mathbbm{1}(\text{Yogurt 1}), \mathbbm{1}(\text{Yogurt 2}), \mathbbm{1}(\text{Yogurt 3}), X_f, X_p] $$

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). 

What we would like to do is reorganize the data from a "wide" shape with $n$ rows and multiple columns for each covariate, to a "long" shape with $n \times J$ rows and a single column for each covariate.  As part of this re-organization, we'll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be "pivoted" or "melted" from wide to long.  

The purpose of the code is to read a CSV file containing data about different yogurt products, reshape the data into a long format, and create new columns to indicate specific characteristics of the products, such as whether they are featured or their price.

The input is a CSV file named 'yogurt_data.csv' located in the '/home/jovyan/Desktop/new web 111/sheena_website 2 2/' directory.

The output is a transformed pandas DataFrame named 'yogurt_data_long' containing the original data reshaped into a long format, along with new columns indicating whether each product is a yogurt (y1, y2, y3), whether it is featured (featured), and its price (price).

Here's how the code achieves its purpose:

First, it reads the CSV file into a pandas DataFrame named 'yogurt_data' using pd.read_csv(). The header=None parameter indicates that the first row should not be used as column names, and skiprows=1 skips the first row entirely.

Next, it renames the columns of the 'yogurt_data' DataFrame to more descriptive names using yogurt_data.columns = [...].

Then, it uses pd.melt() to reshape the data into a long format, where each row represents a single product-choice combination. The value_vars parameter specifies which columns to include in the melted DataFrame, var_name and value_name specify the names of the new columns created by melt().

After that, it creates new columns in the 'yogurt_data_long' DataFrame to indicate whether each product is a yogurt (y1, y2, y3) by checking if the 'product' column contains the strings 'y1', 'y2', or 'y3', and converting the result to an integer (0 or 1).

Similarly, it creates a 'featured' column to indicate whether each product is featured by checking if the 'product' column contains the string 'f'.

It also extracts the price information from the 'product' column using str.extract() and converts it to a float to create the 'price' column.

Finally, it drops the 'product' and 'choice' columns from the 'yogurt_data_long' DataFrame, as they are no longer needed.

The key data transformations happening in this code are:

Reading the CSV file into a DataFrame
Renaming the columns
Reshaping the data from wide to long format using melt()
Creating new columns based on string patterns in the 'product' column
Extracting price information from the 'product' column
Dropping unnecessary columns
The code follows a logical flow of reading the data, reshaping it, and creating new columns based on the existing data to prepare it for further analysis or processing.




```{python}

import pandas as pd
import numpy as np

yogurt_data = pd.read_csv('/home/jovyan/Desktop/new web 111/sheena_website 2 2/yogurt_data.csv', header=None, skiprows=1)
print(yogurt_data.head())

yogurt_data.columns = ['c', 'y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4']


yogurt_data_long = pd.melt(yogurt_data, value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'], var_name='product', value_name='choice')

yogurt_data_long['yogurt1'] = yogurt_data_long['product'].str.contains('y1').astype(int)
yogurt_data_long['yogurt2'] = yogurt_data_long['product'].str.contains('y2').astype(int)
yogurt_data_long['yogurt3'] = yogurt_data_long['product'].str.contains('y3').astype(int)

yogurt_data_long['featured'] = yogurt_data_long['product'].str.contains('f').astype(int)
yogurt_data_long['price'] = yogurt_data_long['product'].str.extract('p(\d+)', expand=False).astype(float)

yogurt_data_long = yogurt_data_long.drop(['product', 'choice'], axis=1)

print(yogurt_data_long.head())



```

The purpose of the code is to reshape a wide-format DataFrame, where each column represents a different variable, into a long-format DataFrame, where each row represents a single observation, and the variable names are stored in a separate column.

The input is a wide-format pandas DataFrame called 'df', which has columns for an identifier ('id') and multiple variables ('product1', 'product2', 'product3', 'other_products').

The output is a new long-format DataFrame called 'df_long', which has columns for the identifier ('id'), the variable name ('variable'), and the value ('value'). Additionally, it has three new columns ('is_product1', 'is_product2', 'is_product3') that indicate whether the original variable was 'product1', 'product2', or 'product3', respectively.

The code achieves its purpose through the following steps:
a) It uses the pd.melt() function to reshape the wide-format DataFrame 'df' into a long-format DataFrame 'df_long'. The 'id_vars' parameter specifies the columns to use as identifiers, and the 'value_vars' parameter specifies the columns to "unmelt" or reshape.
b) It creates three new columns ('is_product1', 'is_product2', 'is_product3') in 'df_long' by comparing the 'variable' column with the strings 'product1', 'product2', and 'product3', respectively, using the eq() method.
c) It drops the 'variable' column from 'df_long' since the information is now stored in the new columns.

The main data transformation happening is the reshaping of the DataFrame from wide to long format. This is a common operation in data analysis when working with datasets that have multiple variables stored in separate columns. The long format makes it easier to perform operations on all variables simultaneously or to analyze data across variables.

The code also creates additional columns to store information about the original variable names. This can be useful for later analysis or filtering operations based on the variable type.
```{python}


import pandas as pd

# Define a sample DataFrame
df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'product1': [10, 20, 30, 40, 50],
    'product2': [15, 25, 35, 45, 55],
    'product3': [20, 30, 40, 50, 60],
    'other_products': [25, 35, 45, 55, 65]
})

# Assuming you have a wide-format DataFrame 'df'
df_long = pd.melt(df, id_vars=['id'], value_vars=['product1', 'product2', 'product3', 'other_products'])
df_long = df_long.assign(is_product1=df_long['variable'].eq('product1'),
                         is_product2=df_long['variable'].eq('product2'),
                         is_product3=df_long['variable'].eq('product3'))
df_long = df_long.drop('variable', axis=1)
print(df_long)





```


The code loglik is a function that calculates the negative log-likelihood for a multinomial logit model. The purpose of this function is to evaluate how well the model's parameters fit the given data.

The function takes three inputs:

params: A numpy array containing the model's parameter values (beta_1, beta_2, beta_3, beta_f, beta_p).
data: A pandas DataFrame containing the original data.
feature_matrix: A pandas DataFrame containing the feature matrix.
The output of the function is a single float value representing the negative log-likelihood.

Here's how the function achieves its purpose:

It calculates the utilities (V) using the calculate_utilities function (not shown in the provided code) with the params and feature_matrix as inputs.
It calculates the choice probabilities (P) by taking the exponential of the utilities (V) and dividing each element by the sum of the exponentials.
It finds the probabilities of the chosen alternatives (chosen_probs) by selecting the probabilities corresponding to the maximum values in the 'yogurt1', 'yogurt2', and 'yogurt3' columns of the data DataFrame.
It calculates the log-likelihood by summing the natural logarithms of the chosen probabilities.
Finally, it returns the negative of the log-likelihood value.
The key logic flow in this function is the calculation of choice probabilities using the utilities and the exponential function. This is a common approach in multinomial logit models, where the probabilities are derived from the utilities through a softmax-like transformation. The function also performs data transformations by selecting the chosen probabilities from the data DataFrame and taking the natural logarithm of those probabilities.

Overall, the loglik function is a crucial component in evaluating the fit of a multinomial logit model by calculating the negative log-likelihood, which is a measure of how well the model's parameters explain the observed data.

```{python}
def loglik(params: np.ndarray, data: pd.DataFrame, feature_matrix: pd.DataFrame) -> float:
    """
    Log-likelihood function for the multinomial logit model.

    Args:
        params (numpy.ndarray): The parameter values (beta_1, beta_2, beta_3, beta_f, beta_p).
        data (pandas.DataFrame): The original data frame.
        feature_matrix (pandas.DataFrame): The feature matrix.

    Returns:
        float: The negative log-likelihood value.
    """
    # Calculate the utilities
    V = calculate_utilities(params, feature_matrix)

    # Calculate the choice probabilities
    exp_V = np.exp(V)
    if exp_V.ndim == 1:
        sum_exp_V = exp_V.sum(keepdims=True)
    else:
        sum_exp_V = exp_V.sum(axis=1, keepdims=True)
    P = exp_V / sum_exp_V

    # Calculate the log-likelihood
    chosen_probs = P[data[['yogurt1', 'yogurt2', 'yogurt3']].values.argmax(axis=1)]
    log_likelihood = np.log(chosen_probs).sum()

    return -log_likelihood

```

Purpose: The purpose of this code is to estimate the parameters (coefficients) of a multinomial logit model, which is a type of statistical model used to analyze discrete choice data. In this case, the data seems to be related to consumer preferences for different yogurt products.

Input: The code takes a pandas DataFrame called yogurt_data_long as input. This DataFrame likely contains information about different yogurt products, such as their prices and other attributes, as well as the choices made by consumers.

Output: The code outputs the estimated values of the model parameters (coefficients), which are denoted as beta_1, beta_2, beta_3, beta_f, and beta_p. These parameters represent the effects of different factors (e.g., product type, price, featured status) on the probability of a consumer choosing a particular yogurt product.

How it achieves its purpose:
a) The code starts by preprocessing the data using the preprocess_data function. This function creates dummy variables for the yogurt products and adds a 'price' column and a 'featured' column to the feature matrix.
b) The calculate_utilities function calculates the utilities (a measure of preference or satisfaction) for each yogurt product based on the feature matrix and the model parameters.
c) The loglik function calculates the log-likelihood, which is a measure of how well the model fits the data. This function uses the utilities calculated in the previous step to compute the choice probabilities and then computes the log-likelihood based on the observed choices in the data.
d) The code initializes the model parameters with some starting values and then uses the minimize function from the scipy.optimize module to find the parameter values that maximize the log-likelihood (i.e., provide the best fit to the data).
e) Finally, the code prints out the estimated parameter values (Maximum Likelihood Estimates) for the multinomial logit model.

Important logic flows and data transformations:
a) The preprocess_data function creates dummy variables for the yogurt products, which allows the model to account for the different product types.
b) The calculate_utilities function computes the utilities by taking a linear combination of the feature matrix and the model parameters.
c) The loglik function calculates the choice probabilities using the utilities and the exponential function, and then computes the log-likelihood based on the observed choices in the data.
d) The minimize function from scipy.optimize is used to find the parameter values that maximize the log-likelihood, which is a common optimization problem in statistical modeling.


```{python}
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from typing import Tuple

def preprocess_data(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Preprocess the data by creating dummy variables and feature columns.

    Args:
        data (pandas.DataFrame): The input data frame.

    Returns:
        Tuple[pandas.DataFrame, pandas.DataFrame]: The preprocessed feature matrix and the original data frame.
    """
    # Create dummy variables for products
    feature_matrix = pd.get_dummies(data[['yogurt1', 'yogurt2', 'yogurt3']], drop_first=True)
    print("\nFeature Matrix:\n", feature_matrix)

    # Add price column
    feature_matrix['price'] = data['price']

    # Create a featured column based on the price column
    feature_matrix['featured'] = data['price'].apply(lambda x: 1 if x in [15, 25, 35, 45, 55] else 0)

    return feature_matrix, data

def calculate_utilities(params: np.ndarray, feature_matrix: pd.DataFrame) -> np.ndarray:
    """
    Calculate the utilities based on the feature matrix and parameters.

    Args:
        params (numpy.ndarray): The parameter values (beta_1, beta_2, beta_3, beta_f, beta_p).
        feature_matrix (pandas.DataFrame): The feature matrix.

    Returns:
        numpy.ndarray: The calculated utilities.
    """
    utilities = np.dot(feature_matrix, params)
    print("\nUtilities:\n", utilities)
    return utilities

def loglik(params: np.ndarray, data: pd.DataFrame, feature_matrix: pd.DataFrame) -> float:
    """
    Log-likelihood function for the multinomial logit model.

    Args:
        params (numpy.ndarray): The parameter values (beta_1, beta_2, beta_3, beta_f, beta_p).
        data (pandas.DataFrame): The original data frame.
        feature_matrix (pandas.DataFrame): The feature matrix.

    Returns:
        float: The negative log-likelihood value.
    """
    # Calculate the utilities
    V = calculate_utilities(params, feature_matrix)

    # Calculate the choice probabilities
    exp_V = np.exp(V)
    if exp_V.ndim == 1:
        sum_exp_V = exp_V.sum(keepdims=True)
    else:
        sum_exp_V = exp_V.sum(axis=1, keepdims=True)
    P = exp_V / sum_exp_V
    print("\nChoice Probabilities:\n", P)

    # Calculate the log-likelihood
    chosen_probs = P[data[['yogurt1', 'yogurt2', 'yogurt3']].values.argmax(axis=1)]
    log_likelihood = np.log(chosen_probs).sum()
    print("\nLog-likelihood:", -log_likelihood)

    return -log_likelihood

# Preprocess the data
feature_matrix, yogurt_data_long = preprocess_data(yogurt_data_long)

# Initial parameter values
initial_params = np.array([0.5, 0.5, 0.5, 0.1, -1.0])
print("\nInitial Parameters:", initial_params)

# Optimize the log-likelihood function
result = minimize(loglik, initial_params, args=(yogurt_data_long, feature_matrix), method='Nelder-Mead')

# Print the MLEs
print("\nMaximum Likelihood Estimates:")
print("beta_1 (Product 1 intercept):", result.x[0])
print("beta_2 (Product 2 intercept):", result.x[1])
print("beta_3 (Product 3 intercept):", result.x[2])
print("beta_f (Featured coefficient):", result.x[3])
print("beta_p (Price coefficient):", result.x[4])

```


Finding the maximum and minimum values from a set of three values (beta_1, beta_2, beta_3) to determine the most-preferred and least-preferred products.
Calculating the difference between the most-preferred and least-preferred product utilities (utility_difference).
Converting the utility_difference into a dollar value (dollar_value) by dividing it by the negative price coefficient (-beta_p).
The code assumes that the input values (result.x array) are already calculated and available. It performs a series of simple arithmetic operations and comparisons to extract the relevant information and present the results in a human-readable format.






```{python}
# Assuming the MLEs are already calculated and stored in the result.x array
# Extract the MLEs
beta_1 = result.x[0]  # Product 1 intercept
beta_2 = result.x[1]  # Product 2 intercept
beta_3 = result.x[2]  # Product 3 intercept
beta_f = result.x[3]  # Featured coefficient
beta_p = result.x[4]  # Price coefficient

# Find the most-preferred and least-preferred yogurts
most_preferred_intercept = max(beta_1, beta_2, beta_3)
least_preferred_intercept = min(beta_1, beta_2, beta_3)

# Calculate the utility difference
utility_difference = most_preferred_intercept - least_preferred_intercept

# Convert utility difference to dollar value using the price coefficient
dollar_value = utility_difference / (-beta_p)

# Print the results
print("Most-preferred yogurt intercept:", most_preferred_intercept)
print("Least-preferred yogurt intercept:", least_preferred_intercept)
print("Utility difference:", utility_difference)
print("Price coefficient (beta_p):", beta_p)
print("Dollar benefit between most-preferred and least-preferred yogurt: $%.2f per unit" % dollar_value)

```

the current market shares of three different yogurt products based on the data available in the yogurt_data_long DataFrame.

The code takes the yogurt_data_long DataFrame as input, which is assumed to contain columns named 'yogurt1', 'yogurt2', and 'yogurt3'. These columns likely represent the sales or consumption data for the three yogurt products.

The first line current_shares = yogurt_data_long[['yogurt1', 'yogurt2', 'yogurt3']].mean() calculates the mean (average) value of each column ('yogurt1', 'yogurt2', and 'yogurt3') across all rows in the DataFrame. The resulting current_shares variable is a Pandas Series containing the average values for each yogurt product, representing their respective market shares.

The subsequent lines print out the calculated market shares for each yogurt product. The print statements display the text "Current Market Shares:" followed by the market share value for each yogurt product, labeled as "Yogurt 1:", "Yogurt 2:", and "Yogurt 3:".

The output of this code will be the current market share values for the three yogurt products, displayed in a readable format on the console or output stream.

The code achieves its purpose by leveraging the Pandas library's functionality to calculate the mean (average) value of specific columns in a DataFrame. It assumes that the data in the 'yogurt1', 'yogurt2', and 'yogurt3' columns represents the sales or consumption data for the respective yogurt products, and that the average of these values can be interpreted as the market share for each product.
The lambda function adds 0.1 to the price if the price is one of the values [10, 20, 30, 40, 50], which likely correspond to the prices of yogurt 1.
It preprocesses the data with the new prices using the preprocess_data function, which is not shown in the code snippet.
It calculates the utilities (V_new) of the three yogurt products with the new prices using the calculate_utilities function and the estimated coefficients (result.x) from the logit model.
It calculates the choice probabilities (P_new) for each yogurt product using the utilities (V_new) and the logit formula: P = exp(V) / sum(exp(V)).
It calculates the new market shares by taking the mean of the choice probabilities for each yogurt product.
Finally, it prints the current and new market shares for the three yogurt products.
The code involves data transformations, such as updating the prices in the DataFrame and calculating utilities and choice probabilities based on the logit model. The logic flow involves preprocessing the data, calculating utilities and choice probabilities, and then aggregating the choice probabilities to obtain the market shares.TT


```{python}
# Calculate the current market shares
current_shares = yogurt_data_long[['yogurt1', 'yogurt2', 'yogurt3']].mean()
print("Current Market Shares:")
print("Yogurt 1:", current_shares['yogurt1'])
print("Yogurt 2:", current_shares['yogurt2'])
print("Yogurt 3:", current_shares['yogurt3'])

# Increase the price of yogurt 1 by $0.10
yogurt_data_long['price'] = yogurt_data_long['price'].apply(lambda x: x + 0.1 if x in [10, 20, 30, 40, 50] else x)

# Preprocess the data with the new prices
feature_matrix_new, _ = preprocess_data(yogurt_data_long)

# Calculate the utilities with the new prices
V_new = calculate_utilities(result.x, feature_matrix_new)

# Calculate the choice probabilities with the new prices
exp_V_new = np.exp(V_new)
if exp_V_new.ndim == 1:
    sum_exp_V_new = exp_V_new.sum(keepdims=True)
    P_new = exp_V_new / sum_exp_V_new
else:
    sum_exp_V_new = exp_V_new.sum(axis=1, keepdims=True)
    P_new = exp_V_new / sum_exp_V_new

# Calculate the new market shares
if P_new.ndim == 1:
    new_yogurt1_share = P_new[0]
    new_yogurt2_share = P_new[1]
    new_yogurt3_share = P_new[2]
else:
    new_yogurt1_share = P_new[:, 0].mean()
    new_yogurt2_share = P_new[:, 1].mean()
    new_yogurt3_share = P_new[:, 2].mean()

print("\nNew Market Shares (after increasing the price of yogurt 1 by $0.10):")
print("Yogurt 1:", new_yogurt1_share)
print("Yogurt 2:", new_yogurt2_share)
print("Yogurt 3:", new_yogurt3_share)

```




```{python}

```










```{python}

```
```{python}

```