---
title: "Project 4: Key Drivers Analysis for Credit Card Customer Satisfaction"
editor: visual
---
Introduction

In the context of the credit card industry, understanding the factors that influence customer satisfaction is crucial for companies to retain existing customers and attract new ones. This analysis aims to quantify the relationships between various perceptual factors and customer satisfaction levels, using a dataset containing information about credit card offerings and customer perceptions.

Methodology

The analysis employs a comprehensive approach, calculating and presenting different statistical measures to assess the influence and importance of each perceptual factor on customer satisfaction levels. The code reads a CSV file containing data related to customer perceptions and satisfaction levels, and separates the perceptual factors (features) and the satisfaction levels (target variable) into separate variables.

The following statistical measures are calculated and presented in a DataFrame called 'results':

Pearson Correlations: Measures the linear relationship between each perceptual factor and satisfaction levels.
Polychoric Correlations: Simulates the correlations between binary variables using Pearson correlations as a proxy.
Standardized Regression Coefficients: Indicates the change in satisfaction levels associated with a one-unit change in each perceptual factor, while holding other factors constant.
Shapley Values: Estimates the average marginal contribution of each perceptual factor to the prediction of satisfaction levels, using a Random Forest model.
Johnson's Epsilon: Assigns weights to each perceptual factor based on their contributions to the overall predictive power of the model.
Mean Decrease in RF Gini Coefficient: Measures the importance of each perceptual factor in a Random Forest model by calculating the decrease in the Gini impurity criterion when that factor is used for splitting.
The code also includes an example of implementing Lasso Regression with Cross-Validation as an alternative to the Linear Regression model, providing another perspective on the influence of each perceptual factor on customer satisfaction.

Results and Discussion

The results presented in the example DataFrame provide valuable insights into the relationships between various perceptual factors and customer satisfaction levels in the credit card industry. Some key observations can be made:

The perceptual factors "Makes a difference in my life" and "Is offered by a brand I trust" have the highest Pearson Correlations, Standardized Regression Coefficients, Shapley Values, and Johnson's Epsilon values. This suggests that these factors have a significant influence on customer satisfaction levels.

The factor "Provides outstanding customer service" also exhibits high values across multiple measures, indicating its importance in driving customer satisfaction.

Factors such as "Helps build credit quickly," "Is different from other cards," and "Is used by a lot of people" generally have lower values across the different measures, suggesting a relatively lower impact on customer satisfaction levels.

The Polychoric Correlations, simulating correlations between binary variables, show similar patterns to the Pearson Correlations, indicating that the linear relationships observed are consistent with the binary nature of the data.

The Mean Decrease in RF Gini Coefficient provides an alternative perspective on feature importance, with some variations compared to the other measures.

These results can guide credit card companies in prioritizing their efforts and resources to improve customer satisfaction. Factors with higher values across multiple measures should be given more attention, as they have a greater impact on customer satisfaction levels. Additionally, the analysis can help identify areas for improvement and inform product development, marketing strategies, and customer service initiatives.

It is important to note that while this analysis provides valuable insights, it should be complemented with domain knowledge and further investigation to fully understand the underlying drivers of customer satisfaction in the credit card industry.

The code starts by importing the necessary libraries (pandas, numpy, scikit-learn, and scipy.stats) and loading a CSV file named 
'data_for_drivers_analysis.csv' into a pandas DataFrame called 'data'. It then extracts the feature columns ('trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact') and the target column ('satisfaction') from the data.

Next, it calculates the Pearson Correlations between each feature and the target variable, and stores them in a dictionary called 'pearson_corr'. It then standardizes the features using StandardScaler from scikit-learn and performs a Linear Regression to obtain the standardized regression coefficients.

The initial results (Pearson Correlations and Standardized Regression Coefficients) are stored in a DataFrame called 'results'.

After that, the code trains a Random Forest Regression model on the data and uses its feature importances as a proxy for Shapley Values. It calculates Johnson's Relative Weights using the Random Forest feature importances. The Polychoric Correlations are simulated using the Pearson Correlations as a proxy, assuming binary variables. The Usefulness is interpreted as the standardized regression coefficients.

All the calculated measures (Pearson Correlations, Standardized Regression Coefficients, Polychoric Correlations, Shapley Values, Johnson's Epsilon, and Mean Decrease in RF Gini Coefficient) are added to the 'results' DataFrame and displayed.

The final output is a DataFrame called 'results' with the following columns:

Perception: The feature names ('trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact')
Pearson Correlations: The Pearson correlation coefficients between each feature and the target variable, expressed as percentages.
Standardized Regression Coefficients: The standardized regression coefficients from the Linear Regression model, expressed as percentages.
Polychoric Correlations: The simulated Polychoric Correlations using Pearson Correlations as a proxy, expressed as percentages.
Shapley Values: The feature importances from the Random Forest model, used as a proxy for Shapley Values, expressed as percentages.
Johnson's Epsilon: Johnson's Relative Weights calculated using the Random Forest feature importances, expressed as percentages.
Mean Decrease in RF Gini Coefficient: The mean decrease in the Gini impurity criterion from the Random Forest model, expressed as percentages.
The code also includes an example of implementing Lasso Regression with Cross-Validation as an alternative to the Linear Regression model. It standardizes the features, creates a LassoCV object with 5-fold cross-validation, and fits the model to the data. The resulting Lasso Regression coefficients are displayed in a separate DataFrame called 'lasso_results'.

Overall, the code aims to provide a comprehensive analysis of the relationships between various perceptual factors and satisfaction levels, by calculating and presenting different statistical measures that quantify the influence and importance of each factor. This analysis can help identify the most significant factors affecting satisfaction levels and guide decision-making processes related to improving driver satisfaction.

```{python}
import pandas as pd
import numpy as np
```

```{python}
# Load the CSV file to examine the data
file_path = 'data_for_drivers_analysis.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the data to understand its structure
data.head()

```

We'll start by performing the calculations for the Pearson Correlations and the Standardized Multiple Regression Coefficients. 
Polychoric Correlations: This requires specialized packages that may not be available in Python, but we can simulate it with the Pearson correlation for simplicity, assuming binary variables.


```{python}
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr

# Extracting the feature columns and the target column
features = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']
X = data[features]
y = data['satisfaction']

# Pearson Correlations
pearson_corr = {feature: pearsonr(X[feature], y)[0] for feature in features}

# Standardizing the features for regression
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Linear Regression to find standardized coefficients
reg = LinearRegression()
reg.fit(X_scaled, y)
standardized_coefficients = reg.coef_

# Create a dataframe to store these results
results = pd.DataFrame({
    'Perception': features,
    'Pearson Correlations': [f"{pearson_corr[feature] * 100:.1f}%" for feature in features],
    'Standardized Regression Coefficients': [f"{coef * 100:.1f}%" for coef in standardized_coefficients]
})

# Display the initial results
results
```

Next, let's calculate the Polychoric Correlations, Usefulness, Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the Gini coefficient from a random forest.

Polychoric Correlations: This requires specialized packages that may not be available in Python, but we can simulate it with the Pearson correlation for simplicity, assuming binary variables.

Usefulness: This can be interpreted as the contribution of each variable to the model's predictive power.

Shapley values for a linear regression: This involves calculating the average marginal contribution of each feature to the prediction.

Johnson's Relative Weights: This technique assigns weights to predictors based on their contributions to the R-squared value of the regression.

Mean Decrease in the Gini Coefficient: This will be calculated using a random forest model.

Let's proceed step by step. First, we'll calculate the Polychoric Correlations (simulated using Pearson correlations), then Usefulness, Shapley values, and finally, Johnson's relative weights and Mean Decrease in the Gini Coefficient.

```{python}
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
#import ace_tools as tools

# Calculate Shapley values using SHAP
rf_model = RandomForestRegressor()
rf_model.fit(X, y)


# Johnson's Relative Weights
def relative_weights(X, y):
    r = np.corrcoef(X, rowvar=False)
    r_y = np.corrcoef(X, y, rowvar=False)[-1, :-1]
    R = np.dot(r, r_y)
    beta = np.linalg.solve(r, R)
    raw_weights = beta * R
    relative_weights = raw_weights / raw_weights.sum()
    return relative_weights

# Use the built-in feature importance from the Random Forest model as a proxy for Shapley values
# and for calculating Johnson's relative weights and mean decrease in Gini coefficient

# Feature importance (as a proxy for Shapley values)
rf_importances = rf_model.feature_importances_

# Johnson's Relative Weights using Random Forest importances
johnson_weights_rf = rf_importances / np.sum(rf_importances)

# Polychoric Correlations (simulated using Pearson)
polychoric_corr = pearson_corr  # Using Pearson as a proxy

# Usefulness interpreted as standardized regression coefficients
usefulness = standardized_coefficients

# Combine all the results into the DataFrame
results['Polychoric Correlations'] = [f"{polychoric_corr[feature] * 100:.1f}%" for feature in features]
results['Shapley Values'] = [f"{value * 100:.1f}%" for value in rf_importances]
results['Johnson\'s Epsilon'] = [f"{weight * 100:.1f}%" for weight in johnson_weights_rf]
results['Mean Decrease in RF Gini Coefficient'] = [f"{importance * 100:.1f}%" for importance in rf_importances]

# Display the final results
results


```


Improving the regression coefficients, particularly their interpretability and accuracy, often involves several steps in the data preparation, modeling, and evaluation process. Here are some strategies to improve your regression coefficients:

Data Cleaning and Preparation:

Handle Missing Values: Ensure that any missing values in your dataset are properly handled, either by imputation or removal.
Outlier Detection: Detect and handle outliers which can skew the regression results.
Feature Scaling: Standardize or normalize your features to ensure that they are on a similar scale, which helps in improving the interpretability of coefficients.
Feature Engineering:

Create Interaction Terms: Consider creating interaction terms between features if you suspect that the interaction between variables could explain the target variable better.
Polynomial Features: Adding polynomial terms can help capture non-linear relationships.
Feature Selection: Use techniques like backward elimination, forward selection, or regularization methods to select the most relevant features.
Model Selection and Regularization:

Regularized Regression: Use Lasso (L1) or Ridge (L2) regression to penalize large coefficients and reduce overfitting. Elastic Net combines both penalties and can be useful if you have many correlated features.
Cross-Validation: Use cross-validation to tune the hyperparameters and select the best model.
Addressing Multicollinearity:

Variance Inflation Factor (VIF): Calculate VIF for your features and remove or combine features that have high multicollinearity.
Principal Component Analysis (PCA): Use PCA to transform the features into a set of linearly uncorrelated components.
Model Diagnostics and Validation:

Residual Analysis: Check residual plots to ensure that the assumptions of linear regression (linearity, independence, homoscedasticity, and normality of residuals) are not violated.
Model Comparison: Compare the performance of your model with simpler and more complex models to ensure that you are not overfitting or underfitting.
Domain Knowledge and Iteration:

Incorporate Domain Knowledge: Use domain knowledge to guide feature engineering and model selection.
Iterate and Refine: Model building is an iterative process. Continuously refine your features and model based on performance metrics and diagnostic plots.

## Implementing Regularized Regression (Example with Lasso Regression)


```{python}
from sklearn.linear_model import LassoCV
from sklearn.model_selection import cross_val_score

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Lasso Regression with Cross-Validation
lasso = LassoCV(cv=5, random_state=0)
lasso.fit(X_scaled, y)

# Coefficients from Lasso Regression
lasso_coefficients = lasso.coef_

# Display the coefficients
lasso_results = pd.DataFrame({
    'Perception': features,
    'Lasso Regression Coefficients': [f"{coef * 100:.1f}%" for coef in lasso_coefficients]
})

lasso_results
```