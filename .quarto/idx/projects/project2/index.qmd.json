{"title":"Homework 2","markdown":{"yaml":{"title":"Homework 2","editor":"visual"},"headingText":"Visualize distribution of number of patents by customer status","containsRefs":false,"markdown":"\n\nSheena Taylor is a data scientist specializing in machine learning and data analysis. She has a strong background in Python and has experience working with various data science libraries such as NumPy, Pandas, and Scikit-learn. Sheena is known for her ability to clean and preprocess data, build predictive models, and communicate her findings effectively to stakeholders. \n\n\nThe purpose of the code is to explore the relationship between various factors, such as age, region, and customer status, and the number of patents held by individuals or companies. It also aims to build a statistical model to predict the number of patents based on these factors.\n\nThe code takes a single input, which is a CSV file named \"blueprinty.csv\". This file likely contains data about individuals or companies, including information such as their age, region, customer status, and the number of patents they hold.\n\nThe output of the code includes various visualizations and statistical summaries, as well as the results of a Poisson regression model that predicts the number of patents based on the input variables.\n\nHere's how the code achieves its purpose:\n\nIt starts by importing the necessary Python libraries, such as pandas for data manipulation, numpy for numerical operations, matplotlib and seaborn for data visualization, and statsmodels for statistical modeling.\n\nThe code reads the \"blueprinty.csv\" file into a pandas DataFrame called \"blueprinty\".\n\nIt performs exploratory data analysis by printing the first few rows of the data, displaying information about the data types and columns, and calculating summary statistics like mean, median, and quartiles.\n\nThe code checks for missing values in the dataset.\n\nIt creates a histogram to visualize the distribution of the number of patents, grouped by customer status (whether the individual or company is a customer or not).\n\nThe code calculates and prints the mean number of patents for customers and non-customers separately.\n\nIt analyzes the distribution of regions and the mean age for customers and non-customers.\n\nThe code creates new columns in the DataFrame, such as \"age_squared\" and dummy variables for different regions, to be used as input variables for the regression model.\n\nIt fits a Poisson regression model using the statsmodels library, with the number of patents as the dependent variable and age, age squared, region dummies, and customer status as independent variables.\n\nThe code prints a summary of the regression results, including the estimated coefficients and their statistical significance.\n\nIt interprets the exponential of the coefficients, which represent the multiplicative effect of each variable on the expected number of patents.\n\nFinally, the code creates additional visualizations to explore the distributions of age and region by customer status.\n\nThe code follows a logical flow, starting with data loading and exploration, followed by data preprocessing and feature engineering, and then building and interpreting the statistical model. It performs various data transformations, such as creating dummy variables and calculating squared terms, to prepare the data for the regression analysis.\n\nThe dataset has 1500 rows (entries).\nThe \"patents\" column contains integer values ranging from 0 to 16, with a mean of around 3.68.\nThe \"age\" column contains float values ranging from 9 to 49, with a mean of around 26.36.\nThe \"iscustomer\" column is binary, with 0 representing non-customers and 1 representing customers. Around 13.13% of the entries are customers.\nThere are no missing values in the dataset, as indicated by the \"0\" values in the last line of the output.\n\nThe purpose of the code is to explore the relationship between the number of patents held by individuals and their customer status, as well as other factors such as age and region. It also aims to build a statistical model to predict the number of patents based on these variables.\n\nThe code takes a single input, which is a CSV file named \"blueprinty.csv\". This file is assumed to contain data with columns representing the number of patents, customer status (iscustomer), age, and region for a set of individuals.\n\nThe output of the code includes:\n\nExploratory data analysis results, such as summary statistics, missing value checks, and visualizations of the distribution of patents and other variables by customer status.\nComparison of means and distributions of patents, age, and region between customers and non-customers.\nA fitted Poisson regression model that relates the number of patents to age, age squared, region (encoded as dummy variables), and customer status.\nThe model coefficients and their exponential values, which can be interpreted as the multiplicative effect of each variable on the expected number of patents.\nAdditional visualizations of the distribution of age and region by customer status.\nTo achieve its purpose, the code follows these steps:\n\nImport necessary Python libraries for data manipulation, visualization, and statistical modeling.\nRead the \"blueprinty.csv\" file into a pandas DataFrame named \"blueprinty\".\nPerform exploratory data analysis by printing the first few rows, data types, summary statistics, and checking for missing values.\nVisualize the distribution of the number of patents by customer status using a histogram.\nCompare the mean number of patents between customers and non-customers.\nAnalyze the distribution of regions and mean age by customer status.\nCreate new columns in the DataFrame for age squared and region dummy variables.\nFit a Poisson regression model using the statsmodels library, with the number of patents as the dependent variable and age, age squared, region dummies, and customer status as independent variables.\nPrint the model summary and interpret the coefficients by taking their exponential.\nVisualize the distribution of age and region by customer status using histograms and bar plots.\nThe key logic flows and data transformations happening in the code include:\n\nExploratory data analysis to understand the characteristics of the dataset and identify potential relationships between variables.\nCreation of new features (age squared and region dummies) to capture non-linear effects and categorical variables in the regression model.\nFitting a Poisson regression model, which is suitable for modeling count data like the number of patents.\nInterpretation of model coefficients by taking their exponential, which provides the multiplicative effect of each variable on the expected number of patents.\n \n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n```\n\n```{python}\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n```\n\n```{python}\nprint(blueprinty.head())\nprint(blueprinty.info())\nprint(blueprinty.describe())\n```\n\n```{python}\nprint(blueprinty.isnull().sum())\n```\n\n```{python}\nplt.figure(figsize=(8, 6))\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', bins=20)\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.title('Distribution of Number of Patents by Customer Status')\nplt.show()\n```\n\n```{python}\n# Compare means of number of patents by customer status\nprint(\"Mean number of patents for customers:\", blueprinty[blueprinty['iscustomer']==1]['patents'].mean())\nprint(\"Mean number of patents for non-customers:\", blueprinty[blueprinty['iscustomer']==0]['patents'].mean())\n\n# Compare regions and ages by customer status\nprint(\"Region distribution for customers:\")\nprint(blueprinty[blueprinty['iscustomer']==1]['region'].value_counts(normalize=True))\nprint(\"Region distribution for non-customers:\")\nprint(blueprinty[blueprinty['iscustomer']==0]['region'].value_counts(normalize=True))\n\nprint(\"Mean age for customers:\", blueprinty[blueprinty['iscustomer']==1]['age'].mean())\nprint(\"Mean age for non-customers:\", blueprinty[blueprinty['iscustomer']==0]['age'].mean())\n\n```\n\nThe mean number of patents for customers (4.09) is higher than the mean number of patents for non-customers (3.62).\n\nThe distribution of regions for customers is different from the distribution for non-customers. For customers, the highest proportion is in the Northeast region (57.36%), followed by Southwest (15.74%), South (10.15%), Midwest (8.63%), and Northwest (8.12%). For non-customers, the highest proportion is also in the Northeast region (37.45%), followed by Southwest (20.41%), Midwest (15.89%), Northwest (13.12%), and South (13.12%).\n\nThe mean age for customers (24.15) is lower than the mean age for non-customers (26.69).\n\nThe output shows the summary of a Poisson regression model, which models the number of patents (the dependent variable) as a function of age, age squared, region dummies, and customer status (the independent variables).\n\nThe coefficient for the \"iscustomer\" variable is positive (0.1181) and statistically significant (p-value = 0.002), indicating that being a customer is associated with a higher expected number of patents, holding other variables constant.\n\nThe coefficients for age and age squared are positive and negative, respectively, suggesting a non-linear relationship between age and the number of patents.\n\nThe coefficients for the region dummies indicate that, compared to the reference region (Northwest), the Northeast region has a higher expected number of patents, while the other regions are not significantly different from the reference region.\n\nThe last part of the output shows the exponential of the coefficients, which can be interpreted as the multiplicative effect of each variable on the expected number of patents. For example, being a customer is associated with a 12.54% (exp(0.1181) - 1) increase in the expected number of patents, holding other variables constant.\n\nOverall, this output provides insights into the relationship between various factors (age, region, customer status) and the number of patents held by individuals or companies in the dataset. The Poisson regression model quantifies these relationships and can be used to make predictions or understand the relative importance of different factors.\n\n\n\n```{python}\nblueprinty['age_squared'] = blueprinty['age']**2\nblueprinty['region_Midwest'] = (blueprinty['region'] == 'Midwest').astype(int)\nblueprinty['region_Northeast'] = (blueprinty['region'] == 'Northeast').astype(int)\nblueprinty['region_South'] = (blueprinty['region'] == 'South').astype(int)\nblueprinty['region_Southwest'] = (blueprinty['region'] == 'Southwest').astype(int)\n\n```\n\n\n```{python}\n# Fit Poisson regression model\npoisson_model = sm.GLM(blueprinty['patents'], \n                       sm.add_constant(blueprinty[['age', 'age_squared', 'region_Midwest', \n                                                   'region_Northeast', 'region_South', \n                                                   'region_Southwest', 'iscustomer']]), \n                       family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nprint(poisson_results.summary())\n\n# Interpret model coefficients\nprint(\"Exponential of coefficients:\")\nprint(np.exp(poisson_results.params))\n```\n\n```{python}\nplt.figure(figsize=(8, 6))\nsns.histplot(data=blueprinty, x='age', hue='iscustomer', bins=20)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Distribution of Age by Customer Status')\nplt.show()\n```\n\nRegion distribution by customer status:\n\nFor customers, the Northeast region has the highest proportion (57.36%), followed by Southwest (15.74%), South (10.15%), Midwest (8.63%), and Northwest (8.12%).\nFor non-customers, the Northeast region also has the highest proportion (37.45%), but it is lower than the proportion for customers. The next highest proportions for non-customers are Southwest (20.41%), Midwest (15.89%), Northwest (13.12%), and South (13.12%).\nMean age by customer status:\n\nThe mean age for customers is 24.15 years.\nThe mean age for non-customers is 26.69 years, which is higher than the mean age for customers.\nFrom these observations, we can infer the following:\n\nThe Northeast region has a higher concentration of customers compared to non-customers, suggesting that the company's products or services may be more popular or better marketed in this region.\n\nThe Southwest region also has a higher proportion of customers compared to non-customers, indicating that it could be another important market for the company.\n\nThe Midwest and Northwest regions have a lower proportion of customers compared to non-customers, which could mean that the company's offerings are less popular or less accessible in these regions.\n\nCustomers tend to be younger, with a mean age of 24.15 years, compared to non-customers, who have a mean age of 26.69 years. This age difference could be due to various factors, such as the company's products or services being more appealing to younger individuals, or younger people being more likely to become customers.\n\nThese observations can help the company identify potential target markets based on regional preferences and age demographics. For example, the company may want to focus more marketing efforts in the Northeast and Southwest regions, or tailor their products and services to better appeal to younger age groups.\n\n\n\n\n\n\nThe distribution of regions for customers and non-customers in the dataset.\n\nFor customers:\n\nThe Northeast region has the highest proportion of customers at 57.36%.\nThe Southwest region has the second-highest proportion at 15.74%.\nThe South region has a proportion of 10.15%.\nThe Midwest region has a proportion of 8.63%.\nThe Northwest region has the lowest proportion of customers at 8.12%.\nFor non-customers:\n\nThe Northeast region also has the highest proportion of non-customers at 37.45%, but lower than the proportion for customers.\nThe Southwest region has the second-highest proportion of non-customers at 20.41%, higher than the proportion for customers.\nThe Midwest region has a proportion of 15.89%, higher than the proportion for customers.\nThe Northwest region has a proportion of 13.12%, higher than the proportion for customers.\nThe South region has a proportion of 13.12%, higher than the proportion for customers.\nIn summary, the key differences in the regional distribution between customers and non-customers are:\n\nThe Northeast region has a significantly higher proportion of customers compared to non-customers.\nThe Southwest region has a lower proportion of customers compared to non-customers.\nThe Midwest, Northwest, and South regions have a lower proportion of customers compared to non-customers.\nThis information suggests that the customer base is more concentrated in the Northeast region, while non-customers are more evenly distributed across regions, with a higher representation in the Southwest, Midwest, Northwest, and South regions compared to customers.\n\n\n\nAvatar for sht025-vvakx\nMean age for customers: 24.1497461928934\nMean age for non-customers: 26.691481197237145\n\n\nThe output you provided shows the mean age for customers and non-customers in the dataset.\n\nFor customers:\n\nThe mean age is 24.15 years.\nFor non-customers:\n\nThe mean age is 26.69 years.\nThis indicates that, on average, customers in the dataset are younger than non-customers.\n\nSpecifically:\n\nThe mean age for customers is approximately 24 years.\nThe mean age for non-customers is approximately 27 years.\nThe difference between the mean ages of customers and non-customers is around 2.5 years.\n\nThis information suggests that the customer base skews younger compared to the non-customer group. Potential reasons for this could be:\n\nThe product or service being offered may be more appealing or relevant to a younger demographic.\nYounger individuals or companies may be more likely to become customers for the particular offering.\nThere could be differences in the age distribution of the target market for customers versus non-customers.\nHowever, it's important to note that these are just potential explanations, and further analysis or additional context would be needed to draw more definitive conclusions about the relationship between age and customer status in this dataset\n\n\nprobability mass function (PMF) of the Poisson distribution, which is given by:\n\n$$f(Y|\\lambda) = \\frac{e^{-\\lambda}\\lambda^Y}{Y!}$$\n\nwhere:\n\n$Y$ is the random variable representing the number of events (in this case, the number of patents awarded)\n$\\lambda$ is the rate parameter, which represents the average number of events in the given time or space interval\n$e$ is the base of the natural logarithm (approximately 2.71828)\n$Y!$ represents the factorial of $Y$\nThe likelihood function is the joint probability of observing the given data, treated as a function of the unknown parameter(s). In the case of the Poisson distribution, we have a single parameter $\\lambda$.\n\nSuppose we have a sample of $n$ independent observations, $y_1, y_2, \\ldots, y_n$, where each $y_i$ represents the number of patents awarded to the $i$-th engineering firm. The likelihood function for this sample is the product of the individual Poisson probabilities:\n\n$$\\mathcal{L}(\\lambda|y_1, y_2, \\ldots, y_n) = \\prod_{i=1}^{n} f(y_i|\\lambda)$$\n\nSubstituting the Poisson PMF, we get:\n\n$$\\mathcal{L}(\\lambda|y_1, y_2, \\ldots, y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}$$\n\nThis likelihood function represents the joint probability of observing the data $y_1, y_2, \\ldots, y_n$ given the parameter $\\lambda$. The maximum likelihood estimate (MLE) of $\\lambda$ is the value that maximizes this likelihood function.\n\n\nThe Purpose: The code aims to provide an understanding of how to calculate the probability of observing a certain number of events (e.g., patents awarded) using the Poisson distribution, and how to find the maximum likelihood estimate (MLE) of the rate parameter (λ) given a set of observations.\n\nInput(s): The code does not take any direct input. However, it assumes that you have a set of observations (y_1, y_2, ..., y_n), where each observation represents the number of events (e.g., patents awarded) for a particular entity (e.g., engineering firm).\n\nOutput(s): The code does not produce a direct output. Instead, it provides the mathematical formulas and explanations for calculating the PMF and likelihood function, which can be used to find the MLE of the rate parameter (λ) based on the observed data.\n\nHow it achieves its purpose:\na) The code first introduces the PMF (probability mass function) for the Poisson distribution, which gives the probability of observing a specific number of events (Y) given the rate parameter (λ).\nb) It then explains the likelihood function, which is the joint probability of observing the given data (y_1, y_2, ..., y_n) treated as a function of the unknown parameter (λ).\nc) The likelihood function is calculated by multiplying the individual Poisson probabilities (PMFs) for each observation.\nd) The MLE of λ is the value that maximizes this likelihood function, meaning it is the value of λ that makes the observed data most likely to occur.\n\nImportant logic flows and data transformations:\na) The code assumes that the observations (y_1, y_2, ..., y_n) are independent and follow the Poisson distribution.\nb) It uses the product of individual Poisson probabilities (PMFs) to calculate the likelihood function.\nc) The goal is to find the value of λ that maximizes the likelihood function, which is the MLE of λ given the observed data.\n\nThe code provides a theoretical foundation for understanding and working with the Poisson distribution, which is useful in various applications where you need to model the occurrence of rare events or count data.\n\n\n\n\n```{python}\nimport math\n\ndef poisson_loglikelihood(lmbda, Y):\n    loglik = sum(Y * math.log(lmbda) - lmbda - math.lgamma(Y + 1))\n    return loglik\n```\n\nThe poisson_loglikelihood function calculates the log-likelihood of observing a set of data points (Y) given a specific rate parameter (lmbda) for the Poisson distribution.\n\nThe function takes two inputs:\n\nlmbda: The rate parameter of the Poisson distribution, which represents the average number of events occurring in a fixed interval.\nY: A list or array of observed data points, which are assumed to be counts or non-negative integers.\nThe output of the function is a single numerical value representing the log-likelihood of observing the data Y given the rate parameter lmbda.\n\nHere's how the function achieves its purpose:\n\nIt initializes a variable loglik to 0.\nFor each data point y in Y, it calculates the log-likelihood contribution using the formula: y * log(lmbda) - lmbda - log(y!), where log(y!) is calculated using the math.lgamma function, which computes the log of the gamma function.\nThe log-likelihood contributions for all data points are summed up and stored in the loglik variable.\nFinally, the function returns the total log-likelihood value.\nThe log-likelihood function is commonly used in statistical modeling and parameter estimation tasks. By maximizing the log-likelihood (or minimizing the negative log-likelihood), one can find the optimal value of the rate parameter lmbda that best explains the observed data Y under the assumption of a Poisson distribution.\n\nIt's important to note that this function assumes that the data points in Y are independent and identically distributed (i.i.d.) according to the Poisson distribution with the given rate parameter lmbda.\n\n```{python}\n# Load the data\ndata = pd.read_csv(\"blueprinty.csv\")\n\n# Get the observed number of patents\nY = data[\"patents\"].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 20, 100)\n\n# Calculate the log-likelihood for each lambda value\ndef poisson_loglikelihood(lamb, Y):\n    return np.sum(Y * np.log(lamb) - lamb)\n\nlog_likelihoods = [poisson_loglikelihood(lamb, Y) for lamb in lambda_values]\n\n# Plot the log-likelihood against lambda\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods)\n\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-likelihood\")\nplt.title(\"Log-likelihood of Poisson Distribution\")\nplt.show()\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 20, 100)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lamb, Y) for lamb in lambda_values]\n```\n\n\nfinding the maximum likelihood estimate (MLE) for the parameter (lambda) of a Poisson distribution, given some observed data.\n\nThe purpose of the code is to plot the log-likelihood function for a range of lambda values, which will help identify the value of lambda that maximizes the likelihood of observing the given data under the Poisson distribution assumption.\n\nThe input to the code is a CSV file named \"blueprinty.csv\", which contains a column named \"patents\". The code reads this file and extracts the values in the \"patents\" column as the observed data (Y).\n\nThe output of the code is a plot that shows the log-likelihood values for different lambda values. The lambda value that corresponds to the maximum log-likelihood value is the MLE for the Poisson distribution parameter.\n\nHere's how the code achieves its purpose:\na) It first loads the data from the CSV file into a pandas DataFrame and extracts the \"patents\" column as a NumPy array (Y).\nb) It defines a range of lambda values (from 0.1 to 20, with 100 values in between) using np.linspace().\nc) It defines a function poisson_loglikelihood() that calculates the log-likelihood of the Poisson distribution for a given lambda value and the observed data (Y).\nd) It uses a list comprehension to calculate the log-likelihood for each lambda value in the defined range, storing the results in the log_likelihoods list.\ne) Finally, it plots the lambda values on the x-axis and the corresponding log-likelihood values on the y-axis using matplotlib.pyplot.\n\nThe key logic flow and data transformation happening in the code are:\na) Reading the data from a CSV file and extracting the relevant column as a NumPy array.\nb) Defining a range of lambda values to evaluate the log-likelihood function.\nc) Calculating the log-likelihood for each lambda value using the Poisson distribution formula and the observed data.\nd) Plotting the log-likelihood values against the corresponding lambda values to visualize the log-likelihood function.\n\nThe code assumes that the observed data (Y) follows a Poisson distribution, and it aims to find the value of the lambda parameter that maximizes the likelihood of observing the given data under this assumption. The plot produced by the code can be used to identify the MLE for lambda visually, as it will correspond to the peak of the log-likelihood curve.\n\n```{python}\nybar = 0\n_todlambda_mle = ybar\n\n\n```\n\n```{python}\nimport scipy as sp\n\ndef likelihood(params, data):\n    mu, sigma = params\n    prob = sp.stats.norm(mu, sigma).pdf(data)\n    return np.prod(prob)\n\ndata = [1, 2, 3, 4, 5] \n\nres = sp.optimize.minimize(lambda params: -likelihood(params, data), [0, 1])\nmu_mle, sigma_mle = res.x\n\nprint(\"MLE for mu:\", mu_mle)\nprint(\"MLE for sigma:\", sigma_mle)\n```\n\nIt appears that the maximum likelihood estimates (MLEs) for the mean (μ) and standard deviation (σ) of the normal distribution, given the input data [1, 2, 3, 4, 5], are 0.0 and 1.0, respectively.\n\nThis result seems counterintuitive because the sample mean of the input data is 3.0, and the sample standard deviation is approximately 1.41. However, it's important to note that the maximum likelihood estimation (MLE) method does not necessarily produce the same results as the sample mean and sample standard deviation, especially for small sample sizes or when the data does not closely follow a normal distribution.\n\n estimates the maximum likelihood estimates (MLEs) of the parameters (mean and standard deviation) of a normal distribution, given a set of data points.\n\nThe Purpose of the Code:\nThe primary purpose of this code is to find the values of the mean (μ) and standard deviation (σ) that maximize the likelihood of observing the given data points, assuming that the data points are drawn from a normal (Gaussian) distribution.\n\nInput(s):\nThe code takes a list of data points as input. In the provided example, the data is a list of five numbers: [1, 2, 3, 4, 5].\n\nOutput(s):\nThe code outputs the maximum likelihood estimates (MLEs) of the mean (μ) and standard deviation (σ) for the given data points.\n\nHow it Achieves its Purpose:\nThe code defines a function called likelihood that calculates the likelihood of observing the given data points for a particular set of parameters (mean and standard deviation). The likelihood is calculated by multiplying the probability density function (PDF) of the normal distribution, evaluated at each data point, using the given parameters.\n\nThe scipy.optimize.minimize function is then used to find the values of the mean and standard deviation that maximize the likelihood function. The minimize function takes two arguments: the first is a function to be minimized (in this case, the negative of the likelihood function), and the second is an initial guess for the parameters (in this case, [0, 1] for the mean and standard deviation, respectively).\n\nThe minimize function uses an optimization algorithm to iteratively adjust the parameter values until it finds the values that minimize the negative likelihood function, which is equivalent to maximizing the likelihood function itself.\n\nImportant Logic Flows and Data Transformations:\na) The likelihood function calculates the probability density function (PDF) of the normal distribution for each data point, using the given parameters (mean and standard deviation).\nb) The individual PDF values are multiplied together to obtain the overall likelihood of observing the entire dataset under the assumed normal distribution with the given parameters.\nc) The scipy.optimize.minimize function is used to find the parameter values (mean and standard deviation) that maximize the likelihood function by minimizing the negative of the likelihood function.\nd) The optimization algorithm iteratively adjusts the parameter values until it converges to the maximum likelihood estimates (MLEs) for the mean and standard deviation.\nThe code assumes that the input data points are drawn from a normal distribution and uses the maximum likelihood estimation technique to estimate the parameters of that distribution based on the observed data.\n\n```{python}\nimport pandas as pd\n\n# Assuming data is a list\ndata = [[1, 2], [3, 4], [5, 6]]\n\n# Convert the list to a DataFrame\ndf = pd.DataFrame(data, columns=['column_1', 'column_2'])\n\n# Now you can access the shape\nprint(df.shape)  # Output: (3, 2)\n```\n\n```{python}\nimport pandas as pd\n\n# Assuming data is a 2D list or NumPy array with shape (3, 2)\ndf = pd.DataFrame(data, columns=['column_name_1', 'column_name_2'])\n\n```\n\n```{python}\nprint(df.shape)\nprint(df.columns)\n```\n\n```{python}\ny = df['column_name_1']\nX = df[['column_name_2']]\n\n```\n\n```{python}\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom typing import Tuple\n\ndef log_factorial(n):\n    \"\"\"\n    Calculate the natural logarithm of the factorial of a non-negative integer n.\n\n    Args:\n        n (int): Non-negative integer.\n\n    Returns:\n        float: Natural logarithm of the factorial of n.\n    \"\"\"\n    result = 0.0\n    for i in range(1, n + 1):\n        result += np.log(i)\n    return result\n\ndef negative_poisson_log_likelihood(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -> float:\n    \"\"\"\n    Calculate the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        float: Negative log-likelihood value.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the negative log-likelihood\n    log_factorials = np.array([log_factorial(y) for y in Y])\n    neg_log_likelihood = -np.sum(Y * np.log(lambda_i) - lambda_i - log_factorials)\n\n    return neg_log_likelihood\n\ndef negative_poisson_log_likelihood_grad(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate the gradient of the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        np.ndarray: Gradient of the negative log-likelihood.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the gradient of the negative log-likelihood\n    grad = X.T @ (lambda_i - Y)\n\n    return grad\n\ndef negative_poisson_log_likelihood_hess(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate the Hessian matrix of the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        np.ndarray: Hessian matrix of the negative log-likelihood.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the diagonal elements of the Hessian matrix\n    diag_elements = lambda_i\n\n    # Construct the Hessian matrix\n    Hessian = X.T @ np.diag(diag_elements) @ X\n\n    return Hessian\n\ndef fit_poisson_regression(Y: np.ndarray, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Fit a Poisson regression model using maximum likelihood estimation.\n\n    Args:\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Maximum likelihood estimates of beta and their standard errors.\n    \"\"\"\n    # Check input data\n    if not isinstance(Y, np.ndarray) or not isinstance(X, np.ndarray):\n        raise TypeError(\"Input data must be NumPy arrays.\")\n    if Y.ndim != 1 or X.ndim != 2:\n        raise ValueError(\"Input data must have the correct dimensions.\")\n\n    # Initial guess for beta\n    beta0 = np.zeros(X.shape[1])\n\n    # Optimize the negative log-likelihood function\n    try:\n        result = minimize(negative_poisson_log_likelihood, beta0, args=(Y, X), method='trust-constr',\n                          jac=negative_poisson_log_likelihood_grad, hess=negative_poisson_log_likelihood_hess)\n    except np.linalg.LinAlgError as e:\n        raise RuntimeError(\"Optimization failed due to a linear algebra error.\") from e\n\n    # Get the maximum likelihood estimates of beta\n    beta_mle = result.x\n    print(\"Maximum Likelihood Estimates of Beta:\")\n    print(beta_mle)\n\n    # Get the Hessian (second-order derivative) at the MLE\n    hessian = negative_poisson_log_likelihood_hess(beta_mle, Y, X)\n\n    # Standard errors of beta estimates\n    beta_std_errors = np.sqrt(np.diag(np.linalg.inv(hessian)))\n    print(\"\\nStandard Errors of Beta Estimates:\")\n    print(beta_std_errors)\n\n    # Print a table of coefficients and standard errors\n    print(\"\\nCoefficients and Standard Errors:\")\n    print(\"{:<20}{:<20}\".format(\"Coefficient\", \"Standard Error\"))\n    for i in range(len(beta_mle)):\n        print(\"{:<20}{:<20}\".format(beta_mle[i], beta_std_errors[i]))\n\n    return beta_mle, beta_std_errors\n\n# Example usage\n# Generate some sample data\nnp.random.seed(42)\nn_samples = 1000\nn_features = 3\n\nX = np.random.randn(n_samples, n_features)\ntrue_beta = np.array([1.0, -0.5, 0.3])\nlambda_true = np.exp(X @ true_beta)\nY = np.random.poisson(lambda_true)\n\n# Fit the Poisson regression model\nbeta_mle, beta_std_errors = fit_poisson_regression(Y, X)\n\n```\n\n Python implementation of Poisson regression using maximum likelihood estimation. Poisson regression is a statistical technique used to model count data, where the response variable (the thing we want to predict) represents the number of occurrences of an event.\n\nThe code takes two inputs:\n\nY: A one-dimensional NumPy array containing the observed count data (the response variable).\nX: A two-dimensional NumPy array containing the covariate data (the predictor variables).\nThe output of the code is a tuple containing:\n\nbeta_mle: A NumPy array with the maximum likelihood estimates of the regression coefficients (betas).\nbeta_std_errors: A NumPy array with the standard errors of the estimated regression coefficients.\nThe code achieves its purpose through the following steps:\n\nIt defines helper functions to calculate the log-factorial (log_factorial), the negative log-likelihood (negative_poisson_log_likelihood), its gradient (negative_poisson_log_likelihood_grad), and its Hessian matrix (negative_poisson_log_likelihood_hess).\nThe fit_poisson_regression function takes the input data Y and X and performs the following:\na. Checks if the input data is in the correct format (NumPy arrays with the expected dimensions).\nb. Sets an initial guess for the regression coefficients (beta0).\nc. Uses the scipy.optimize.minimize function to find the values of beta that minimize the negative log-likelihood function, using the provided gradient and Hessian functions.\nd. Calculates the standard errors of the estimated coefficients using the Hessian matrix at the maximum likelihood estimates.\ne. Prints the maximum likelihood estimates of beta and their standard errors.\nThe code includes an example usage section that generates some sample data, fits the Poisson regression model, and stores the results in beta_mle and beta_std_errors.\nThe key logic flows and data transformations happening in the code are:\n\nCalculating the log-factorial for each observed count in Y to use in the negative log-likelihood function.\nCalculating the predicted counts (lambda_i) for each observation using the linear combination of covariates and coefficients (X @ beta).\nCalculating the negative log-likelihood, its gradient, and its Hessian matrix using the observed counts (Y), predicted counts (lambda_i), and covariate data (X).\nOptimizing the negative log-likelihood function using the scipy.optimize.minimize function, which iteratively updates the coefficients (beta) to find the maximum likelihood estimates.\nCalculating the standard errors of the estimated coefficients using the Hessian matrix at the maximum likelihood estimates.\nThe code aims to provide a way to fit a Poisson regression model to count data and obtain the maximum likelihood estimates of the regression coefficients and their standard errors, which can be used for inference and prediction purposes.\n\nthe relationship between the number of patents and various predictor variables, including age, region, and whether the customer is a current customer or not.\n\nThe key results are:\n\nRegression Coefficients:\n\nThe coefficient for iscustomer is 0.1181, which is positive and statistically significant (p-value = 0.002). This suggests that being a current customer of Blueprinty's software is associated with a higher expected number of patents.\nThe coefficient for age is 0.1445, which is positive and significant, indicating that older customers tend to have more patents. However, the negative coefficient for age_squared (-0.0029) suggests that the effect of age on the number of patents is non-linear and starts to decrease after a certain age.\nThe coefficients for the different regions are mostly insignificant, except for the Northeast region, which has a positive and significant coefficient (0.1187) compared to the baseline region.\nIncidence Rate Ratios (IRRs):\n\nThe IRR for iscustomer is 1.125373, which means that current customers of Blueprinty's software are expected to have about 12.5% more patents than non-customers, holding all other variables constant.\nThe IRR for age is 1.155504, indicating that for each additional year of age, the expected number of patents increases by about 15.5%, up to a certain point where the effect starts to decrease due to the negative age_squared term.\nModel Fit:\n\nThe pseudo R-squared value of 0.1152 suggests that the model explains around 11.5% of the variation in the number of patents, which is relatively low but expected for count data models.\nThe deviance and Pearson chi-square statistics indicate some potential overdispersion in the data, which means that the variance of the dependent variable is greater than the mean, violating the assumption of the Poisson distribution. This could be addressed by using a different distribution (e.g., negative binomial) or adjusting the standard errors.\nOverall, the results suggest that Blueprinty's software has a positive effect on patent success, as current customers tend to have a higher expected number of patents compared to non-customers, even after accounting for other factors like age and region. However, the effect size is moderate, with a 12.5% increase in the expected number of patents for current customers.\n\nIt's important to note that this analysis assumes a causal relationship between being a Blueprinty customer and patent success, but there could be other confounding factors or selection biases that are not accounted for in the model. Additionally, the relatively low pseudo R-squared value indicates that there are other important factors influencing patent success that are not included in the model.\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\n\n\nairbnb = pd.read_csv('airbnb.csv')\n\n# Print the first few rows\nprint(airbnb.head())\n\n# Check for missing values\nprint(airbnb.info())\n\n# Descriptive statistics\nprint(airbnb.describe())\n\n# Check for missing values in each column\nprint(airbnb.isnull().sum())\n\n```\n\n```{python}\nimport pandas as pd\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv')\n# Handling missing values\nairbnb_data_clean = airbnb_data.copy()\n# Display the first few rows of the dataset\nprint(airbnb_data.head())\n\n# Check for missing values\nprint(airbnb_data.isnull().sum())\n\n# Get summary statistics for numerical variables\nprint(airbnb_data.describe())\n\n# Check the data types of each variable\nprint(airbnb_data.dtypes)\n```\n\n```{python}\n# Check the data types of each variable\nprint(airbnb_data.dtypes)\n```\n\n\n\n```{python}\n# Convert 'last_scraped' and 'host_since' to datetime\nairbnb_data_clean.loc[:, 'last_scraped'] = pd.to_datetime(airbnb_data_clean['last_scraped'])\nairbnb_data_clean.loc[:, 'host_since'] = pd.to_datetime(airbnb_data_clean['host_since'])\n\n# Convert 'room_type' and 'instant_bookable' to categorical\nairbnb_data_clean.loc[:, 'room_type'] = airbnb_data_clean['room_type'].astype('category')\nairbnb_data_clean.loc[:, 'instant_bookable'] = airbnb_data_clean['instant_bookable'].astype('category')\n\n```\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Load Airbnb data\nairbnb_data = pd.read_csv('airbnb.csv')\n\n# Handling missing values\nairbnb_data_clean = airbnb_data.copy()\n\n# Convert date columns to datetime\nairbnb_data_clean['last_scraped'] = pd.to_datetime(airbnb_data_clean['last_scraped'])\nairbnb_data_clean['host_since'] = pd.to_datetime(airbnb_data_clean['host_since'])\n\n# Impute missing values for numeric variables\nnumeric_cols = ['bathrooms', 'bedrooms', 'review_scores_cleanliness', \n                'review_scores_location', 'review_scores_value']\nairbnb_data_clean[numeric_cols] = airbnb_data_clean[numeric_cols].fillna(airbnb_data_clean[numeric_cols].median())\n\n# Replace missing values for categorical variables\nairbnb_data_clean['room_type'] = airbnb_data_clean['room_type'].fillna(airbnb_data_clean['room_type'].mode().iloc[0])\n\n# Drop rows with missing values in crucial columns\nairbnb_data_clean = airbnb_data_clean.dropna(subset=['host_since'])\n\n# Convert non-numeric columns to numeric\nairbnb_data_clean['room_type'] = airbnb_data_clean['room_type'].astype('category').cat.codes\nairbnb_data_clean['instant_bookable'] = airbnb_data_clean['instant_bookable'].astype('category').cat.codes\n\n# Exploratory Data Analysis (EDA)\n# Summary statistics\nsummary_stats = airbnb_data_clean.describe(include='all')\n\n# Visualizations \n# Histograms of numeric variables\nnumeric_cols = ['bathrooms', 'bedrooms', 'price', 'number_of_reviews']\nairbnb_data_clean[numeric_cols].hist(bins=20, figsize=(12, 8))\nplt.show()\n\n# Correlation matrix\ncorr_matrix = airbnb_data_clean[numeric_cols + ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']].corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()\n\n\n```\n\n```{python}\n# Define the dependent and independent variables\ndependent_var = airbnb_data_clean['number_of_reviews']\nindependent_vars = airbnb_data_clean[['bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness',\n                                      'review_scores_location', 'review_scores_value', 'room_type', 'instant_bookable']]\n\n# Add intercept term  \nindependent_vars = sm.add_constant(independent_vars)\n\n# Split data into training and testing sets\n# For simplicity, you can use the entire dataset for demonstration\ntrain_data = independent_vars  \ntest_data = dependent_var\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(dependent_var, independent_vars, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\nprint(poisson_model.summary())\n\n```\n\nwe can interpret the coefficients as follows:\n\nThe intercept coefficient (const) of 5.6050 represents the expected log count of reviews when all other variables are zero.\n\nThe coefficient for bathrooms is -0.1215, indicating that for every one-unit increase in the number of bathrooms, the expected log count of reviews decreases by 0.1215, holding all other variables constant.\n\nThe coefficient for bedrooms is 0.0710, suggesting that for every one-unit increase in the number of bedrooms, the expected log count of reviews increases by 0.0710, holding all other variables constant.\n\nThe coefficient for price is -0.0003, which means that for every $1 increase in the price, the expected log count of reviews decreases by 0.0003, holding all other variables constant.\n\nThe coefficient for review_scores_cleanliness is 0.0377, indicating that for every one-unit increase in the cleanliness review score, the expected log count of reviews increases by 0.0377, holding all other variables constant.\n\nThe coefficient for review_scores_location is -0.1804, suggesting that for every one-unit increase in the location review score, the expected log count of reviews decreases by 0.1804, holding all other variables constant.\n\nThe coefficient for review_scores_value is -0.1467, which means that for every one-unit increase in the value review score, the expected log count of reviews decreases by 0.1467, holding all other variables constant.\n\nThe coefficient for room_type is -0.1669, indicating that for every one-unit increase in the room type category (e.g., moving from a reference category to the next category), the expected log count of reviews decreases by 0.1669, holding all other variables constant.\n\nThe coefficient for instant_bookable is 0.3335, suggesting that for listings that are instantly bookable (compared to those that are not), the expected log count of reviews increases by 0.3335, holding all other variables constant.\n\nIt's important to note that the interpretation of coefficients in Poisson regression models is in terms of the log count of the dependent variable (number_of_reviews), rather than the actual count itself. Additionally, you should consider the statistical significance of the coefficients (based on p-values or confidence intervals) when interpreting the results.\n\nThe model summary also provides other useful information, such as the number of observations, degrees of freedom, log-likelihood, deviance, and pseudo R-squared values, which can be used to assess the overall model fit and performance.\n\nThe Airbnb dataset consists of over 40,000 listings from New York City, scraped in March 2017. The analysis aimed to understand the factors influencing the number of reviews received by Airbnb listings, which can be considered a proxy for the number of bookings.\n\nAfter data cleaning and preprocessing, including handling missing values and converting data types, a Poisson regression model was fitted to model the number of reviews as a function of various independent variables.\n\nThe Poisson regression model revealed several significant factors influencing the number of reviews:\n\nBathrooms: An increase in the number of bathrooms was associated with a decrease in the expected log count of reviews, suggesting that listings with fewer bathrooms tend to receive more reviews.\n\nBedrooms: An increase in the number of bedrooms was associated with an increase in the expected log count of reviews, indicating that listings with more bedrooms tend to receive more reviews.\n\nPrice: Higher listing prices were associated with a decrease in the expected log count of reviews, suggesting that more affordable listings tend to receive more reviews.\n\nReview Scores: Higher cleanliness review scores were associated with an increase in the expected log count of reviews, while higher location and value review scores were associated with a decrease in the expected log count of reviews.\n\nRoom Type: Moving from one room type category to the next (e.g., from a reference category to the next category) was associated with a decrease in the expected log count of reviews.\n\nInstant Bookable: Listings that were instantly bookable were associated with an increase in the expected log count of reviews compared to those that were not instantly bookable.\n\nThe analysis also included exploratory data analysis, such as summary statistics, histograms of numeric variables, and a correlation matrix, which provided insights into the data distribution and relationships between variables.\n\nOverall, the Airbnb data analysis revealed several factors that influence the number of reviews received by listings, which can be used by hosts and the platform to optimize their offerings and improve the overall user experience.\n\n\n\n\n<!-- ---\ntitle: \"Homework 2\"\neditor: visual\n---\n{{< pdf files/Air B .pdf.pdf width=100% height=800 >}} -->\n","srcMarkdownNoYaml":"\n\nSheena Taylor is a data scientist specializing in machine learning and data analysis. She has a strong background in Python and has experience working with various data science libraries such as NumPy, Pandas, and Scikit-learn. Sheena is known for her ability to clean and preprocess data, build predictive models, and communicate her findings effectively to stakeholders. \n\n\nThe purpose of the code is to explore the relationship between various factors, such as age, region, and customer status, and the number of patents held by individuals or companies. It also aims to build a statistical model to predict the number of patents based on these factors.\n\nThe code takes a single input, which is a CSV file named \"blueprinty.csv\". This file likely contains data about individuals or companies, including information such as their age, region, customer status, and the number of patents they hold.\n\nThe output of the code includes various visualizations and statistical summaries, as well as the results of a Poisson regression model that predicts the number of patents based on the input variables.\n\nHere's how the code achieves its purpose:\n\nIt starts by importing the necessary Python libraries, such as pandas for data manipulation, numpy for numerical operations, matplotlib and seaborn for data visualization, and statsmodels for statistical modeling.\n\nThe code reads the \"blueprinty.csv\" file into a pandas DataFrame called \"blueprinty\".\n\nIt performs exploratory data analysis by printing the first few rows of the data, displaying information about the data types and columns, and calculating summary statistics like mean, median, and quartiles.\n\nThe code checks for missing values in the dataset.\n\nIt creates a histogram to visualize the distribution of the number of patents, grouped by customer status (whether the individual or company is a customer or not).\n\nThe code calculates and prints the mean number of patents for customers and non-customers separately.\n\nIt analyzes the distribution of regions and the mean age for customers and non-customers.\n\nThe code creates new columns in the DataFrame, such as \"age_squared\" and dummy variables for different regions, to be used as input variables for the regression model.\n\nIt fits a Poisson regression model using the statsmodels library, with the number of patents as the dependent variable and age, age squared, region dummies, and customer status as independent variables.\n\nThe code prints a summary of the regression results, including the estimated coefficients and their statistical significance.\n\nIt interprets the exponential of the coefficients, which represent the multiplicative effect of each variable on the expected number of patents.\n\nFinally, the code creates additional visualizations to explore the distributions of age and region by customer status.\n\nThe code follows a logical flow, starting with data loading and exploration, followed by data preprocessing and feature engineering, and then building and interpreting the statistical model. It performs various data transformations, such as creating dummy variables and calculating squared terms, to prepare the data for the regression analysis.\n\nThe dataset has 1500 rows (entries).\nThe \"patents\" column contains integer values ranging from 0 to 16, with a mean of around 3.68.\nThe \"age\" column contains float values ranging from 9 to 49, with a mean of around 26.36.\nThe \"iscustomer\" column is binary, with 0 representing non-customers and 1 representing customers. Around 13.13% of the entries are customers.\nThere are no missing values in the dataset, as indicated by the \"0\" values in the last line of the output.\n\nThe purpose of the code is to explore the relationship between the number of patents held by individuals and their customer status, as well as other factors such as age and region. It also aims to build a statistical model to predict the number of patents based on these variables.\n\nThe code takes a single input, which is a CSV file named \"blueprinty.csv\". This file is assumed to contain data with columns representing the number of patents, customer status (iscustomer), age, and region for a set of individuals.\n\nThe output of the code includes:\n\nExploratory data analysis results, such as summary statistics, missing value checks, and visualizations of the distribution of patents and other variables by customer status.\nComparison of means and distributions of patents, age, and region between customers and non-customers.\nA fitted Poisson regression model that relates the number of patents to age, age squared, region (encoded as dummy variables), and customer status.\nThe model coefficients and their exponential values, which can be interpreted as the multiplicative effect of each variable on the expected number of patents.\nAdditional visualizations of the distribution of age and region by customer status.\nTo achieve its purpose, the code follows these steps:\n\nImport necessary Python libraries for data manipulation, visualization, and statistical modeling.\nRead the \"blueprinty.csv\" file into a pandas DataFrame named \"blueprinty\".\nPerform exploratory data analysis by printing the first few rows, data types, summary statistics, and checking for missing values.\nVisualize the distribution of the number of patents by customer status using a histogram.\nCompare the mean number of patents between customers and non-customers.\nAnalyze the distribution of regions and mean age by customer status.\nCreate new columns in the DataFrame for age squared and region dummy variables.\nFit a Poisson regression model using the statsmodels library, with the number of patents as the dependent variable and age, age squared, region dummies, and customer status as independent variables.\nPrint the model summary and interpret the coefficients by taking their exponential.\nVisualize the distribution of age and region by customer status using histograms and bar plots.\nThe key logic flows and data transformations happening in the code include:\n\nExploratory data analysis to understand the characteristics of the dataset and identify potential relationships between variables.\nCreation of new features (age squared and region dummies) to capture non-linear effects and categorical variables in the regression model.\nFitting a Poisson regression model, which is suitable for modeling count data like the number of patents.\nInterpretation of model coefficients by taking their exponential, which provides the multiplicative effect of each variable on the expected number of patents.\n \n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n```\n\n```{python}\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n```\n\n```{python}\nprint(blueprinty.head())\nprint(blueprinty.info())\nprint(blueprinty.describe())\n```\n\n```{python}\nprint(blueprinty.isnull().sum())\n```\n\n```{python}\n# Visualize distribution of number of patents by customer status\nplt.figure(figsize=(8, 6))\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', bins=20)\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.title('Distribution of Number of Patents by Customer Status')\nplt.show()\n```\n\n```{python}\n# Compare means of number of patents by customer status\nprint(\"Mean number of patents for customers:\", blueprinty[blueprinty['iscustomer']==1]['patents'].mean())\nprint(\"Mean number of patents for non-customers:\", blueprinty[blueprinty['iscustomer']==0]['patents'].mean())\n\n# Compare regions and ages by customer status\nprint(\"Region distribution for customers:\")\nprint(blueprinty[blueprinty['iscustomer']==1]['region'].value_counts(normalize=True))\nprint(\"Region distribution for non-customers:\")\nprint(blueprinty[blueprinty['iscustomer']==0]['region'].value_counts(normalize=True))\n\nprint(\"Mean age for customers:\", blueprinty[blueprinty['iscustomer']==1]['age'].mean())\nprint(\"Mean age for non-customers:\", blueprinty[blueprinty['iscustomer']==0]['age'].mean())\n\n```\n\nThe mean number of patents for customers (4.09) is higher than the mean number of patents for non-customers (3.62).\n\nThe distribution of regions for customers is different from the distribution for non-customers. For customers, the highest proportion is in the Northeast region (57.36%), followed by Southwest (15.74%), South (10.15%), Midwest (8.63%), and Northwest (8.12%). For non-customers, the highest proportion is also in the Northeast region (37.45%), followed by Southwest (20.41%), Midwest (15.89%), Northwest (13.12%), and South (13.12%).\n\nThe mean age for customers (24.15) is lower than the mean age for non-customers (26.69).\n\nThe output shows the summary of a Poisson regression model, which models the number of patents (the dependent variable) as a function of age, age squared, region dummies, and customer status (the independent variables).\n\nThe coefficient for the \"iscustomer\" variable is positive (0.1181) and statistically significant (p-value = 0.002), indicating that being a customer is associated with a higher expected number of patents, holding other variables constant.\n\nThe coefficients for age and age squared are positive and negative, respectively, suggesting a non-linear relationship between age and the number of patents.\n\nThe coefficients for the region dummies indicate that, compared to the reference region (Northwest), the Northeast region has a higher expected number of patents, while the other regions are not significantly different from the reference region.\n\nThe last part of the output shows the exponential of the coefficients, which can be interpreted as the multiplicative effect of each variable on the expected number of patents. For example, being a customer is associated with a 12.54% (exp(0.1181) - 1) increase in the expected number of patents, holding other variables constant.\n\nOverall, this output provides insights into the relationship between various factors (age, region, customer status) and the number of patents held by individuals or companies in the dataset. The Poisson regression model quantifies these relationships and can be used to make predictions or understand the relative importance of different factors.\n\n\n\n```{python}\nblueprinty['age_squared'] = blueprinty['age']**2\nblueprinty['region_Midwest'] = (blueprinty['region'] == 'Midwest').astype(int)\nblueprinty['region_Northeast'] = (blueprinty['region'] == 'Northeast').astype(int)\nblueprinty['region_South'] = (blueprinty['region'] == 'South').astype(int)\nblueprinty['region_Southwest'] = (blueprinty['region'] == 'Southwest').astype(int)\n\n```\n\n\n```{python}\n# Fit Poisson regression model\npoisson_model = sm.GLM(blueprinty['patents'], \n                       sm.add_constant(blueprinty[['age', 'age_squared', 'region_Midwest', \n                                                   'region_Northeast', 'region_South', \n                                                   'region_Southwest', 'iscustomer']]), \n                       family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nprint(poisson_results.summary())\n\n# Interpret model coefficients\nprint(\"Exponential of coefficients:\")\nprint(np.exp(poisson_results.params))\n```\n\n```{python}\nplt.figure(figsize=(8, 6))\nsns.histplot(data=blueprinty, x='age', hue='iscustomer', bins=20)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Distribution of Age by Customer Status')\nplt.show()\n```\n\nRegion distribution by customer status:\n\nFor customers, the Northeast region has the highest proportion (57.36%), followed by Southwest (15.74%), South (10.15%), Midwest (8.63%), and Northwest (8.12%).\nFor non-customers, the Northeast region also has the highest proportion (37.45%), but it is lower than the proportion for customers. The next highest proportions for non-customers are Southwest (20.41%), Midwest (15.89%), Northwest (13.12%), and South (13.12%).\nMean age by customer status:\n\nThe mean age for customers is 24.15 years.\nThe mean age for non-customers is 26.69 years, which is higher than the mean age for customers.\nFrom these observations, we can infer the following:\n\nThe Northeast region has a higher concentration of customers compared to non-customers, suggesting that the company's products or services may be more popular or better marketed in this region.\n\nThe Southwest region also has a higher proportion of customers compared to non-customers, indicating that it could be another important market for the company.\n\nThe Midwest and Northwest regions have a lower proportion of customers compared to non-customers, which could mean that the company's offerings are less popular or less accessible in these regions.\n\nCustomers tend to be younger, with a mean age of 24.15 years, compared to non-customers, who have a mean age of 26.69 years. This age difference could be due to various factors, such as the company's products or services being more appealing to younger individuals, or younger people being more likely to become customers.\n\nThese observations can help the company identify potential target markets based on regional preferences and age demographics. For example, the company may want to focus more marketing efforts in the Northeast and Southwest regions, or tailor their products and services to better appeal to younger age groups.\n\n\n\n\n\n\nThe distribution of regions for customers and non-customers in the dataset.\n\nFor customers:\n\nThe Northeast region has the highest proportion of customers at 57.36%.\nThe Southwest region has the second-highest proportion at 15.74%.\nThe South region has a proportion of 10.15%.\nThe Midwest region has a proportion of 8.63%.\nThe Northwest region has the lowest proportion of customers at 8.12%.\nFor non-customers:\n\nThe Northeast region also has the highest proportion of non-customers at 37.45%, but lower than the proportion for customers.\nThe Southwest region has the second-highest proportion of non-customers at 20.41%, higher than the proportion for customers.\nThe Midwest region has a proportion of 15.89%, higher than the proportion for customers.\nThe Northwest region has a proportion of 13.12%, higher than the proportion for customers.\nThe South region has a proportion of 13.12%, higher than the proportion for customers.\nIn summary, the key differences in the regional distribution between customers and non-customers are:\n\nThe Northeast region has a significantly higher proportion of customers compared to non-customers.\nThe Southwest region has a lower proportion of customers compared to non-customers.\nThe Midwest, Northwest, and South regions have a lower proportion of customers compared to non-customers.\nThis information suggests that the customer base is more concentrated in the Northeast region, while non-customers are more evenly distributed across regions, with a higher representation in the Southwest, Midwest, Northwest, and South regions compared to customers.\n\n\n\nAvatar for sht025-vvakx\nMean age for customers: 24.1497461928934\nMean age for non-customers: 26.691481197237145\n\n\nThe output you provided shows the mean age for customers and non-customers in the dataset.\n\nFor customers:\n\nThe mean age is 24.15 years.\nFor non-customers:\n\nThe mean age is 26.69 years.\nThis indicates that, on average, customers in the dataset are younger than non-customers.\n\nSpecifically:\n\nThe mean age for customers is approximately 24 years.\nThe mean age for non-customers is approximately 27 years.\nThe difference between the mean ages of customers and non-customers is around 2.5 years.\n\nThis information suggests that the customer base skews younger compared to the non-customer group. Potential reasons for this could be:\n\nThe product or service being offered may be more appealing or relevant to a younger demographic.\nYounger individuals or companies may be more likely to become customers for the particular offering.\nThere could be differences in the age distribution of the target market for customers versus non-customers.\nHowever, it's important to note that these are just potential explanations, and further analysis or additional context would be needed to draw more definitive conclusions about the relationship between age and customer status in this dataset\n\n\nprobability mass function (PMF) of the Poisson distribution, which is given by:\n\n$$f(Y|\\lambda) = \\frac{e^{-\\lambda}\\lambda^Y}{Y!}$$\n\nwhere:\n\n$Y$ is the random variable representing the number of events (in this case, the number of patents awarded)\n$\\lambda$ is the rate parameter, which represents the average number of events in the given time or space interval\n$e$ is the base of the natural logarithm (approximately 2.71828)\n$Y!$ represents the factorial of $Y$\nThe likelihood function is the joint probability of observing the given data, treated as a function of the unknown parameter(s). In the case of the Poisson distribution, we have a single parameter $\\lambda$.\n\nSuppose we have a sample of $n$ independent observations, $y_1, y_2, \\ldots, y_n$, where each $y_i$ represents the number of patents awarded to the $i$-th engineering firm. The likelihood function for this sample is the product of the individual Poisson probabilities:\n\n$$\\mathcal{L}(\\lambda|y_1, y_2, \\ldots, y_n) = \\prod_{i=1}^{n} f(y_i|\\lambda)$$\n\nSubstituting the Poisson PMF, we get:\n\n$$\\mathcal{L}(\\lambda|y_1, y_2, \\ldots, y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}$$\n\nThis likelihood function represents the joint probability of observing the data $y_1, y_2, \\ldots, y_n$ given the parameter $\\lambda$. The maximum likelihood estimate (MLE) of $\\lambda$ is the value that maximizes this likelihood function.\n\n\nThe Purpose: The code aims to provide an understanding of how to calculate the probability of observing a certain number of events (e.g., patents awarded) using the Poisson distribution, and how to find the maximum likelihood estimate (MLE) of the rate parameter (λ) given a set of observations.\n\nInput(s): The code does not take any direct input. However, it assumes that you have a set of observations (y_1, y_2, ..., y_n), where each observation represents the number of events (e.g., patents awarded) for a particular entity (e.g., engineering firm).\n\nOutput(s): The code does not produce a direct output. Instead, it provides the mathematical formulas and explanations for calculating the PMF and likelihood function, which can be used to find the MLE of the rate parameter (λ) based on the observed data.\n\nHow it achieves its purpose:\na) The code first introduces the PMF (probability mass function) for the Poisson distribution, which gives the probability of observing a specific number of events (Y) given the rate parameter (λ).\nb) It then explains the likelihood function, which is the joint probability of observing the given data (y_1, y_2, ..., y_n) treated as a function of the unknown parameter (λ).\nc) The likelihood function is calculated by multiplying the individual Poisson probabilities (PMFs) for each observation.\nd) The MLE of λ is the value that maximizes this likelihood function, meaning it is the value of λ that makes the observed data most likely to occur.\n\nImportant logic flows and data transformations:\na) The code assumes that the observations (y_1, y_2, ..., y_n) are independent and follow the Poisson distribution.\nb) It uses the product of individual Poisson probabilities (PMFs) to calculate the likelihood function.\nc) The goal is to find the value of λ that maximizes the likelihood function, which is the MLE of λ given the observed data.\n\nThe code provides a theoretical foundation for understanding and working with the Poisson distribution, which is useful in various applications where you need to model the occurrence of rare events or count data.\n\n\n\n\n```{python}\nimport math\n\ndef poisson_loglikelihood(lmbda, Y):\n    loglik = sum(Y * math.log(lmbda) - lmbda - math.lgamma(Y + 1))\n    return loglik\n```\n\nThe poisson_loglikelihood function calculates the log-likelihood of observing a set of data points (Y) given a specific rate parameter (lmbda) for the Poisson distribution.\n\nThe function takes two inputs:\n\nlmbda: The rate parameter of the Poisson distribution, which represents the average number of events occurring in a fixed interval.\nY: A list or array of observed data points, which are assumed to be counts or non-negative integers.\nThe output of the function is a single numerical value representing the log-likelihood of observing the data Y given the rate parameter lmbda.\n\nHere's how the function achieves its purpose:\n\nIt initializes a variable loglik to 0.\nFor each data point y in Y, it calculates the log-likelihood contribution using the formula: y * log(lmbda) - lmbda - log(y!), where log(y!) is calculated using the math.lgamma function, which computes the log of the gamma function.\nThe log-likelihood contributions for all data points are summed up and stored in the loglik variable.\nFinally, the function returns the total log-likelihood value.\nThe log-likelihood function is commonly used in statistical modeling and parameter estimation tasks. By maximizing the log-likelihood (or minimizing the negative log-likelihood), one can find the optimal value of the rate parameter lmbda that best explains the observed data Y under the assumption of a Poisson distribution.\n\nIt's important to note that this function assumes that the data points in Y are independent and identically distributed (i.i.d.) according to the Poisson distribution with the given rate parameter lmbda.\n\n```{python}\n# Load the data\ndata = pd.read_csv(\"blueprinty.csv\")\n\n# Get the observed number of patents\nY = data[\"patents\"].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 20, 100)\n\n# Calculate the log-likelihood for each lambda value\ndef poisson_loglikelihood(lamb, Y):\n    return np.sum(Y * np.log(lamb) - lamb)\n\nlog_likelihoods = [poisson_loglikelihood(lamb, Y) for lamb in lambda_values]\n\n# Plot the log-likelihood against lambda\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods)\n\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-likelihood\")\nplt.title(\"Log-likelihood of Poisson Distribution\")\nplt.show()\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 20, 100)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lamb, Y) for lamb in lambda_values]\n```\n\n\nfinding the maximum likelihood estimate (MLE) for the parameter (lambda) of a Poisson distribution, given some observed data.\n\nThe purpose of the code is to plot the log-likelihood function for a range of lambda values, which will help identify the value of lambda that maximizes the likelihood of observing the given data under the Poisson distribution assumption.\n\nThe input to the code is a CSV file named \"blueprinty.csv\", which contains a column named \"patents\". The code reads this file and extracts the values in the \"patents\" column as the observed data (Y).\n\nThe output of the code is a plot that shows the log-likelihood values for different lambda values. The lambda value that corresponds to the maximum log-likelihood value is the MLE for the Poisson distribution parameter.\n\nHere's how the code achieves its purpose:\na) It first loads the data from the CSV file into a pandas DataFrame and extracts the \"patents\" column as a NumPy array (Y).\nb) It defines a range of lambda values (from 0.1 to 20, with 100 values in between) using np.linspace().\nc) It defines a function poisson_loglikelihood() that calculates the log-likelihood of the Poisson distribution for a given lambda value and the observed data (Y).\nd) It uses a list comprehension to calculate the log-likelihood for each lambda value in the defined range, storing the results in the log_likelihoods list.\ne) Finally, it plots the lambda values on the x-axis and the corresponding log-likelihood values on the y-axis using matplotlib.pyplot.\n\nThe key logic flow and data transformation happening in the code are:\na) Reading the data from a CSV file and extracting the relevant column as a NumPy array.\nb) Defining a range of lambda values to evaluate the log-likelihood function.\nc) Calculating the log-likelihood for each lambda value using the Poisson distribution formula and the observed data.\nd) Plotting the log-likelihood values against the corresponding lambda values to visualize the log-likelihood function.\n\nThe code assumes that the observed data (Y) follows a Poisson distribution, and it aims to find the value of the lambda parameter that maximizes the likelihood of observing the given data under this assumption. The plot produced by the code can be used to identify the MLE for lambda visually, as it will correspond to the peak of the log-likelihood curve.\n\n```{python}\nybar = 0\n_todlambda_mle = ybar\n\n\n```\n\n```{python}\nimport scipy as sp\n\ndef likelihood(params, data):\n    mu, sigma = params\n    prob = sp.stats.norm(mu, sigma).pdf(data)\n    return np.prod(prob)\n\ndata = [1, 2, 3, 4, 5] \n\nres = sp.optimize.minimize(lambda params: -likelihood(params, data), [0, 1])\nmu_mle, sigma_mle = res.x\n\nprint(\"MLE for mu:\", mu_mle)\nprint(\"MLE for sigma:\", sigma_mle)\n```\n\nIt appears that the maximum likelihood estimates (MLEs) for the mean (μ) and standard deviation (σ) of the normal distribution, given the input data [1, 2, 3, 4, 5], are 0.0 and 1.0, respectively.\n\nThis result seems counterintuitive because the sample mean of the input data is 3.0, and the sample standard deviation is approximately 1.41. However, it's important to note that the maximum likelihood estimation (MLE) method does not necessarily produce the same results as the sample mean and sample standard deviation, especially for small sample sizes or when the data does not closely follow a normal distribution.\n\n estimates the maximum likelihood estimates (MLEs) of the parameters (mean and standard deviation) of a normal distribution, given a set of data points.\n\nThe Purpose of the Code:\nThe primary purpose of this code is to find the values of the mean (μ) and standard deviation (σ) that maximize the likelihood of observing the given data points, assuming that the data points are drawn from a normal (Gaussian) distribution.\n\nInput(s):\nThe code takes a list of data points as input. In the provided example, the data is a list of five numbers: [1, 2, 3, 4, 5].\n\nOutput(s):\nThe code outputs the maximum likelihood estimates (MLEs) of the mean (μ) and standard deviation (σ) for the given data points.\n\nHow it Achieves its Purpose:\nThe code defines a function called likelihood that calculates the likelihood of observing the given data points for a particular set of parameters (mean and standard deviation). The likelihood is calculated by multiplying the probability density function (PDF) of the normal distribution, evaluated at each data point, using the given parameters.\n\nThe scipy.optimize.minimize function is then used to find the values of the mean and standard deviation that maximize the likelihood function. The minimize function takes two arguments: the first is a function to be minimized (in this case, the negative of the likelihood function), and the second is an initial guess for the parameters (in this case, [0, 1] for the mean and standard deviation, respectively).\n\nThe minimize function uses an optimization algorithm to iteratively adjust the parameter values until it finds the values that minimize the negative likelihood function, which is equivalent to maximizing the likelihood function itself.\n\nImportant Logic Flows and Data Transformations:\na) The likelihood function calculates the probability density function (PDF) of the normal distribution for each data point, using the given parameters (mean and standard deviation).\nb) The individual PDF values are multiplied together to obtain the overall likelihood of observing the entire dataset under the assumed normal distribution with the given parameters.\nc) The scipy.optimize.minimize function is used to find the parameter values (mean and standard deviation) that maximize the likelihood function by minimizing the negative of the likelihood function.\nd) The optimization algorithm iteratively adjusts the parameter values until it converges to the maximum likelihood estimates (MLEs) for the mean and standard deviation.\nThe code assumes that the input data points are drawn from a normal distribution and uses the maximum likelihood estimation technique to estimate the parameters of that distribution based on the observed data.\n\n```{python}\nimport pandas as pd\n\n# Assuming data is a list\ndata = [[1, 2], [3, 4], [5, 6]]\n\n# Convert the list to a DataFrame\ndf = pd.DataFrame(data, columns=['column_1', 'column_2'])\n\n# Now you can access the shape\nprint(df.shape)  # Output: (3, 2)\n```\n\n```{python}\nimport pandas as pd\n\n# Assuming data is a 2D list or NumPy array with shape (3, 2)\ndf = pd.DataFrame(data, columns=['column_name_1', 'column_name_2'])\n\n```\n\n```{python}\nprint(df.shape)\nprint(df.columns)\n```\n\n```{python}\ny = df['column_name_1']\nX = df[['column_name_2']]\n\n```\n\n```{python}\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom typing import Tuple\n\ndef log_factorial(n):\n    \"\"\"\n    Calculate the natural logarithm of the factorial of a non-negative integer n.\n\n    Args:\n        n (int): Non-negative integer.\n\n    Returns:\n        float: Natural logarithm of the factorial of n.\n    \"\"\"\n    result = 0.0\n    for i in range(1, n + 1):\n        result += np.log(i)\n    return result\n\ndef negative_poisson_log_likelihood(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -> float:\n    \"\"\"\n    Calculate the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        float: Negative log-likelihood value.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the negative log-likelihood\n    log_factorials = np.array([log_factorial(y) for y in Y])\n    neg_log_likelihood = -np.sum(Y * np.log(lambda_i) - lambda_i - log_factorials)\n\n    return neg_log_likelihood\n\ndef negative_poisson_log_likelihood_grad(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate the gradient of the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        np.ndarray: Gradient of the negative log-likelihood.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the gradient of the negative log-likelihood\n    grad = X.T @ (lambda_i - Y)\n\n    return grad\n\ndef negative_poisson_log_likelihood_hess(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate the Hessian matrix of the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        np.ndarray: Hessian matrix of the negative log-likelihood.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the diagonal elements of the Hessian matrix\n    diag_elements = lambda_i\n\n    # Construct the Hessian matrix\n    Hessian = X.T @ np.diag(diag_elements) @ X\n\n    return Hessian\n\ndef fit_poisson_regression(Y: np.ndarray, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Fit a Poisson regression model using maximum likelihood estimation.\n\n    Args:\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Maximum likelihood estimates of beta and their standard errors.\n    \"\"\"\n    # Check input data\n    if not isinstance(Y, np.ndarray) or not isinstance(X, np.ndarray):\n        raise TypeError(\"Input data must be NumPy arrays.\")\n    if Y.ndim != 1 or X.ndim != 2:\n        raise ValueError(\"Input data must have the correct dimensions.\")\n\n    # Initial guess for beta\n    beta0 = np.zeros(X.shape[1])\n\n    # Optimize the negative log-likelihood function\n    try:\n        result = minimize(negative_poisson_log_likelihood, beta0, args=(Y, X), method='trust-constr',\n                          jac=negative_poisson_log_likelihood_grad, hess=negative_poisson_log_likelihood_hess)\n    except np.linalg.LinAlgError as e:\n        raise RuntimeError(\"Optimization failed due to a linear algebra error.\") from e\n\n    # Get the maximum likelihood estimates of beta\n    beta_mle = result.x\n    print(\"Maximum Likelihood Estimates of Beta:\")\n    print(beta_mle)\n\n    # Get the Hessian (second-order derivative) at the MLE\n    hessian = negative_poisson_log_likelihood_hess(beta_mle, Y, X)\n\n    # Standard errors of beta estimates\n    beta_std_errors = np.sqrt(np.diag(np.linalg.inv(hessian)))\n    print(\"\\nStandard Errors of Beta Estimates:\")\n    print(beta_std_errors)\n\n    # Print a table of coefficients and standard errors\n    print(\"\\nCoefficients and Standard Errors:\")\n    print(\"{:<20}{:<20}\".format(\"Coefficient\", \"Standard Error\"))\n    for i in range(len(beta_mle)):\n        print(\"{:<20}{:<20}\".format(beta_mle[i], beta_std_errors[i]))\n\n    return beta_mle, beta_std_errors\n\n# Example usage\n# Generate some sample data\nnp.random.seed(42)\nn_samples = 1000\nn_features = 3\n\nX = np.random.randn(n_samples, n_features)\ntrue_beta = np.array([1.0, -0.5, 0.3])\nlambda_true = np.exp(X @ true_beta)\nY = np.random.poisson(lambda_true)\n\n# Fit the Poisson regression model\nbeta_mle, beta_std_errors = fit_poisson_regression(Y, X)\n\n```\n\n Python implementation of Poisson regression using maximum likelihood estimation. Poisson regression is a statistical technique used to model count data, where the response variable (the thing we want to predict) represents the number of occurrences of an event.\n\nThe code takes two inputs:\n\nY: A one-dimensional NumPy array containing the observed count data (the response variable).\nX: A two-dimensional NumPy array containing the covariate data (the predictor variables).\nThe output of the code is a tuple containing:\n\nbeta_mle: A NumPy array with the maximum likelihood estimates of the regression coefficients (betas).\nbeta_std_errors: A NumPy array with the standard errors of the estimated regression coefficients.\nThe code achieves its purpose through the following steps:\n\nIt defines helper functions to calculate the log-factorial (log_factorial), the negative log-likelihood (negative_poisson_log_likelihood), its gradient (negative_poisson_log_likelihood_grad), and its Hessian matrix (negative_poisson_log_likelihood_hess).\nThe fit_poisson_regression function takes the input data Y and X and performs the following:\na. Checks if the input data is in the correct format (NumPy arrays with the expected dimensions).\nb. Sets an initial guess for the regression coefficients (beta0).\nc. Uses the scipy.optimize.minimize function to find the values of beta that minimize the negative log-likelihood function, using the provided gradient and Hessian functions.\nd. Calculates the standard errors of the estimated coefficients using the Hessian matrix at the maximum likelihood estimates.\ne. Prints the maximum likelihood estimates of beta and their standard errors.\nThe code includes an example usage section that generates some sample data, fits the Poisson regression model, and stores the results in beta_mle and beta_std_errors.\nThe key logic flows and data transformations happening in the code are:\n\nCalculating the log-factorial for each observed count in Y to use in the negative log-likelihood function.\nCalculating the predicted counts (lambda_i) for each observation using the linear combination of covariates and coefficients (X @ beta).\nCalculating the negative log-likelihood, its gradient, and its Hessian matrix using the observed counts (Y), predicted counts (lambda_i), and covariate data (X).\nOptimizing the negative log-likelihood function using the scipy.optimize.minimize function, which iteratively updates the coefficients (beta) to find the maximum likelihood estimates.\nCalculating the standard errors of the estimated coefficients using the Hessian matrix at the maximum likelihood estimates.\nThe code aims to provide a way to fit a Poisson regression model to count data and obtain the maximum likelihood estimates of the regression coefficients and their standard errors, which can be used for inference and prediction purposes.\n\nthe relationship between the number of patents and various predictor variables, including age, region, and whether the customer is a current customer or not.\n\nThe key results are:\n\nRegression Coefficients:\n\nThe coefficient for iscustomer is 0.1181, which is positive and statistically significant (p-value = 0.002). This suggests that being a current customer of Blueprinty's software is associated with a higher expected number of patents.\nThe coefficient for age is 0.1445, which is positive and significant, indicating that older customers tend to have more patents. However, the negative coefficient for age_squared (-0.0029) suggests that the effect of age on the number of patents is non-linear and starts to decrease after a certain age.\nThe coefficients for the different regions are mostly insignificant, except for the Northeast region, which has a positive and significant coefficient (0.1187) compared to the baseline region.\nIncidence Rate Ratios (IRRs):\n\nThe IRR for iscustomer is 1.125373, which means that current customers of Blueprinty's software are expected to have about 12.5% more patents than non-customers, holding all other variables constant.\nThe IRR for age is 1.155504, indicating that for each additional year of age, the expected number of patents increases by about 15.5%, up to a certain point where the effect starts to decrease due to the negative age_squared term.\nModel Fit:\n\nThe pseudo R-squared value of 0.1152 suggests that the model explains around 11.5% of the variation in the number of patents, which is relatively low but expected for count data models.\nThe deviance and Pearson chi-square statistics indicate some potential overdispersion in the data, which means that the variance of the dependent variable is greater than the mean, violating the assumption of the Poisson distribution. This could be addressed by using a different distribution (e.g., negative binomial) or adjusting the standard errors.\nOverall, the results suggest that Blueprinty's software has a positive effect on patent success, as current customers tend to have a higher expected number of patents compared to non-customers, even after accounting for other factors like age and region. However, the effect size is moderate, with a 12.5% increase in the expected number of patents for current customers.\n\nIt's important to note that this analysis assumes a causal relationship between being a Blueprinty customer and patent success, but there could be other confounding factors or selection biases that are not accounted for in the model. Additionally, the relatively low pseudo R-squared value indicates that there are other important factors influencing patent success that are not included in the model.\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\n\n\nairbnb = pd.read_csv('airbnb.csv')\n\n# Print the first few rows\nprint(airbnb.head())\n\n# Check for missing values\nprint(airbnb.info())\n\n# Descriptive statistics\nprint(airbnb.describe())\n\n# Check for missing values in each column\nprint(airbnb.isnull().sum())\n\n```\n\n```{python}\nimport pandas as pd\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv')\n# Handling missing values\nairbnb_data_clean = airbnb_data.copy()\n# Display the first few rows of the dataset\nprint(airbnb_data.head())\n\n# Check for missing values\nprint(airbnb_data.isnull().sum())\n\n# Get summary statistics for numerical variables\nprint(airbnb_data.describe())\n\n# Check the data types of each variable\nprint(airbnb_data.dtypes)\n```\n\n```{python}\n# Check the data types of each variable\nprint(airbnb_data.dtypes)\n```\n\n\n\n```{python}\n# Convert 'last_scraped' and 'host_since' to datetime\nairbnb_data_clean.loc[:, 'last_scraped'] = pd.to_datetime(airbnb_data_clean['last_scraped'])\nairbnb_data_clean.loc[:, 'host_since'] = pd.to_datetime(airbnb_data_clean['host_since'])\n\n# Convert 'room_type' and 'instant_bookable' to categorical\nairbnb_data_clean.loc[:, 'room_type'] = airbnb_data_clean['room_type'].astype('category')\nairbnb_data_clean.loc[:, 'instant_bookable'] = airbnb_data_clean['instant_bookable'].astype('category')\n\n```\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Load Airbnb data\nairbnb_data = pd.read_csv('airbnb.csv')\n\n# Handling missing values\nairbnb_data_clean = airbnb_data.copy()\n\n# Convert date columns to datetime\nairbnb_data_clean['last_scraped'] = pd.to_datetime(airbnb_data_clean['last_scraped'])\nairbnb_data_clean['host_since'] = pd.to_datetime(airbnb_data_clean['host_since'])\n\n# Impute missing values for numeric variables\nnumeric_cols = ['bathrooms', 'bedrooms', 'review_scores_cleanliness', \n                'review_scores_location', 'review_scores_value']\nairbnb_data_clean[numeric_cols] = airbnb_data_clean[numeric_cols].fillna(airbnb_data_clean[numeric_cols].median())\n\n# Replace missing values for categorical variables\nairbnb_data_clean['room_type'] = airbnb_data_clean['room_type'].fillna(airbnb_data_clean['room_type'].mode().iloc[0])\n\n# Drop rows with missing values in crucial columns\nairbnb_data_clean = airbnb_data_clean.dropna(subset=['host_since'])\n\n# Convert non-numeric columns to numeric\nairbnb_data_clean['room_type'] = airbnb_data_clean['room_type'].astype('category').cat.codes\nairbnb_data_clean['instant_bookable'] = airbnb_data_clean['instant_bookable'].astype('category').cat.codes\n\n# Exploratory Data Analysis (EDA)\n# Summary statistics\nsummary_stats = airbnb_data_clean.describe(include='all')\n\n# Visualizations \n# Histograms of numeric variables\nnumeric_cols = ['bathrooms', 'bedrooms', 'price', 'number_of_reviews']\nairbnb_data_clean[numeric_cols].hist(bins=20, figsize=(12, 8))\nplt.show()\n\n# Correlation matrix\ncorr_matrix = airbnb_data_clean[numeric_cols + ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']].corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()\n\n\n```\n\n```{python}\n# Define the dependent and independent variables\ndependent_var = airbnb_data_clean['number_of_reviews']\nindependent_vars = airbnb_data_clean[['bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness',\n                                      'review_scores_location', 'review_scores_value', 'room_type', 'instant_bookable']]\n\n# Add intercept term  \nindependent_vars = sm.add_constant(independent_vars)\n\n# Split data into training and testing sets\n# For simplicity, you can use the entire dataset for demonstration\ntrain_data = independent_vars  \ntest_data = dependent_var\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(dependent_var, independent_vars, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\nprint(poisson_model.summary())\n\n```\n\nwe can interpret the coefficients as follows:\n\nThe intercept coefficient (const) of 5.6050 represents the expected log count of reviews when all other variables are zero.\n\nThe coefficient for bathrooms is -0.1215, indicating that for every one-unit increase in the number of bathrooms, the expected log count of reviews decreases by 0.1215, holding all other variables constant.\n\nThe coefficient for bedrooms is 0.0710, suggesting that for every one-unit increase in the number of bedrooms, the expected log count of reviews increases by 0.0710, holding all other variables constant.\n\nThe coefficient for price is -0.0003, which means that for every $1 increase in the price, the expected log count of reviews decreases by 0.0003, holding all other variables constant.\n\nThe coefficient for review_scores_cleanliness is 0.0377, indicating that for every one-unit increase in the cleanliness review score, the expected log count of reviews increases by 0.0377, holding all other variables constant.\n\nThe coefficient for review_scores_location is -0.1804, suggesting that for every one-unit increase in the location review score, the expected log count of reviews decreases by 0.1804, holding all other variables constant.\n\nThe coefficient for review_scores_value is -0.1467, which means that for every one-unit increase in the value review score, the expected log count of reviews decreases by 0.1467, holding all other variables constant.\n\nThe coefficient for room_type is -0.1669, indicating that for every one-unit increase in the room type category (e.g., moving from a reference category to the next category), the expected log count of reviews decreases by 0.1669, holding all other variables constant.\n\nThe coefficient for instant_bookable is 0.3335, suggesting that for listings that are instantly bookable (compared to those that are not), the expected log count of reviews increases by 0.3335, holding all other variables constant.\n\nIt's important to note that the interpretation of coefficients in Poisson regression models is in terms of the log count of the dependent variable (number_of_reviews), rather than the actual count itself. Additionally, you should consider the statistical significance of the coefficients (based on p-values or confidence intervals) when interpreting the results.\n\nThe model summary also provides other useful information, such as the number of observations, degrees of freedom, log-likelihood, deviance, and pseudo R-squared values, which can be used to assess the overall model fit and performance.\n\nThe Airbnb dataset consists of over 40,000 listings from New York City, scraped in March 2017. The analysis aimed to understand the factors influencing the number of reviews received by Airbnb listings, which can be considered a proxy for the number of bookings.\n\nAfter data cleaning and preprocessing, including handling missing values and converting data types, a Poisson regression model was fitted to model the number of reviews as a function of various independent variables.\n\nThe Poisson regression model revealed several significant factors influencing the number of reviews:\n\nBathrooms: An increase in the number of bathrooms was associated with a decrease in the expected log count of reviews, suggesting that listings with fewer bathrooms tend to receive more reviews.\n\nBedrooms: An increase in the number of bedrooms was associated with an increase in the expected log count of reviews, indicating that listings with more bedrooms tend to receive more reviews.\n\nPrice: Higher listing prices were associated with a decrease in the expected log count of reviews, suggesting that more affordable listings tend to receive more reviews.\n\nReview Scores: Higher cleanliness review scores were associated with an increase in the expected log count of reviews, while higher location and value review scores were associated with a decrease in the expected log count of reviews.\n\nRoom Type: Moving from one room type category to the next (e.g., from a reference category to the next category) was associated with a decrease in the expected log count of reviews.\n\nInstant Bookable: Listings that were instantly bookable were associated with an increase in the expected log count of reviews compared to those that were not instantly bookable.\n\nThe analysis also included exploratory data analysis, such as summary statistics, histograms of numeric variables, and a correlation matrix, which provided insights into the data distribution and relationships between variables.\n\nOverall, the Airbnb data analysis revealed several factors that influence the number of reviews received by listings, which can be used by hosts and the platform to optimize their offerings and improve the overall user experience.\n\n\n\n\n<!-- ---\ntitle: \"Homework 2\"\neditor: visual\n---\n{{< pdf files/Air B .pdf.pdf width=100% height=800 >}} -->\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.506","theme":"cosmo","title":"Homework 2","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}