[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sheena Taylor is a versatile data-driven marketing and sales professional with a unique background in management and client success. Leveraging her robust foundation in Python and hands-on experience with key data science libraries such as NumPy, Pandas, and Scikit-learn, Sheena excels in employing innovative methodologies to solve complex problems and deliver tailored solutions that align with clients’ project objectives and budget constraints.Driven by a strong analytical mindset and a commitment to staying abreast of industry trends, Sheena seamlessly integrates qualitative and quantitative insights to inform strategic decision-making and drive measurable business impact. Her adaptability, attention to detail, and effective communication skills enable her to collaborate cross-functionally and forge lasting partnerships with stakeholders.With a keen eye for detail and a passion for continuous learning, Sheena is adept at utilizing various advanced analytical methods to extract meaningful insights from data. She is well-versed in data cleaning and preprocessing techniques, ensuring the quality and integrity of datasets for analysis. Additionally, Sheena specializes in predictive modeling, employing machine learning algorithms to forecast future trends and behaviors, thereby empowering businesses to make informed decisions.Moreover, Sheena possesses strong communication skills, enabling her to effectively convey insights and recommendations to stakeholders in a clear and actionable manner. Whether leading small-scale initiatives or spearheading large-scale projects on digital platforms, Sheena demonstrates a knack for driving results and achieving measurable outcomes.In her pursuit of expanding her intellectual horsepower, Sheena delves into diverse methodologies such as conjoint analysis and Bayesian inference.These techniques allow her to explore new avenues of problem-solving and gain a deeper understanding of consumer preferences and behavior, ultimately contributing to her ability to make a positive impact in the world and the global economy.With a passion for learning and a commitment to excellence, Sheena Taylor continues to push the boundaries of innovation in data-driven marketing and sales, consistently delivering value to clients and driving business growth in a dynamic digital landscape."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sheena Taylor",
    "section": "",
    "text": "Welcome to my website! I’m Sheena Taylor, a versatile data-driven marketing and sales professional with a unique background in management and client success."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Project 1",
    "section": "",
    "text": "The experiment aims to investigate the effect of matching donations on charitable giving. It randomly assigns individuals to treatment (matching) and control (no matching) groups and varies the match ratio and threshold to assess their impact on donation behavior.\nThe dataset contains information about the treatment assignment, match ratio, threshold, suggested donation amounts, and various demographic and historical giving variables for each individual.\nThe t-test and linear regression results show no significant difference in months since last donation between the treatment and control groups, indicating a balanced randomization. Table 1 in the paper is included to demonstrate the success of the randomization process.\nThe barplot visually compares the proportion of individuals who donated in the treatment and control groups.\nThe t-test and linear regression results indicate a statistically significant difference in the proportion of individuals who made a donation between the treatment and control groups. This suggests that matching donations positively influences the likelihood of making a charitable contribution.\nThe probit regression results confirm the findings from the linear regression, showing a significant positive effect of the treatment on the probability of making a donation.\nThe t-test results suggest that all match rates (1:1, 2:1, and 3:1) significantly increase the likelihood of donating compared to the control group. However, there is no clear evidence that higher match rates (2:1 or 3:1) are more effective than the 1:1 match rate.\nThe regression with ratio dummies confirms that all match rates have a significant positive effect on the likelihood of donating. The coefficients for ratio2 and ratio3 are not statistically different from ratio1, suggesting no incremental benefit of higher match rates.\nThe response rate differences between match ratios, calculated directly from the data and from the regression coefficients, are small and not statistically significant. This suggests that increasing the match ratio beyond 1:1 does not lead to a substantial increase in the likelihood of donating.\nThe t-test and linear regression results show a significant difference in the average donation amount between the treatment and control groups. However, this analysis includes both donors and non-donors, which may not provide a clear picture of the treatment effect on donation amount.\nThe analysis for donors only reveals no significant difference in the average donation amount between the treatment and control groups. The treatment coefficient in the linear regression does not have a causal interpretation, as it is conditional on the decision to donate, which is itself affected by the treatment.\nThe histograms of donation amounts for donors in the treatment and control groups show similar distributions, with the average donation amounts (indicated by the red vertical lines) being close to each other.\nThe plot demonstrates the Law of Large Numbers, showing that the cumulative average difference between the treatment and control groups converges to the true difference in means (0.022 - 0.018 = 0.004) as the number of draws increases.\nThe histograms illustrate the Central Limit Theorem, showing that as the sample size increases, the distribution of the average difference between the treatment and control groups becomes more normal and centered around the true difference in means (0.004). At smaller sample sizes, zero (the red vertical line) is closer to the middle of the distribution, while at larger sample sizes, zero moves towards the tail, indicating that the observed difference is less likely to be due to chance.\nSummary: The analysis of the experiment can be broken down into several key components. First, the data is read into R/Python and described, with a balance test performed to ensure the treatment and control groups are not significantly different on key variables. Next, the effect of matched donations on the response rate of making a donation can be analyzed using bar plots, t-tests, linear regression, and probit regression. The effectiveness of different match rates can be assessed using t-tests and regression, with the response rate differences between match ratios calculated directly from the data and from regression coefficients. The effect of matched donations on the size of the charitable contribution can be analyzed using t-tests and linear regression, both for all individuals and for only those who made a donation. Histograms of donation amounts can be plotted for the treatment and control groups.\nFinally, simulation can be used to demonstrate the Law of Large Numbers and the Central Limit Theorem, with plots of the cumulative average difference and histograms of average differences at different sample sizes. The results of these analyses can provide insights into the effectiveness of matched donations and the impact of different match rates on charitable giving behavior.\nThe experiment conducted by Karlan and List (2007) provides valuable insights into the effects of matching donations on charitable giving behavior. By randomly assigning individuals to treatment and control groups and varying the match ratio and threshold, the researchers were able to isolate the impact of these factors on donation decisions.\nThe analysis of the data reveals that the treatment, which involved offering matching donations, had a significant positive effect on the likelihood of making a charitable contribution. The t-test and linear regression results indicate a statistically significant difference in the proportion of individuals who made a donation between the treatment and control groups. This finding suggests that matching donations can be an effective strategy for encouraging people to give to charity.\nFurther investigation into the effects of different match rates (1:1, 2:1, and 3:1) shows that all match rates significantly increase the likelihood of donating compared to the control group. However, the results do not provide clear evidence that higher match rates are more effective than the 1:1 match rate. The regression with ratio dummies confirms this finding, as the coefficients for the 2:1 and 3:1 match rates are not statistically different from the 1:1 match rate. Additionally, the response rate differences between match ratios, calculated directly from the data and from the regression coefficients, are small and not statistically significant. This suggests that increasing the match ratio beyond 1:1 may not lead to a substantial increase in the likelihood of donating.\nWhen examining the impact of matching donations on the size of the charitable contribution, the t-test and linear regression results initially show a significant difference in the average donation amount between the treatment and control groups. However, this analysis includes both donors and non-donors, which may not provide a clear picture of the treatment effect on donation amount. When focusing solely on donors, the analysis reveals no significant difference in the average donation amount between the treatment and control groups. It is important to note that the treatment coefficient in the linear regression for donors does not have a causal interpretation, as it is conditional on the decision to donate, which is itself affected by the treatment.\nThe histograms of donation amounts for donors in the treatment and control groups show similar distributions, with the average donation amounts being close to each other. This visual representation supports the finding that matching donations may not have a significant impact on the size of the contribution among those who decide to donate.\nThe simulation experiments conducted in the study demonstrate the Law of Large Numbers and the Central Limit Theorem. The Law of Large Numbers plot shows that the cumulative average difference between the treatment and control groups converges to the true difference in means as the number of draws increases. The Central Limit Theorem histograms illustrate that as the sample size increases, the distribution of the average difference between the treatment and control groups becomes more normal and centered around the true difference in means. These simulations provide a deeper understanding of the statistical properties underlying the observed results.\nIn conclusion, the experiment by Karlan and List (2007) offers evidence that matching donations can be an effective tool for increasing the likelihood of charitable giving. However, the results suggest that higher match rates may not necessarily lead to a significant increase in donation rates or amounts compared to a 1:1 match rate. The findings of this study can inform the strategies employed by charities and policymakers to encourage charitable giving, while also highlighting the importance of careful analysis and interpretation of data when assessing the impact of such interventions."
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.1\n\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.1\n\n\nWarning: package 'readr' was built under R version 4.3.1\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\nWarning: package 'stringr' was built under R version 4.3.1\n\n\nWarning: package 'forcats' was built under R version 4.3.1\n\n\nWarning: package 'lubridate' was built under R version 4.3.1\n\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/index.html#import-data",
    "href": "projects/project1/index.html#import-data",
    "title": "HW1",
    "section": "",
    "text": "Below I import the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_ind, norm\nimport statsmodels.api as sm\n\nprint(\"hello world\")\n\nhello world"
  },
  {
    "objectID": "projects/project1/index.html#balance-checks",
    "href": "projects/project1/index.html#balance-checks",
    "title": "HW1",
    "section": "",
    "text": "# run regressions"
  },
  {
    "objectID": "projects/project 3/index.html",
    "href": "projects/project 3/index.html",
    "title": "Project 3",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project 3/index.html#import-data",
    "href": "projects/project 3/index.html#import-data",
    "title": "HW3",
    "section": "",
    "text": "Below I import the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_ind, norm\nimport statsmodels.api as sm\n\nprint(\"hello world\")\n\nhello world"
  },
  {
    "objectID": "projects/project 3/index.html#balance-checks",
    "href": "projects/project 3/index.html#balance-checks",
    "title": "HW3",
    "section": "",
    "text": "# run regressions"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Project 2",
    "section": "",
    "text": "The purpose of the code is to explore the relationship between various factors, such as age, region, and customer status, and the number of patents held by individuals or companies. It also aims to build a statistical model to predict the number of patents based on these factors.\nThe code takes a single input, which is a CSV file named “blueprinty.csv”. This file likely contains data about individuals or companies, including information such as their age, region, customer status, and the number of patents they hold.\nThe output of the code includes various visualizations and statistical summaries, as well as the results of a Poisson regression model that predicts the number of patents based on the input variables.\nHere’s how the code achieves its purpose:\nIt starts by importing the necessary Python libraries, such as pandas for data manipulation, numpy for numerical operations, matplotlib and seaborn for data visualization, and statsmodels for statistical modeling.\nThe code reads the “blueprinty.csv” file into a pandas DataFrame called “blueprinty”.\nIt performs exploratory data analysis by printing the first few rows of the data, displaying information about the data types and columns, and calculating summary statistics like mean, median, and quartiles.\nThe code checks for missing values in the dataset.\nIt creates a histogram to visualize the distribution of the number of patents, grouped by customer status (whether the individual or company is a customer or not).\nThe code calculates and prints the mean number of patents for customers and non-customers separately.\nIt analyzes the distribution of regions and the mean age for customers and non-customers.\nThe code creates new columns in the DataFrame, such as “age_squared” and dummy variables for different regions, to be used as input variables for the regression model.\nIt fits a Poisson regression model using the statsmodels library, with the number of patents as the dependent variable and age, age squared, region dummies, and customer status as independent variables.\nThe code prints a summary of the regression results, including the estimated coefficients and their statistical significance.\nIt interprets the exponential of the coefficients, which represent the multiplicative effect of each variable on the expected number of patents.\nFinally, the code creates additional visualizations to explore the distributions of age and region by customer status.\nThe code follows a logical flow, starting with data loading and exploration, followed by data preprocessing and feature engineering, and then building and interpreting the statistical model. It performs various data transformations, such as creating dummy variables and calculating squared terms, to prepare the data for the regression analysis.\nThe dataset has 1500 rows (entries). The “patents” column contains integer values ranging from 0 to 16, with a mean of around 3.68. The “age” column contains float values ranging from 9 to 49, with a mean of around 26.36. The “iscustomer” column is binary, with 0 representing non-customers and 1 representing customers. Around 13.13% of the entries are customers. There are no missing values in the dataset, as indicated by the “0” values in the last line of the output.\nThe purpose of the code is to explore the relationship between the number of patents held by individuals and their customer status, as well as other factors such as age and region. It also aims to build a statistical model to predict the number of patents based on these variables.\nThe code takes a single input, which is a CSV file named “blueprinty.csv”. This file is assumed to contain data with columns representing the number of patents, customer status (iscustomer), age, and region for a set of individuals.\nThe output of the code includes:\nExploratory data analysis results, such as summary statistics, missing value checks, and visualizations of the distribution of patents and other variables by customer status. Comparison of means and distributions of patents, age, and region between customers and non-customers. A fitted Poisson regression model that relates the number of patents to age, age squared, region (encoded as dummy variables), and customer status. The model coefficients and their exponential values, which can be interpreted as the multiplicative effect of each variable on the expected number of patents. Additional visualizations of the distribution of age and region by customer status. To achieve its purpose, the code follows these steps:\nImport necessary Python libraries for data manipulation, visualization, and statistical modeling. Read the “blueprinty.csv” file into a pandas DataFrame named “blueprinty”. Perform exploratory data analysis by printing the first few rows, data types, summary statistics, and checking for missing values. Visualize the distribution of the number of patents by customer status using a histogram. Compare the mean number of patents between customers and non-customers. Analyze the distribution of regions and mean age by customer status. Create new columns in the DataFrame for age squared and region dummy variables. Fit a Poisson regression model using the statsmodels library, with the number of patents as the dependent variable and age, age squared, region dummies, and customer status as independent variables. Print the model summary and interpret the coefficients by taking their exponential. Visualize the distribution of age and region by customer status using histograms and bar plots. The key logic flows and data transformations happening in the code include:\nExploratory data analysis to understand the characteristics of the dataset and identify potential relationships between variables. Creation of new features (age squared and region dummies) to capture non-linear effects and categorical variables in the regression model. Fitting a Poisson regression model, which is suitable for modeling count data like the number of patents. Interpretation of model coefficients by taking their exponential, which provides the multiplicative effect of each variable on the expected number of patents.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n\nprint(blueprinty.head())\nprint(blueprinty.info())\nprint(blueprinty.describe())\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Unnamed: 0  1500 non-null   int64  \n 1   patents     1500 non-null   int64  \n 2   region      1500 non-null   object \n 3   age         1500 non-null   float64\n 4   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(3), object(1)\nmemory usage: 58.7+ KB\nNone\n        Unnamed: 0      patents          age   iscustomer\ncount  1500.000000  1500.000000  1500.000000  1500.000000\nmean    750.500000     3.684667    26.357667     0.131333\nstd     433.157015     2.352500     7.242528     0.337877\nmin       1.000000     0.000000     9.000000     0.000000\n25%     375.750000     2.000000    21.000000     0.000000\n50%     750.500000     3.000000    26.000000     0.000000\n75%    1125.250000     5.000000    31.625000     0.000000\nmax    1500.000000    16.000000    49.000000     1.000000\n\n\n\nprint(blueprinty.isnull().sum())\n\nUnnamed: 0    0\npatents       0\nregion        0\nage           0\niscustomer    0\ndtype: int64\n\n\n\n# Visualize distribution of number of patents by customer status\nplt.figure(figsize=(8, 6))\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', bins=20)\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.title('Distribution of Number of Patents by Customer Status')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compare means of number of patents by customer status\nprint(\"Mean number of patents for customers:\", blueprinty[blueprinty['iscustomer']==1]['patents'].mean())\nprint(\"Mean number of patents for non-customers:\", blueprinty[blueprinty['iscustomer']==0]['patents'].mean())\n\n# Compare regions and ages by customer status\nprint(\"Region distribution for customers:\")\nprint(blueprinty[blueprinty['iscustomer']==1]['region'].value_counts(normalize=True))\nprint(\"Region distribution for non-customers:\")\nprint(blueprinty[blueprinty['iscustomer']==0]['region'].value_counts(normalize=True))\n\nprint(\"Mean age for customers:\", blueprinty[blueprinty['iscustomer']==1]['age'].mean())\nprint(\"Mean age for non-customers:\", blueprinty[blueprinty['iscustomer']==0]['age'].mean())\n\nMean number of patents for customers: 4.091370558375634\nMean number of patents for non-customers: 3.6231772831926325\nRegion distribution for customers:\nregion\nNortheast    0.573604\nSouthwest    0.157360\nSouth        0.101523\nMidwest      0.086294\nNorthwest    0.081218\nName: proportion, dtype: float64\nRegion distribution for non-customers:\nregion\nNortheast    0.374520\nSouthwest    0.204144\nMidwest      0.158864\nNorthwest    0.131236\nSouth        0.131236\nName: proportion, dtype: float64\nMean age for customers: 24.1497461928934\nMean age for non-customers: 26.691481197237145\n\n\nThe mean number of patents for customers (4.09) is higher than the mean number of patents for non-customers (3.62).\nThe distribution of regions for customers is different from the distribution for non-customers. For customers, the highest proportion is in the Northeast region (57.36%), followed by Southwest (15.74%), South (10.15%), Midwest (8.63%), and Northwest (8.12%). For non-customers, the highest proportion is also in the Northeast region (37.45%), followed by Southwest (20.41%), Midwest (15.89%), Northwest (13.12%), and South (13.12%).\nThe mean age for customers (24.15) is lower than the mean age for non-customers (26.69).\nThe output shows the summary of a Poisson regression model, which models the number of patents (the dependent variable) as a function of age, age squared, region dummies, and customer status (the independent variables).\nThe coefficient for the “iscustomer” variable is positive (0.1181) and statistically significant (p-value = 0.002), indicating that being a customer is associated with a higher expected number of patents, holding other variables constant.\nThe coefficients for age and age squared are positive and negative, respectively, suggesting a non-linear relationship between age and the number of patents.\nThe coefficients for the region dummies indicate that, compared to the reference region (Northwest), the Northeast region has a higher expected number of patents, while the other regions are not significantly different from the reference region.\nThe last part of the output shows the exponential of the coefficients, which can be interpreted as the multiplicative effect of each variable on the expected number of patents. For example, being a customer is associated with a 12.54% (exp(0.1181) - 1) increase in the expected number of patents, holding other variables constant.\nOverall, this output provides insights into the relationship between various factors (age, region, customer status) and the number of patents held by individuals or companies in the dataset. The Poisson regression model quantifies these relationships and can be used to make predictions or understand the relative importance of different factors.\n\nblueprinty['age_squared'] = blueprinty['age']**2\nblueprinty['region_Midwest'] = (blueprinty['region'] == 'Midwest').astype(int)\nblueprinty['region_Northeast'] = (blueprinty['region'] == 'Northeast').astype(int)\nblueprinty['region_South'] = (blueprinty['region'] == 'South').astype(int)\nblueprinty['region_Southwest'] = (blueprinty['region'] == 'Southwest').astype(int)\n\n\n# Fit Poisson regression model\npoisson_model = sm.GLM(blueprinty['patents'], \n                       sm.add_constant(blueprinty[['age', 'age_squared', 'region_Midwest', \n                                                   'region_Northeast', 'region_South', \n                                                   'region_Southwest', 'iscustomer']]), \n                       family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nprint(poisson_results.summary())\n\n# Interpret model coefficients\nprint(\"Exponential of coefficients:\")\nprint(np.exp(poisson_results.params))\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Sat, 11 May 2024   Deviance:                       2178.8\nTime:                        09:01:44   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.4714      0.184     -2.556      0.011      -0.833      -0.110\nage                  0.1445      0.014     10.414      0.000       0.117       0.172\nage_squared         -0.0029      0.000    -11.131      0.000      -0.003      -0.002\nregion_Midwest       0.0201      0.054      0.374      0.709      -0.085       0.126\nregion_Northeast     0.1187      0.045      2.639      0.008       0.031       0.207\nregion_South         0.0773      0.055      1.401      0.161      -0.031       0.185\nregion_Southwest     0.0714      0.050      1.432      0.152      -0.026       0.169\niscustomer           0.1181      0.039      3.035      0.002       0.042       0.194\n====================================================================================\nExponential of coefficients:\nconst               0.624143\nage                 1.155504\nage_squared         0.997136\nregion_Midwest      1.020297\nregion_Northeast    1.126021\nregion_South        1.080330\nregion_Southwest    1.074055\niscustomer          1.125373\ndtype: float64\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(data=blueprinty, x='age', hue='iscustomer', bins=20)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Distribution of Age by Customer Status')\nplt.show()\n\n\n\n\n\n\n\n\nRegion distribution by customer status:\nFor customers, the Northeast region has the highest proportion (57.36%), followed by Southwest (15.74%), South (10.15%), Midwest (8.63%), and Northwest (8.12%). For non-customers, the Northeast region also has the highest proportion (37.45%), but it is lower than the proportion for customers. The next highest proportions for non-customers are Southwest (20.41%), Midwest (15.89%), Northwest (13.12%), and South (13.12%). Mean age by customer status:\nThe mean age for customers is 24.15 years. The mean age for non-customers is 26.69 years, which is higher than the mean age for customers. From these observations, we can infer the following:\nThe Northeast region has a higher concentration of customers compared to non-customers, suggesting that the company’s products or services may be more popular or better marketed in this region.\nThe Southwest region also has a higher proportion of customers compared to non-customers, indicating that it could be another important market for the company.\nThe Midwest and Northwest regions have a lower proportion of customers compared to non-customers, which could mean that the company’s offerings are less popular or less accessible in these regions.\nCustomers tend to be younger, with a mean age of 24.15 years, compared to non-customers, who have a mean age of 26.69 years. This age difference could be due to various factors, such as the company’s products or services being more appealing to younger individuals, or younger people being more likely to become customers.\nThese observations can help the company identify potential target markets based on regional preferences and age demographics. For example, the company may want to focus more marketing efforts in the Northeast and Southwest regions, or tailor their products and services to better appeal to younger age groups.\nThe distribution of regions for customers and non-customers in the dataset.\nFor customers:\nThe Northeast region has the highest proportion of customers at 57.36%. The Southwest region has the second-highest proportion at 15.74%. The South region has a proportion of 10.15%. The Midwest region has a proportion of 8.63%. The Northwest region has the lowest proportion of customers at 8.12%. For non-customers:\nThe Northeast region also has the highest proportion of non-customers at 37.45%, but lower than the proportion for customers. The Southwest region has the second-highest proportion of non-customers at 20.41%, higher than the proportion for customers. The Midwest region has a proportion of 15.89%, higher than the proportion for customers. The Northwest region has a proportion of 13.12%, higher than the proportion for customers. The South region has a proportion of 13.12%, higher than the proportion for customers. In summary, the key differences in the regional distribution between customers and non-customers are:\nThe Northeast region has a significantly higher proportion of customers compared to non-customers. The Southwest region has a lower proportion of customers compared to non-customers. The Midwest, Northwest, and South regions have a lower proportion of customers compared to non-customers. This information suggests that the customer base is more concentrated in the Northeast region, while non-customers are more evenly distributed across regions, with a higher representation in the Southwest, Midwest, Northwest, and South regions compared to customers.\nAvatar for sht025-vvakx Mean age for customers: 24.1497461928934 Mean age for non-customers: 26.691481197237145\nThe output you provided shows the mean age for customers and non-customers in the dataset.\nFor customers:\nThe mean age is 24.15 years. For non-customers:\nThe mean age is 26.69 years. This indicates that, on average, customers in the dataset are younger than non-customers.\nSpecifically:\nThe mean age for customers is approximately 24 years. The mean age for non-customers is approximately 27 years. The difference between the mean ages of customers and non-customers is around 2.5 years.\nThis information suggests that the customer base skews younger compared to the non-customer group. Potential reasons for this could be:\nThe product or service being offered may be more appealing or relevant to a younger demographic. Younger individuals or companies may be more likely to become customers for the particular offering. There could be differences in the age distribution of the target market for customers versus non-customers. However, it’s important to note that these are just potential explanations, and further analysis or additional context would be needed to draw more definitive conclusions about the relationship between age and customer status in this dataset\nprobability mass function (PMF) of the Poisson distribution, which is given by:\n\\[f(Y|\\lambda) = \\frac{e^{-\\lambda}\\lambda^Y}{Y!}\\]\nwhere:\n\\(Y\\) is the random variable representing the number of events (in this case, the number of patents awarded) \\(\\lambda\\) is the rate parameter, which represents the average number of events in the given time or space interval \\(e\\) is the base of the natural logarithm (approximately 2.71828) \\(Y!\\) represents the factorial of \\(Y\\) The likelihood function is the joint probability of observing the given data, treated as a function of the unknown parameter(s). In the case of the Poisson distribution, we have a single parameter \\(\\lambda\\).\nSuppose we have a sample of \\(n\\) independent observations, \\(y_1, y_2, \\ldots, y_n\\), where each \\(y_i\\) represents the number of patents awarded to the \\(i\\)-th engineering firm. The likelihood function for this sample is the product of the individual Poisson probabilities:\n\\[\\mathcal{L}(\\lambda|y_1, y_2, \\ldots, y_n) = \\prod_{i=1}^{n} f(y_i|\\lambda)\\]\nSubstituting the Poisson PMF, we get:\n\\[\\mathcal{L}(\\lambda|y_1, y_2, \\ldots, y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}\\]\nThis likelihood function represents the joint probability of observing the data \\(y_1, y_2, \\ldots, y_n\\) given the parameter \\(\\lambda\\). The maximum likelihood estimate (MLE) of \\(\\lambda\\) is the value that maximizes this likelihood function.\nThe Purpose: The code aims to provide an understanding of how to calculate the probability of observing a certain number of events (e.g., patents awarded) using the Poisson distribution, and how to find the maximum likelihood estimate (MLE) of the rate parameter (λ) given a set of observations.\nInput(s): The code does not take any direct input. However, it assumes that you have a set of observations (y_1, y_2, …, y_n), where each observation represents the number of events (e.g., patents awarded) for a particular entity (e.g., engineering firm).\nOutput(s): The code does not produce a direct output. Instead, it provides the mathematical formulas and explanations for calculating the PMF and likelihood function, which can be used to find the MLE of the rate parameter (λ) based on the observed data.\nHow it achieves its purpose: a) The code first introduces the PMF (probability mass function) for the Poisson distribution, which gives the probability of observing a specific number of events (Y) given the rate parameter (λ). b) It then explains the likelihood function, which is the joint probability of observing the given data (y_1, y_2, …, y_n) treated as a function of the unknown parameter (λ). c) The likelihood function is calculated by multiplying the individual Poisson probabilities (PMFs) for each observation. d) The MLE of λ is the value that maximizes this likelihood function, meaning it is the value of λ that makes the observed data most likely to occur.\nImportant logic flows and data transformations: a) The code assumes that the observations (y_1, y_2, …, y_n) are independent and follow the Poisson distribution. b) It uses the product of individual Poisson probabilities (PMFs) to calculate the likelihood function. c) The goal is to find the value of λ that maximizes the likelihood function, which is the MLE of λ given the observed data.\nThe code provides a theoretical foundation for understanding and working with the Poisson distribution, which is useful in various applications where you need to model the occurrence of rare events or count data.\n\nimport math\n\ndef poisson_loglikelihood(lmbda, Y):\n    loglik = sum(Y * math.log(lmbda) - lmbda - math.lgamma(Y + 1))\n    return loglik\n\nThe poisson_loglikelihood function calculates the log-likelihood of observing a set of data points (Y) given a specific rate parameter (lmbda) for the Poisson distribution.\nThe function takes two inputs:\nlmbda: The rate parameter of the Poisson distribution, which represents the average number of events occurring in a fixed interval. Y: A list or array of observed data points, which are assumed to be counts or non-negative integers. The output of the function is a single numerical value representing the log-likelihood of observing the data Y given the rate parameter lmbda.\nHere’s how the function achieves its purpose:\nIt initializes a variable loglik to 0. For each data point y in Y, it calculates the log-likelihood contribution using the formula: y * log(lmbda) - lmbda - log(y!), where log(y!) is calculated using the math.lgamma function, which computes the log of the gamma function. The log-likelihood contributions for all data points are summed up and stored in the loglik variable. Finally, the function returns the total log-likelihood value. The log-likelihood function is commonly used in statistical modeling and parameter estimation tasks. By maximizing the log-likelihood (or minimizing the negative log-likelihood), one can find the optimal value of the rate parameter lmbda that best explains the observed data Y under the assumption of a Poisson distribution.\nIt’s important to note that this function assumes that the data points in Y are independent and identically distributed (i.i.d.) according to the Poisson distribution with the given rate parameter lmbda.\n\n# Load the data\ndata = pd.read_csv(\"blueprinty.csv\")\n\n# Get the observed number of patents\nY = data[\"patents\"].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 20, 100)\n\n# Calculate the log-likelihood for each lambda value\ndef poisson_loglikelihood(lamb, Y):\n    return np.sum(Y * np.log(lamb) - lamb)\n\nlog_likelihoods = [poisson_loglikelihood(lamb, Y) for lamb in lambda_values]\n\n# Plot the log-likelihood against lambda\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods)\n\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-likelihood\")\nplt.title(\"Log-likelihood of Poisson Distribution\")\nplt.show()\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 20, 100)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lamb, Y) for lamb in lambda_values]\n\n\n\n\n\n\n\n\nfinding the maximum likelihood estimate (MLE) for the parameter (lambda) of a Poisson distribution, given some observed data.\nThe purpose of the code is to plot the log-likelihood function for a range of lambda values, which will help identify the value of lambda that maximizes the likelihood of observing the given data under the Poisson distribution assumption.\nThe input to the code is a CSV file named “blueprinty.csv”, which contains a column named “patents”. The code reads this file and extracts the values in the “patents” column as the observed data (Y).\nThe output of the code is a plot that shows the log-likelihood values for different lambda values. The lambda value that corresponds to the maximum log-likelihood value is the MLE for the Poisson distribution parameter.\nHere’s how the code achieves its purpose: a) It first loads the data from the CSV file into a pandas DataFrame and extracts the “patents” column as a NumPy array (Y). b) It defines a range of lambda values (from 0.1 to 20, with 100 values in between) using np.linspace(). c) It defines a function poisson_loglikelihood() that calculates the log-likelihood of the Poisson distribution for a given lambda value and the observed data (Y). d) It uses a list comprehension to calculate the log-likelihood for each lambda value in the defined range, storing the results in the log_likelihoods list. e) Finally, it plots the lambda values on the x-axis and the corresponding log-likelihood values on the y-axis using matplotlib.pyplot.\nThe key logic flow and data transformation happening in the code are: a) Reading the data from a CSV file and extracting the relevant column as a NumPy array. b) Defining a range of lambda values to evaluate the log-likelihood function. c) Calculating the log-likelihood for each lambda value using the Poisson distribution formula and the observed data. d) Plotting the log-likelihood values against the corresponding lambda values to visualize the log-likelihood function.\nThe code assumes that the observed data (Y) follows a Poisson distribution, and it aims to find the value of the lambda parameter that maximizes the likelihood of observing the given data under this assumption. The plot produced by the code can be used to identify the MLE for lambda visually, as it will correspond to the peak of the log-likelihood curve.\n\nybar = 0\n_todlambda_mle = ybar\n\n\nimport scipy as sp\n\ndef likelihood(params, data):\n    mu, sigma = params\n    prob = sp.stats.norm(mu, sigma).pdf(data)\n    return np.prod(prob)\n\ndata = [1, 2, 3, 4, 5] \n\nres = sp.optimize.minimize(lambda params: -likelihood(params, data), [0, 1])\nmu_mle, sigma_mle = res.x\n\nprint(\"MLE for mu:\", mu_mle)\nprint(\"MLE for sigma:\", sigma_mle)\n\nMLE for mu: 0.0\nMLE for sigma: 1.0\n\n\nIt appears that the maximum likelihood estimates (MLEs) for the mean (μ) and standard deviation (σ) of the normal distribution, given the input data [1, 2, 3, 4, 5], are 0.0 and 1.0, respectively.\nThis result seems counterintuitive because the sample mean of the input data is 3.0, and the sample standard deviation is approximately 1.41. However, it’s important to note that the maximum likelihood estimation (MLE) method does not necessarily produce the same results as the sample mean and sample standard deviation, especially for small sample sizes or when the data does not closely follow a normal distribution.\nestimates the maximum likelihood estimates (MLEs) of the parameters (mean and standard deviation) of a normal distribution, given a set of data points.\nThe Purpose of the Code: The primary purpose of this code is to find the values of the mean (μ) and standard deviation (σ) that maximize the likelihood of observing the given data points, assuming that the data points are drawn from a normal (Gaussian) distribution.\nInput(s): The code takes a list of data points as input. In the provided example, the data is a list of five numbers: [1, 2, 3, 4, 5].\nOutput(s): The code outputs the maximum likelihood estimates (MLEs) of the mean (μ) and standard deviation (σ) for the given data points.\nHow it Achieves its Purpose: The code defines a function called likelihood that calculates the likelihood of observing the given data points for a particular set of parameters (mean and standard deviation). The likelihood is calculated by multiplying the probability density function (PDF) of the normal distribution, evaluated at each data point, using the given parameters.\nThe scipy.optimize.minimize function is then used to find the values of the mean and standard deviation that maximize the likelihood function. The minimize function takes two arguments: the first is a function to be minimized (in this case, the negative of the likelihood function), and the second is an initial guess for the parameters (in this case, [0, 1] for the mean and standard deviation, respectively).\nThe minimize function uses an optimization algorithm to iteratively adjust the parameter values until it finds the values that minimize the negative likelihood function, which is equivalent to maximizing the likelihood function itself.\nImportant Logic Flows and Data Transformations: a) The likelihood function calculates the probability density function (PDF) of the normal distribution for each data point, using the given parameters (mean and standard deviation). b) The individual PDF values are multiplied together to obtain the overall likelihood of observing the entire dataset under the assumed normal distribution with the given parameters. c) The scipy.optimize.minimize function is used to find the parameter values (mean and standard deviation) that maximize the likelihood function by minimizing the negative of the likelihood function. d) The optimization algorithm iteratively adjusts the parameter values until it converges to the maximum likelihood estimates (MLEs) for the mean and standard deviation. The code assumes that the input data points are drawn from a normal distribution and uses the maximum likelihood estimation technique to estimate the parameters of that distribution based on the observed data.\n\nimport pandas as pd\n\n# Assuming data is a list\ndata = [[1, 2], [3, 4], [5, 6]]\n\n# Convert the list to a DataFrame\ndf = pd.DataFrame(data, columns=['column_1', 'column_2'])\n\n# Now you can access the shape\nprint(df.shape)  # Output: (3, 2)\n\n(3, 2)\n\n\n\nimport pandas as pd\n\n# Assuming data is a 2D list or NumPy array with shape (3, 2)\ndf = pd.DataFrame(data, columns=['column_name_1', 'column_name_2'])\n\n\nprint(df.shape)\nprint(df.columns)\n\n(3, 2)\nIndex(['column_name_1', 'column_name_2'], dtype='object')\n\n\n\ny = df['column_name_1']\nX = df[['column_name_2']]\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom typing import Tuple\n\ndef log_factorial(n):\n    \"\"\"\n    Calculate the natural logarithm of the factorial of a non-negative integer n.\n\n    Args:\n        n (int): Non-negative integer.\n\n    Returns:\n        float: Natural logarithm of the factorial of n.\n    \"\"\"\n    result = 0.0\n    for i in range(1, n + 1):\n        result += np.log(i)\n    return result\n\ndef negative_poisson_log_likelihood(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        float: Negative log-likelihood value.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the negative log-likelihood\n    log_factorials = np.array([log_factorial(y) for y in Y])\n    neg_log_likelihood = -np.sum(Y * np.log(lambda_i) - lambda_i - log_factorials)\n\n    return neg_log_likelihood\n\ndef negative_poisson_log_likelihood_grad(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the gradient of the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        np.ndarray: Gradient of the negative log-likelihood.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the gradient of the negative log-likelihood\n    grad = X.T @ (lambda_i - Y)\n\n    return grad\n\ndef negative_poisson_log_likelihood_hess(beta: np.ndarray, Y: np.ndarray, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the Hessian matrix of the negative log-likelihood for Poisson regression.\n\n    Args:\n        beta (np.ndarray): Coefficients of the Poisson regression model.\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        np.ndarray: Hessian matrix of the negative log-likelihood.\n    \"\"\"\n    # Calculate lambda for each observation\n    lambda_i = np.exp(X @ beta)\n\n    # Calculate the diagonal elements of the Hessian matrix\n    diag_elements = lambda_i\n\n    # Construct the Hessian matrix\n    Hessian = X.T @ np.diag(diag_elements) @ X\n\n    return Hessian\n\ndef fit_poisson_regression(Y: np.ndarray, X: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Fit a Poisson regression model using maximum likelihood estimation.\n\n    Args:\n        Y (np.ndarray): Observed response variable (counts).\n        X (np.ndarray): Covariate matrix (design matrix).\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Maximum likelihood estimates of beta and their standard errors.\n    \"\"\"\n    # Check input data\n    if not isinstance(Y, np.ndarray) or not isinstance(X, np.ndarray):\n        raise TypeError(\"Input data must be NumPy arrays.\")\n    if Y.ndim != 1 or X.ndim != 2:\n        raise ValueError(\"Input data must have the correct dimensions.\")\n\n    # Initial guess for beta\n    beta0 = np.zeros(X.shape[1])\n\n    # Optimize the negative log-likelihood function\n    try:\n        result = minimize(negative_poisson_log_likelihood, beta0, args=(Y, X), method='trust-constr',\n                          jac=negative_poisson_log_likelihood_grad, hess=negative_poisson_log_likelihood_hess)\n    except np.linalg.LinAlgError as e:\n        raise RuntimeError(\"Optimization failed due to a linear algebra error.\") from e\n\n    # Get the maximum likelihood estimates of beta\n    beta_mle = result.x\n    print(\"Maximum Likelihood Estimates of Beta:\")\n    print(beta_mle)\n\n    # Get the Hessian (second-order derivative) at the MLE\n    hessian = negative_poisson_log_likelihood_hess(beta_mle, Y, X)\n\n    # Standard errors of beta estimates\n    beta_std_errors = np.sqrt(np.diag(np.linalg.inv(hessian)))\n    print(\"\\nStandard Errors of Beta Estimates:\")\n    print(beta_std_errors)\n\n    # Print a table of coefficients and standard errors\n    print(\"\\nCoefficients and Standard Errors:\")\n    print(\"{:&lt;20}{:&lt;20}\".format(\"Coefficient\", \"Standard Error\"))\n    for i in range(len(beta_mle)):\n        print(\"{:&lt;20}{:&lt;20}\".format(beta_mle[i], beta_std_errors[i]))\n\n    return beta_mle, beta_std_errors\n\n# Example usage\n# Generate some sample data\nnp.random.seed(42)\nn_samples = 1000\nn_features = 3\n\nX = np.random.randn(n_samples, n_features)\ntrue_beta = np.array([1.0, -0.5, 0.3])\nlambda_true = np.exp(X @ true_beta)\nY = np.random.poisson(lambda_true)\n\n# Fit the Poisson regression model\nbeta_mle, beta_std_errors = fit_poisson_regression(Y, X)\n\nMaximum Likelihood Estimates of Beta:\n[ 0.98288225 -0.49626067  0.3154154 ]\n\nStandard Errors of Beta Estimates:\n[0.01614833 0.02230447 0.02179918]\n\nCoefficients and Standard Errors:\nCoefficient         Standard Error      \n0.9828822489956339  0.016148332022996547\n-0.4962606677915634 0.02230447279867141 \n0.3154154048523373  0.02179917763934889 \n\n\nPython implementation of Poisson regression using maximum likelihood estimation. Poisson regression is a statistical technique used to model count data, where the response variable (the thing we want to predict) represents the number of occurrences of an event.\nThe code takes two inputs:\nY: A one-dimensional NumPy array containing the observed count data (the response variable). X: A two-dimensional NumPy array containing the covariate data (the predictor variables). The output of the code is a tuple containing:\nbeta_mle: A NumPy array with the maximum likelihood estimates of the regression coefficients (betas). beta_std_errors: A NumPy array with the standard errors of the estimated regression coefficients. The code achieves its purpose through the following steps:\nIt defines helper functions to calculate the log-factorial (log_factorial), the negative log-likelihood (negative_poisson_log_likelihood), its gradient (negative_poisson_log_likelihood_grad), and its Hessian matrix (negative_poisson_log_likelihood_hess). The fit_poisson_regression function takes the input data Y and X and performs the following: a. Checks if the input data is in the correct format (NumPy arrays with the expected dimensions). b. Sets an initial guess for the regression coefficients (beta0). c. Uses the scipy.optimize.minimize function to find the values of beta that minimize the negative log-likelihood function, using the provided gradient and Hessian functions. d. Calculates the standard errors of the estimated coefficients using the Hessian matrix at the maximum likelihood estimates. e. Prints the maximum likelihood estimates of beta and their standard errors. The code includes an example usage section that generates some sample data, fits the Poisson regression model, and stores the results in beta_mle and beta_std_errors. The key logic flows and data transformations happening in the code are:\nCalculating the log-factorial for each observed count in Y to use in the negative log-likelihood function. Calculating the predicted counts (lambda_i) for each observation using the linear combination of covariates and coefficients (X @ beta). Calculating the negative log-likelihood, its gradient, and its Hessian matrix using the observed counts (Y), predicted counts (lambda_i), and covariate data (X). Optimizing the negative log-likelihood function using the scipy.optimize.minimize function, which iteratively updates the coefficients (beta) to find the maximum likelihood estimates. Calculating the standard errors of the estimated coefficients using the Hessian matrix at the maximum likelihood estimates. The code aims to provide a way to fit a Poisson regression model to count data and obtain the maximum likelihood estimates of the regression coefficients and their standard errors, which can be used for inference and prediction purposes.\nthe relationship between the number of patents and various predictor variables, including age, region, and whether the customer is a current customer or not.\nThe key results are:\nRegression Coefficients:\nThe coefficient for iscustomer is 0.1181, which is positive and statistically significant (p-value = 0.002). This suggests that being a current customer of Blueprinty’s software is associated with a higher expected number of patents. The coefficient for age is 0.1445, which is positive and significant, indicating that older customers tend to have more patents. However, the negative coefficient for age_squared (-0.0029) suggests that the effect of age on the number of patents is non-linear and starts to decrease after a certain age. The coefficients for the different regions are mostly insignificant, except for the Northeast region, which has a positive and significant coefficient (0.1187) compared to the baseline region. Incidence Rate Ratios (IRRs):\nThe IRR for iscustomer is 1.125373, which means that current customers of Blueprinty’s software are expected to have about 12.5% more patents than non-customers, holding all other variables constant. The IRR for age is 1.155504, indicating that for each additional year of age, the expected number of patents increases by about 15.5%, up to a certain point where the effect starts to decrease due to the negative age_squared term. Model Fit:\nThe pseudo R-squared value of 0.1152 suggests that the model explains around 11.5% of the variation in the number of patents, which is relatively low but expected for count data models. The deviance and Pearson chi-square statistics indicate some potential overdispersion in the data, which means that the variance of the dependent variable is greater than the mean, violating the assumption of the Poisson distribution. This could be addressed by using a different distribution (e.g., negative binomial) or adjusting the standard errors. Overall, the results suggest that Blueprinty’s software has a positive effect on patent success, as current customers tend to have a higher expected number of patents compared to non-customers, even after accounting for other factors like age and region. However, the effect size is moderate, with a 12.5% increase in the expected number of patents for current customers.\nIt’s important to note that this analysis assumes a causal relationship between being a Blueprinty customer and patent success, but there could be other confounding factors or selection biases that are not accounted for in the model. Additionally, the relatively low pseudo R-squared value indicates that there are other important factors influencing patent success that are not included in the model.\n\nimport pandas as pd\nimport numpy as np\n\n\nairbnb = pd.read_csv('airbnb.csv')\n\n# Print the first few rows\nprint(airbnb.head())\n\n# Check for missing values\nprint(airbnb.info())\n\n# Descriptive statistics\nprint(airbnb.describe())\n\n# Check for missing values in each column\nprint(airbnb.isnull().sum())\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\nNone\n         Unnamed: 0            id          days     bathrooms      bedrooms  \\\ncount  40628.000000  4.062800e+04  40628.000000  40468.000000  40552.000000   \nmean   20314.500000  9.698889e+06   1102.368219      1.124592      1.147046   \nstd    11728.437705  5.460166e+06   1383.269358      0.385884      0.691746   \nmin        1.000000  2.515000e+03      1.000000      0.000000      0.000000   \n25%    10157.750000  4.889868e+06    542.000000      1.000000      1.000000   \n50%    20314.500000  9.862878e+06    996.000000      1.000000      1.000000   \n75%    30471.250000  1.466789e+07   1535.000000      1.000000      1.000000   \nmax    40628.000000  1.800967e+07  42828.000000      8.000000     10.000000   \n\n              price  number_of_reviews  review_scores_cleanliness  \\\ncount  40628.000000       40628.000000               30433.000000   \nmean     144.760732          15.904426                   9.198370   \nstd      210.657597          29.246009                   1.119935   \nmin       10.000000           0.000000                   2.000000   \n25%       70.000000           1.000000                   9.000000   \n50%      100.000000           4.000000                  10.000000   \n75%      170.000000          17.000000                  10.000000   \nmax    10000.000000         421.000000                  10.000000   \n\n       review_scores_location  review_scores_value  \ncount            30374.000000         30372.000000  \nmean                 9.413544             9.331522  \nstd                  0.844949             0.902966  \nmin                  2.000000             2.000000  \n25%                  9.000000             9.000000  \n50%                 10.000000            10.000000  \n75%                 10.000000            10.000000  \nmax                 10.000000            10.000000  \nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\nimport pandas as pd\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv')\n# Handling missing values\nairbnb_data_clean = airbnb_data.copy()\n# Display the first few rows of the dataset\nprint(airbnb_data.head())\n\n# Check for missing values\nprint(airbnb_data.isnull().sum())\n\n# Get summary statistics for numerical variables\nprint(airbnb_data.describe())\n\n# Check the data types of each variable\nprint(airbnb_data.dtypes)\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n         Unnamed: 0            id          days     bathrooms      bedrooms  \\\ncount  40628.000000  4.062800e+04  40628.000000  40468.000000  40552.000000   \nmean   20314.500000  9.698889e+06   1102.368219      1.124592      1.147046   \nstd    11728.437705  5.460166e+06   1383.269358      0.385884      0.691746   \nmin        1.000000  2.515000e+03      1.000000      0.000000      0.000000   \n25%    10157.750000  4.889868e+06    542.000000      1.000000      1.000000   \n50%    20314.500000  9.862878e+06    996.000000      1.000000      1.000000   \n75%    30471.250000  1.466789e+07   1535.000000      1.000000      1.000000   \nmax    40628.000000  1.800967e+07  42828.000000      8.000000     10.000000   \n\n              price  number_of_reviews  review_scores_cleanliness  \\\ncount  40628.000000       40628.000000               30433.000000   \nmean     144.760732          15.904426                   9.198370   \nstd      210.657597          29.246009                   1.119935   \nmin       10.000000           0.000000                   2.000000   \n25%       70.000000           1.000000                   9.000000   \n50%      100.000000           4.000000                  10.000000   \n75%      170.000000          17.000000                  10.000000   \nmax    10000.000000         421.000000                  10.000000   \n\n       review_scores_location  review_scores_value  \ncount            30374.000000         30372.000000  \nmean                 9.413544             9.331522  \nstd                  0.844949             0.902966  \nmin                  2.000000             2.000000  \n25%                  9.000000             9.000000  \n50%                 10.000000            10.000000  \n75%                 10.000000            10.000000  \nmax                 10.000000            10.000000  \nUnnamed: 0                     int64\nid                             int64\ndays                           int64\nlast_scraped                  object\nhost_since                    object\nroom_type                     object\nbathrooms                    float64\nbedrooms                     float64\nprice                          int64\nnumber_of_reviews              int64\nreview_scores_cleanliness    float64\nreview_scores_location       float64\nreview_scores_value          float64\ninstant_bookable              object\ndtype: object\n\n\n\n# Check the data types of each variable\nprint(airbnb_data.dtypes)\n\nUnnamed: 0                     int64\nid                             int64\ndays                           int64\nlast_scraped                  object\nhost_since                    object\nroom_type                     object\nbathrooms                    float64\nbedrooms                     float64\nprice                          int64\nnumber_of_reviews              int64\nreview_scores_cleanliness    float64\nreview_scores_location       float64\nreview_scores_value          float64\ninstant_bookable              object\ndtype: object\n\n\n\n# Convert 'last_scraped' and 'host_since' to datetime\nairbnb_data_clean.loc[:, 'last_scraped'] = pd.to_datetime(airbnb_data_clean['last_scraped'])\nairbnb_data_clean.loc[:, 'host_since'] = pd.to_datetime(airbnb_data_clean['host_since'])\n\n# Convert 'room_type' and 'instant_bookable' to categorical\nairbnb_data_clean.loc[:, 'room_type'] = airbnb_data_clean['room_type'].astype('category')\nairbnb_data_clean.loc[:, 'instant_bookable'] = airbnb_data_clean['instant_bookable'].astype('category')\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Load Airbnb data\nairbnb_data = pd.read_csv('airbnb.csv')\n\n# Handling missing values\nairbnb_data_clean = airbnb_data.copy()\n\n# Convert date columns to datetime\nairbnb_data_clean['last_scraped'] = pd.to_datetime(airbnb_data_clean['last_scraped'])\nairbnb_data_clean['host_since'] = pd.to_datetime(airbnb_data_clean['host_since'])\n\n# Impute missing values for numeric variables\nnumeric_cols = ['bathrooms', 'bedrooms', 'review_scores_cleanliness', \n                'review_scores_location', 'review_scores_value']\nairbnb_data_clean[numeric_cols] = airbnb_data_clean[numeric_cols].fillna(airbnb_data_clean[numeric_cols].median())\n\n# Replace missing values for categorical variables\nairbnb_data_clean['room_type'] = airbnb_data_clean['room_type'].fillna(airbnb_data_clean['room_type'].mode().iloc[0])\n\n# Drop rows with missing values in crucial columns\nairbnb_data_clean = airbnb_data_clean.dropna(subset=['host_since'])\n\n# Convert non-numeric columns to numeric\nairbnb_data_clean['room_type'] = airbnb_data_clean['room_type'].astype('category').cat.codes\nairbnb_data_clean['instant_bookable'] = airbnb_data_clean['instant_bookable'].astype('category').cat.codes\n\n# Exploratory Data Analysis (EDA)\n# Summary statistics\nsummary_stats = airbnb_data_clean.describe(include='all')\n\n# Visualizations \n# Histograms of numeric variables\nnumeric_cols = ['bathrooms', 'bedrooms', 'price', 'number_of_reviews']\nairbnb_data_clean[numeric_cols].hist(bins=20, figsize=(12, 8))\nplt.show()\n\n# Correlation matrix\ncorr_matrix = airbnb_data_clean[numeric_cols + ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']].corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Define the dependent and independent variables\ndependent_var = airbnb_data_clean['number_of_reviews']\nindependent_vars = airbnb_data_clean[['bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness',\n                                      'review_scores_location', 'review_scores_value', 'room_type', 'instant_bookable']]\n\n# Add intercept term  \nindependent_vars = sm.add_constant(independent_vars)\n\n# Split data into training and testing sets\n# For simplicity, you can use the entire dataset for demonstration\ntrain_data = independent_vars  \ntest_data = dependent_var\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(dependent_var, independent_vars, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                40593\nModel:                            GLM   Df Residuals:                    40584\nModel Family:                 Poisson   Df Model:                            8\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -6.9861e+05\nDate:                Sat, 11 May 2024   Deviance:                   1.2733e+06\nTime:                        09:01:45   Pearson chi2:                 2.05e+06\nNo. Iterations:                     7   Pseudo R-squ. (CS):             0.8046\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         5.6050      0.013    436.916      0.000       5.580       5.630\nbathrooms                    -0.1215      0.004    -31.927      0.000      -0.129      -0.114\nbedrooms                      0.0710      0.002     35.747      0.000       0.067       0.075\nprice                        -0.0003   1.09e-05    -23.524      0.000      -0.000      -0.000\nreview_scores_cleanliness     0.0377      0.001     26.009      0.000       0.035       0.041\nreview_scores_location       -0.1804      0.002   -117.128      0.000      -0.183      -0.177\nreview_scores_value          -0.1467      0.002    -82.747      0.000      -0.150      -0.143\nroom_type                    -0.1669      0.003    -66.441      0.000      -0.172      -0.162\ninstant_bookable              0.3335      0.003    116.055      0.000       0.328       0.339\n=============================================================================================\n\n\nwe can interpret the coefficients as follows:\nThe intercept coefficient (const) of 5.6050 represents the expected log count of reviews when all other variables are zero.\nThe coefficient for bathrooms is -0.1215, indicating that for every one-unit increase in the number of bathrooms, the expected log count of reviews decreases by 0.1215, holding all other variables constant.\nThe coefficient for bedrooms is 0.0710, suggesting that for every one-unit increase in the number of bedrooms, the expected log count of reviews increases by 0.0710, holding all other variables constant.\nThe coefficient for price is -0.0003, which means that for every $1 increase in the price, the expected log count of reviews decreases by 0.0003, holding all other variables constant.\nThe coefficient for review_scores_cleanliness is 0.0377, indicating that for every one-unit increase in the cleanliness review score, the expected log count of reviews increases by 0.0377, holding all other variables constant.\nThe coefficient for review_scores_location is -0.1804, suggesting that for every one-unit increase in the location review score, the expected log count of reviews decreases by 0.1804, holding all other variables constant.\nThe coefficient for review_scores_value is -0.1467, which means that for every one-unit increase in the value review score, the expected log count of reviews decreases by 0.1467, holding all other variables constant.\nThe coefficient for room_type is -0.1669, indicating that for every one-unit increase in the room type category (e.g., moving from a reference category to the next category), the expected log count of reviews decreases by 0.1669, holding all other variables constant.\nThe coefficient for instant_bookable is 0.3335, suggesting that for listings that are instantly bookable (compared to those that are not), the expected log count of reviews increases by 0.3335, holding all other variables constant.\nIt’s important to note that the interpretation of coefficients in Poisson regression models is in terms of the log count of the dependent variable (number_of_reviews), rather than the actual count itself. Additionally, you should consider the statistical significance of the coefficients (based on p-values or confidence intervals) when interpreting the results.\nThe model summary also provides other useful information, such as the number of observations, degrees of freedom, log-likelihood, deviance, and pseudo R-squared values, which can be used to assess the overall model fit and performance.\nThe Airbnb dataset consists of over 40,000 listings from New York City, scraped in March 2017. The analysis aimed to understand the factors influencing the number of reviews received by Airbnb listings, which can be considered a proxy for the number of bookings.\nAfter data cleaning and preprocessing, including handling missing values and converting data types, a Poisson regression model was fitted to model the number of reviews as a function of various independent variables.\nThe Poisson regression model revealed several significant factors influencing the number of reviews:\nBathrooms: An increase in the number of bathrooms was associated with a decrease in the expected log count of reviews, suggesting that listings with fewer bathrooms tend to receive more reviews.\nBedrooms: An increase in the number of bedrooms was associated with an increase in the expected log count of reviews, indicating that listings with more bedrooms tend to receive more reviews.\nPrice: Higher listing prices were associated with a decrease in the expected log count of reviews, suggesting that more affordable listings tend to receive more reviews.\nReview Scores: Higher cleanliness review scores were associated with an increase in the expected log count of reviews, while higher location and value review scores were associated with a decrease in the expected log count of reviews.\nRoom Type: Moving from one room type category to the next (e.g., from a reference category to the next category) was associated with a decrease in the expected log count of reviews.\nInstant Bookable: Listings that were instantly bookable were associated with an increase in the expected log count of reviews compared to those that were not instantly bookable.\nThe analysis also included exploratory data analysis, such as summary statistics, histograms of numeric variables, and a correlation matrix, which provided insights into the data distribution and relationships between variables.\nOverall, the Airbnb data analysis revealed several factors that influence the number of reviews received by listings, which can be used by hosts and the platform to optimize their offerings and improve the overall user experience."
  },
  {
    "objectID": "projects/Project 4/index.html",
    "href": "projects/Project 4/index.html",
    "title": "Project 4",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project 3/index.html#estimating-yogurt-preferences",
    "href": "projects/project 3/index.html#estimating-yogurt-preferences",
    "title": "Project 3",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\ntodo: import the data, maybe show the first few rows, and describe the data a bit.\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\nThe purpose of the code is to read a CSV file containing data about different yogurt products, reshape the data into a long format, and create new columns to indicate specific characteristics of the products, such as whether they are featured or their price. Maximum Likelihood Estimates (MLEs):\nThe MLEs represent the estimated coefficients obtained from some statistical estimation method, such as maximum likelihood estimation. These coefficients are essential for understanding the relationship between independent variables (features) and the dependent variable (outcome). Determining Most-Preferred and Least-Preferred Products:\nThe code first extracts the intercept coefficients for three different yogurt products (beta_1, beta_2, beta_3) from the MLEs. It then determines the most-preferred and least-preferred yogurt products by finding the maximum and minimum values among the intercept coefficients. The difference between the most-preferred and least-preferred product intercepts represents the utility difference. Converting Utility Difference to Dollar Value:\nThe utility difference is converted into a dollar value by dividing it by the negative price coefficient (beta_p). This conversion allows us to understand the monetary benefit associated with the difference in product utilities. Printing the Results:\nThe code prints out the most-preferred yogurt intercept, least-preferred yogurt intercept, utility difference, price coefficient (beta_p), and dollar benefit between the most-preferred and least-preferred yogurt products. Calculating Current Market Shares:\nAfter analyzing individual product preferences, the code calculates the current market shares of three yogurt products (yogurt1, yogurt2, yogurt3) based on the data available in the yogurt_data_long DataFrame. It calculates the mean (average) value of each column (yogurt1, yogurt2, yogurt3) across all rows, representing their respective market shares. Increasing Price of Yogurt 1:\nThe code increases the price of yogurt 1 by $0.10. It likely simulates a pricing scenario to observe its impact on market shares. Preprocessing Data with New Prices:\nThe data is preprocessed with the updated prices, likely involving preparing a feature matrix needed for further analysis. Calculating Utilities and Choice Probabilities with New Prices:\nUtilities of the three yogurt products with the new prices are calculated using estimated coefficients (result.x) from a logit model. Choice probabilities for each yogurt product are computed based on the calculated utilities using the logit formula. These probabilities are used to derive new market shares. Printing the New Market Shares:\nFinally, the code prints out the new market shares of the three yogurt products after increasing the price of yogurt 1 by $0.10.\nThe code integrates statistical analysis, data manipulation, and interpretation to provide insights into yogurt product preferences and market dynamics. The input is a CSV file named ‘yogurt_data.csv’ located in the ‘/home/jovyan/Desktop/new web 111/sheena_website 2 2/’ directory.\nThe output is a transformed pandas DataFrame named ‘yogurt_data_long’ containing the original data reshaped into a long format, along with new columns indicating whether each product is a yogurt (y1, y2, y3), whether it is featured (featured), and its price (price).\nHere’s how the code achieves its purpose:\nFirst, it reads the CSV file into a pandas DataFrame named ‘yogurt_data’ using pd.read_csv(). The header=None parameter indicates that the first row should not be used as column names, and skiprows=1 skips the first row entirely.\nNext, it renames the columns of the ‘yogurt_data’ DataFrame to more descriptive names using yogurt_data.columns = […].\nThen, it uses pd.melt() to reshape the data into a long format, where each row represents a single product-choice combination. The value_vars parameter specifies which columns to include in the melted DataFrame, var_name and value_name specify the names of the new columns created by melt().\nAfter that, it creates new columns in the ‘yogurt_data_long’ DataFrame to indicate whether each product is a yogurt (y1, y2, y3) by checking if the ‘product’ column contains the strings ‘y1’, ‘y2’, or ‘y3’, and converting the result to an integer (0 or 1).\nSimilarly, it creates a ‘featured’ column to indicate whether each product is featured by checking if the ‘product’ column contains the string ‘f’.\nIt also extracts the price information from the ‘product’ column using str.extract() and converts it to a float to create the ‘price’ column.\nFinally, it drops the ‘product’ and ‘choice’ columns from the ‘yogurt_data_long’ DataFrame, as they are no longer needed.\nThe key data transformations happening in this code are:\nReading the CSV file into a DataFrame Renaming the columns Reshaping the data from wide to long format using melt() Creating new columns based on string patterns in the ‘product’ column Extracting price information from the ‘product’ column Dropping unnecessary columns The code follows a logical flow of reading the data, reshaping it, and creating new columns based on the existing data to prepare it for further analysis or processing.\n\nimport pandas as pd\nimport numpy as np\n\nyogurt_data = pd.read_csv('/home/jovyan/Desktop/new web 111/sheena_website 2 2/yogurt_data.csv', header=None, skiprows=1)\nprint(yogurt_data.head())\n\nyogurt_data.columns = ['c', 'y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4']\n\n\nyogurt_data_long = pd.melt(yogurt_data, value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'], var_name='product', value_name='choice')\n\nyogurt_data_long['yogurt1'] = yogurt_data_long['product'].str.contains('y1').astype(int)\nyogurt_data_long['yogurt2'] = yogurt_data_long['product'].str.contains('y2').astype(int)\nyogurt_data_long['yogurt3'] = yogurt_data_long['product'].str.contains('y3').astype(int)\n\nyogurt_data_long['featured'] = yogurt_data_long['product'].str.contains('f').astype(int)\nyogurt_data_long['price'] = yogurt_data_long['product'].str.extract('p(\\d+)', expand=False).astype(float)\n\nyogurt_data_long = yogurt_data_long.drop(['product', 'choice'], axis=1)\n\nprint(yogurt_data_long.head())\n\n    0   1   2   3   4   5   6   7   8      9      10     11     12\n0  107   1   0   0   0   0   0   0   0  0.108  0.081  0.050  0.079\n1  108   0   1   0   0   0   0   0   0  0.115  0.081  0.043  0.079\n2  109   0   1   0   0   0   0   0   0  0.108  0.063  0.050  0.079\n3  110   1   0   0   0   0   0   0   0  0.108  0.051  0.050  0.079\n4  111   1   0   0   0   0   0   0   0  0.108  0.047  0.050  0.079\n   yogurt1  yogurt2  yogurt3  featured  price\n0        1        0        0         0    NaN\n1        1        0        0         0    NaN\n2        1        0        0         0    NaN\n3        1        0        0         0    NaN\n4        1        0        0         0    NaN\n\n\nThe purpose of the code is to reshape a wide-format DataFrame, where each column represents a different variable, into a long-format DataFrame, where each row represents a single observation, and the variable names are stored in a separate column.\nThe input is a wide-format pandas DataFrame called ‘df’, which has columns for an identifier (‘id’) and multiple variables (‘product1’, ‘product2’, ‘product3’, ‘other_products’).\nThe output is a new long-format DataFrame called ‘df_long’, which has columns for the identifier (‘id’), the variable name (‘variable’), and the value (‘value’). Additionally, it has three new columns (‘is_product1’, ‘is_product2’, ‘is_product3’) that indicate whether the original variable was ‘product1’, ‘product2’, or ‘product3’, respectively.\nThe code achieves its purpose through the following steps: a) It uses the pd.melt() function to reshape the wide-format DataFrame ‘df’ into a long-format DataFrame ‘df_long’. The ‘id_vars’ parameter specifies the columns to use as identifiers, and the ‘value_vars’ parameter specifies the columns to “unmelt” or reshape. b) It creates three new columns (‘is_product1’, ‘is_product2’, ‘is_product3’) in ‘df_long’ by comparing the ‘variable’ column with the strings ‘product1’, ‘product2’, and ‘product3’, respectively, using the eq() method. c) It drops the ‘variable’ column from ‘df_long’ since the information is now stored in the new columns.\nThe main data transformation happening is the reshaping of the DataFrame from wide to long format. This is a common operation in data analysis when working with datasets that have multiple variables stored in separate columns. The long format makes it easier to perform operations on all variables simultaneously or to analyze data across variables.\nThe code also creates additional columns to store information about the original variable names. This can be useful for later analysis or filtering operations based on the variable type.\n\nimport pandas as pd\n\n# Define a sample DataFrame\ndf = pd.DataFrame({\n    'id': [1, 2, 3, 4, 5],\n    'product1': [10, 20, 30, 40, 50],\n    'product2': [15, 25, 35, 45, 55],\n    'product3': [20, 30, 40, 50, 60],\n    'other_products': [25, 35, 45, 55, 65]\n})\n\n# Assuming you have a wide-format DataFrame 'df'\ndf_long = pd.melt(df, id_vars=['id'], value_vars=['product1', 'product2', 'product3', 'other_products'])\ndf_long = df_long.assign(is_product1=df_long['variable'].eq('product1'),\n                         is_product2=df_long['variable'].eq('product2'),\n                         is_product3=df_long['variable'].eq('product3'))\ndf_long = df_long.drop('variable', axis=1)\nprint(df_long)\n\n    id  value  is_product1  is_product2  is_product3\n0    1     10         True        False        False\n1    2     20         True        False        False\n2    3     30         True        False        False\n3    4     40         True        False        False\n4    5     50         True        False        False\n5    1     15        False         True        False\n6    2     25        False         True        False\n7    3     35        False         True        False\n8    4     45        False         True        False\n9    5     55        False         True        False\n10   1     20        False        False         True\n11   2     30        False        False         True\n12   3     40        False        False         True\n13   4     50        False        False         True\n14   5     60        False        False         True\n15   1     25        False        False        False\n16   2     35        False        False        False\n17   3     45        False        False        False\n18   4     55        False        False        False\n19   5     65        False        False        False\n\n\nThe code loglik is a function that calculates the negative log-likelihood for a multinomial logit model. The purpose of this function is to evaluate how well the model’s parameters fit the given data.\nThe function takes three inputs:\nparams: A numpy array containing the model’s parameter values (beta_1, beta_2, beta_3, beta_f, beta_p). data: A pandas DataFrame containing the original data. feature_matrix: A pandas DataFrame containing the feature matrix. The output of the function is a single float value representing the negative log-likelihood.\nHere’s how the function achieves its purpose:\nIt calculates the utilities (V) using the calculate_utilities function (not shown in the provided code) with the params and feature_matrix as inputs. It calculates the choice probabilities (P) by taking the exponential of the utilities (V) and dividing each element by the sum of the exponentials. It finds the probabilities of the chosen alternatives (chosen_probs) by selecting the probabilities corresponding to the maximum values in the ‘yogurt1’, ‘yogurt2’, and ‘yogurt3’ columns of the data DataFrame. It calculates the log-likelihood by summing the natural logarithms of the chosen probabilities. Finally, it returns the negative of the log-likelihood value. The key logic flow in this function is the calculation of choice probabilities using the utilities and the exponential function. This is a common approach in multinomial logit models, where the probabilities are derived from the utilities through a softmax-like transformation. The function also performs data transformations by selecting the chosen probabilities from the data DataFrame and taking the natural logarithm of those probabilities.\nOverall, the loglik function is a crucial component in evaluating the fit of a multinomial logit model by calculating the negative log-likelihood, which is a measure of how well the model’s parameters explain the observed data.\n\ndef loglik(params: np.ndarray, data: pd.DataFrame, feature_matrix: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Log-likelihood function for the multinomial logit model.\n\n    Args:\n        params (numpy.ndarray): The parameter values (beta_1, beta_2, beta_3, beta_f, beta_p).\n        data (pandas.DataFrame): The original data frame.\n        feature_matrix (pandas.DataFrame): The feature matrix.\n\n    Returns:\n        float: The negative log-likelihood value.\n    \"\"\"\n    # Calculate the utilities\n    V = calculate_utilities(params, feature_matrix)\n\n    # Calculate the choice probabilities\n    exp_V = np.exp(V)\n    if exp_V.ndim == 1:\n        sum_exp_V = exp_V.sum(keepdims=True)\n    else:\n        sum_exp_V = exp_V.sum(axis=1, keepdims=True)\n    P = exp_V / sum_exp_V\n\n    # Calculate the log-likelihood\n    chosen_probs = P[data[['yogurt1', 'yogurt2', 'yogurt3']].values.argmax(axis=1)]\n    log_likelihood = np.log(chosen_probs).sum()\n\n    return -log_likelihood\n\nPurpose: The purpose of this code is to estimate the parameters (coefficients) of a multinomial logit model, which is a type of statistical model used to analyze discrete choice data. In this case, the data seems to be related to consumer preferences for different yogurt products.\nInput: The code takes a pandas DataFrame called yogurt_data_long as input. This DataFrame likely contains information about different yogurt products, such as their prices and other attributes, as well as the choices made by consumers.\nOutput: The code outputs the estimated values of the model parameters (coefficients), which are denoted as beta_1, beta_2, beta_3, beta_f, and beta_p. These parameters represent the effects of different factors (e.g., product type, price, featured status) on the probability of a consumer choosing a particular yogurt product.\nHow it achieves its purpose: a) The code starts by preprocessing the data using the preprocess_data function. This function creates dummy variables for the yogurt products and adds a ‘price’ column and a ‘featured’ column to the feature matrix. b) The calculate_utilities function calculates the utilities (a measure of preference or satisfaction) for each yogurt product based on the feature matrix and the model parameters. c) The loglik function calculates the log-likelihood, which is a measure of how well the model fits the data. This function uses the utilities calculated in the previous step to compute the choice probabilities and then computes the log-likelihood based on the observed choices in the data. d) The code initializes the model parameters with some starting values and then uses the minimize function from the scipy.optimize module to find the parameter values that maximize the log-likelihood (i.e., provide the best fit to the data). e) Finally, the code prints out the estimated parameter values (Maximum Likelihood Estimates) for the multinomial logit model.\nImportant logic flows and data transformations: a) The preprocess_data function creates dummy variables for the yogurt products, which allows the model to account for the different product types. b) The calculate_utilities function computes the utilities by taking a linear combination of the feature matrix and the model parameters. c) The loglik function calculates the choice probabilities using the utilities and the exponential function, and then computes the log-likelihood based on the observed choices in the data. d) The minimize function from scipy.optimize is used to find the parameter values that maximize the log-likelihood, which is a common optimization problem in statistical modeling.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom typing import Tuple\n\ndef preprocess_data(data: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Preprocess the data by creating dummy variables and feature columns.\n\n    Args:\n        data (pandas.DataFrame): The input data frame.\n\n    Returns:\n        Tuple[pandas.DataFrame, pandas.DataFrame]: The preprocessed feature matrix and the original data frame.\n    \"\"\"\n    # Create dummy variables for products\n    feature_matrix = pd.get_dummies(data[['yogurt1', 'yogurt2', 'yogurt3']], drop_first=True)\n    print(\"\\nFeature Matrix:\\n\", feature_matrix)\n\n    # Add price column\n    feature_matrix['price'] = data['price']\n\n    # Create a featured column based on the price column\n    feature_matrix['featured'] = data['price'].apply(lambda x: 1 if x in [15, 25, 35, 45, 55] else 0)\n\n    return feature_matrix, data\n\ndef calculate_utilities(params: np.ndarray, feature_matrix: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the utilities based on the feature matrix and parameters.\n\n    Args:\n        params (numpy.ndarray): The parameter values (beta_1, beta_2, beta_3, beta_f, beta_p).\n        feature_matrix (pandas.DataFrame): The feature matrix.\n\n    Returns:\n        numpy.ndarray: The calculated utilities.\n    \"\"\"\n    utilities = np.dot(feature_matrix, params)\n    print(\"\\nUtilities:\\n\", utilities)\n    return utilities\n\ndef loglik(params: np.ndarray, data: pd.DataFrame, feature_matrix: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Log-likelihood function for the multinomial logit model.\n\n    Args:\n        params (numpy.ndarray): The parameter values (beta_1, beta_2, beta_3, beta_f, beta_p).\n        data (pandas.DataFrame): The original data frame.\n        feature_matrix (pandas.DataFrame): The feature matrix.\n\n    Returns:\n        float: The negative log-likelihood value.\n    \"\"\"\n    # Calculate the utilities\n    V = calculate_utilities(params, feature_matrix)\n\n    # Calculate the choice probabilities\n    exp_V = np.exp(V)\n    if exp_V.ndim == 1:\n        sum_exp_V = exp_V.sum(keepdims=True)\n    else:\n        sum_exp_V = exp_V.sum(axis=1, keepdims=True)\n    P = exp_V / sum_exp_V\n    print(\"\\nChoice Probabilities:\\n\", P)\n\n    # Calculate the log-likelihood\n    chosen_probs = P[data[['yogurt1', 'yogurt2', 'yogurt3']].values.argmax(axis=1)]\n    log_likelihood = np.log(chosen_probs).sum()\n    print(\"\\nLog-likelihood:\", -log_likelihood)\n\n    return -log_likelihood\n\n# Preprocess the data\nfeature_matrix, yogurt_data_long = preprocess_data(yogurt_data_long)\n\n# Initial parameter values\ninitial_params = np.array([0.5, 0.5, 0.5, 0.1, -1.0])\nprint(\"\\nInitial Parameters:\", initial_params)\n\n# Optimize the log-likelihood function\nresult = minimize(loglik, initial_params, args=(yogurt_data_long, feature_matrix), method='Nelder-Mead')\n\n# Print the MLEs\nprint(\"\\nMaximum Likelihood Estimates:\")\nprint(\"beta_1 (Product 1 intercept):\", result.x[0])\nprint(\"beta_2 (Product 2 intercept):\", result.x[1])\nprint(\"beta_3 (Product 3 intercept):\", result.x[2])\nprint(\"beta_f (Featured coefficient):\", result.x[3])\nprint(\"beta_p (Price coefficient):\", result.x[4])\n\n\nFeature Matrix:\n        yogurt1  yogurt2  yogurt3\n0            1        0        0\n1            1        0        0\n2            1        0        0\n3            1        0        0\n4            1        0        0\n...        ...      ...      ...\n27883        0        0        0\n27884        0        0        0\n27885        0        0        0\n27886        0        0        0\n27887        0        0        0\n\n[27888 rows x 3 columns]\n\nInitial Parameters: [ 0.5  0.5  0.5  0.1 -1. ]\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [ nan  nan  nan ... 0.42 0.42 0.42]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [  nan   nan   nan ... 0.408 0.408 0.408]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [  nan   nan   nan ... 0.402 0.402 0.402]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [ nan  nan  nan ... 0.41 0.41 0.41]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [  nan   nan   nan ... 0.404 0.404 0.404]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [  nan   nan   nan ... 0.401 0.401 0.401]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [  nan   nan   nan ... 0.405 0.405 0.405]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [  nan   nan   nan ... 0.402 0.402 0.402]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [   nan    nan    nan ... 0.4005 0.4005 0.4005]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [   nan    nan    nan ... 0.4025 0.4025 0.4025]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [  nan   nan   nan ... 0.401 0.401 0.401]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [    nan     nan     nan ... 0.40025 0.40025 0.40025]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [    nan     nan     nan ... 0.40125 0.40125 0.40125]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [   nan    nan    nan ... 0.4005 0.4005 0.4005]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [     nan      nan      nan ... 0.400125 0.400125 0.400125]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [     nan      nan      nan ... 0.400625 0.400625 0.400625]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [    nan     nan     nan ... 0.40025 0.40025 0.40025]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [      nan       nan       nan ... 0.4000625 0.4000625 0.4000625]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [      nan       nan       nan ... 0.4003125 0.4003125 0.4003125]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [     nan      nan      nan ... 0.400125 0.400125 0.400125]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40003125 0.40003125 0.40003125]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40015625 0.40015625 0.40015625]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [      nan       nan       nan ... 0.4000625 0.4000625 0.4000625]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40001562 0.40001562 0.40001562]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40007813 0.40007813 0.40007813]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40003125 0.40003125 0.40003125]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000781 0.40000781 0.40000781]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40003906 0.40003906 0.40003906]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40001562 0.40001562 0.40001562]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000391 0.40000391 0.40000391]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40001953 0.40001953 0.40001953]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000781 0.40000781 0.40000781]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000195 0.40000195 0.40000195]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000977 0.40000977 0.40000977]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000391 0.40000391 0.40000391]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000098 0.40000098 0.40000098]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000488 0.40000488 0.40000488]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000195 0.40000195 0.40000195]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000049 0.40000049 0.40000049]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000244 0.40000244 0.40000244]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000098 0.40000098 0.40000098]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000024 0.40000024 0.40000024]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000122 0.40000122 0.40000122]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000049 0.40000049 0.40000049]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000012 0.40000012 0.40000012]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000061 0.40000061 0.40000061]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000024 0.40000024 0.40000024]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000006 0.40000006 0.40000006]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000031 0.40000031 0.40000031]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000012 0.40000012 0.40000012]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000003 0.40000003 0.40000003]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000015 0.40000015 0.40000015]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000006 0.40000006 0.40000006]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000002 0.40000002 0.40000002]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000008 0.40000008 0.40000008]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000003 0.40000003 0.40000003]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000001 0.40000001 0.40000001]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000004 0.40000004 0.40000004]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000002 0.40000002 0.40000002]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000002 0.40000002 0.40000002]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000001 0.40000001 0.40000001]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [       nan        nan        nan ... 0.40000001 0.40000001 0.40000001]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nChoice Probabilities:\n [nan nan nan ... nan nan nan]\n\nLog-likelihood: nan\n\nMaximum Likelihood Estimates:\nbeta_1 (Product 1 intercept): 0.5\nbeta_2 (Product 2 intercept): 0.5\nbeta_3 (Product 3 intercept): 0.5\nbeta_f (Featured coefficient): 0.1\nbeta_p (Price coefficient): -1.0\n\n\nFinding the maximum and minimum values from a set of three values (beta_1, beta_2, beta_3) to determine the most-preferred and least-preferred products. Calculating the difference between the most-preferred and least-preferred product utilities (utility_difference). Converting the utility_difference into a dollar value (dollar_value) by dividing it by the negative price coefficient (-beta_p). The code assumes that the input values (result.x array) are already calculated and available. It performs a series of simple arithmetic operations and comparisons to extract the relevant information and present the results in a human-readable format.\n\n# Assuming the MLEs are already calculated and stored in the result.x array\n# Extract the MLEs\nbeta_1 = result.x[0]  # Product 1 intercept\nbeta_2 = result.x[1]  # Product 2 intercept\nbeta_3 = result.x[2]  # Product 3 intercept\nbeta_f = result.x[3]  # Featured coefficient\nbeta_p = result.x[4]  # Price coefficient\n\n# Find the most-preferred and least-preferred yogurts\nmost_preferred_intercept = max(beta_1, beta_2, beta_3)\nleast_preferred_intercept = min(beta_1, beta_2, beta_3)\n\n# Calculate the utility difference\nutility_difference = most_preferred_intercept - least_preferred_intercept\n\n# Convert utility difference to dollar value using the price coefficient\ndollar_value = utility_difference / (-beta_p)\n\n# Print the results\nprint(\"Most-preferred yogurt intercept:\", most_preferred_intercept)\nprint(\"Least-preferred yogurt intercept:\", least_preferred_intercept)\nprint(\"Utility difference:\", utility_difference)\nprint(\"Price coefficient (beta_p):\", beta_p)\nprint(\"Dollar benefit between most-preferred and least-preferred yogurt: $%.2f per unit\" % dollar_value)\n\nMost-preferred yogurt intercept: 0.5\nLeast-preferred yogurt intercept: 0.5\nUtility difference: 0.0\nPrice coefficient (beta_p): -1.0\nDollar benefit between most-preferred and least-preferred yogurt: $0.00 per unit\n\n\nthe current market shares of three different yogurt products based on the data available in the yogurt_data_long DataFrame.\nThe code takes the yogurt_data_long DataFrame as input, which is assumed to contain columns named ‘yogurt1’, ‘yogurt2’, and ‘yogurt3’. These columns likely represent the sales or consumption data for the three yogurt products.\nThe first line current_shares = yogurt_data_long[[‘yogurt1’, ‘yogurt2’, ‘yogurt3’]].mean() calculates the mean (average) value of each column (‘yogurt1’, ‘yogurt2’, and ‘yogurt3’) across all rows in the DataFrame. The resulting current_shares variable is a Pandas Series containing the average values for each yogurt product, representing their respective market shares.\nThe subsequent lines print out the calculated market shares for each yogurt product. The print statements display the text “Current Market Shares:” followed by the market share value for each yogurt product, labeled as “Yogurt 1:”, “Yogurt 2:”, and “Yogurt 3:”.\nThe output of this code will be the current market share values for the three yogurt products, displayed in a readable format on the console or output stream.\nThe code achieves its purpose by leveraging the Pandas library’s functionality to calculate the mean (average) value of specific columns in a DataFrame. It assumes that the data in the ‘yogurt1’, ‘yogurt2’, and ‘yogurt3’ columns represents the sales or consumption data for the respective yogurt products, and that the average of these values can be interpreted as the market share for each product. The lambda function adds 0.1 to the price if the price is one of the values [10, 20, 30, 40, 50], which likely correspond to the prices of yogurt 1. It preprocesses the data with the new prices using the preprocess_data function, which is not shown in the code snippet. It calculates the utilities (V_new) of the three yogurt products with the new prices using the calculate_utilities function and the estimated coefficients (result.x) from the logit model. It calculates the choice probabilities (P_new) for each yogurt product using the utilities (V_new) and the logit formula: P = exp(V) / sum(exp(V)). It calculates the new market shares by taking the mean of the choice probabilities for each yogurt product. Finally, it prints the current and new market shares for the three yogurt products. The code involves data transformations, such as updating the prices in the DataFrame and calculating utilities and choice probabilities based on the logit model. The logic flow involves preprocessing the data, calculating utilities and choice probabilities, and then aggregating the choice probabilities to obtain the market shares.TT\n\n# Calculate the current market shares\ncurrent_shares = yogurt_data_long[['yogurt1', 'yogurt2', 'yogurt3']].mean()\nprint(\"Current Market Shares:\")\nprint(\"Yogurt 1:\", current_shares['yogurt1'])\nprint(\"Yogurt 2:\", current_shares['yogurt2'])\nprint(\"Yogurt 3:\", current_shares['yogurt3'])\n\n# Increase the price of yogurt 1 by $0.10\nyogurt_data_long['price'] = yogurt_data_long['price'].apply(lambda x: x + 0.1 if x in [10, 20, 30, 40, 50] else x)\n\n# Preprocess the data with the new prices\nfeature_matrix_new, _ = preprocess_data(yogurt_data_long)\n\n# Calculate the utilities with the new prices\nV_new = calculate_utilities(result.x, feature_matrix_new)\n\n# Calculate the choice probabilities with the new prices\nexp_V_new = np.exp(V_new)\nif exp_V_new.ndim == 1:\n    sum_exp_V_new = exp_V_new.sum(keepdims=True)\n    P_new = exp_V_new / sum_exp_V_new\nelse:\n    sum_exp_V_new = exp_V_new.sum(axis=1, keepdims=True)\n    P_new = exp_V_new / sum_exp_V_new\n\n# Calculate the new market shares\nif P_new.ndim == 1:\n    new_yogurt1_share = P_new[0]\n    new_yogurt2_share = P_new[1]\n    new_yogurt3_share = P_new[2]\nelse:\n    new_yogurt1_share = P_new[:, 0].mean()\n    new_yogurt2_share = P_new[:, 1].mean()\n    new_yogurt3_share = P_new[:, 2].mean()\n\nprint(\"\\nNew Market Shares (after increasing the price of yogurt 1 by $0.10):\")\nprint(\"Yogurt 1:\", new_yogurt1_share)\nprint(\"Yogurt 2:\", new_yogurt2_share)\nprint(\"Yogurt 3:\", new_yogurt3_share)\n\nCurrent Market Shares:\nYogurt 1: 0.08333333333333333\nYogurt 2: 0.08333333333333333\nYogurt 3: 0.08333333333333333\n\nFeature Matrix:\n        yogurt1  yogurt2  yogurt3\n0            1        0        0\n1            1        0        0\n2            1        0        0\n3            1        0        0\n4            1        0        0\n...        ...      ...      ...\n27883        0        0        0\n27884        0        0        0\n27885        0        0        0\n27886        0        0        0\n27887        0        0        0\n\n[27888 rows x 3 columns]\n\nUtilities:\n [nan nan nan ... 0.4 0.4 0.4]\n\nNew Market Shares (after increasing the price of yogurt 1 by $0.10):\nYogurt 1: nan\nYogurt 2: nan\nYogurt 3: nan"
  },
  {
    "objectID": "projects/project3/index.html#log-likelihood-function-for-mnl-model",
    "href": "projects/project3/index.html#log-likelihood-function-for-mnl-model",
    "title": "Project 3",
    "section": "Log-Likelihood Function for MNL Model",
    "text": "Log-Likelihood Function for MNL Model\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the MNL model\ndef log_likelihood(beta, data):\n    # Extract parameters\n    beta1, beta2, beta3, beta_f, beta_p = beta\n    \n    # Compute the utility for each product\n    U1 = beta1 * data['product_1'] + beta_f * data['featured'] + beta_p * data['price']\n    U2 = beta2 * data['product_2'] + beta_f * data['featured'] + beta_p * data['price']\n    U3 = beta3 * data['product_3'] + beta_f * data['featured'] + beta_p * data['price']\n    U4 = 0 * data['product_1'] + beta_f * data['featured'] + beta_p * data['price']  # Reference product\n    \n    # Compute the probability for each product\n    exp_U1 = np.exp(U1)\n    exp_U2 = np.exp(U2)\n    exp_U3 = np.exp(U3)\n    exp_U4 = np.exp(U4)\n    sum_exp_U = exp_U1 + exp_U2 + exp_U3 + exp_U4\n    \n    P1 = exp_U1 / sum_exp_U\n    P2 = exp_U2 / sum_exp_U\n    P3 = exp_U3 / sum_exp_U\n    P4 = exp_U4 / sum_exp_U\n    \n    # Compute the log-likelihood\n    ll = np.sum(data['chosen'] * np.log(P1 * data['product_1'] + P2 * data['product_2'] + \n                                        P3 * data['product_3'] + P4 * (1 - data['product_1'] - data['product_2'] - data['product_3'])))\n    return -ll  # Negative log-likelihood for minimization\n\n# Initial parameter estimates (guesses)\ninitial_beta = np.array([0, 0, 0, 0, -1])\n\n# Perform the optimization to find the MLEs\nresult = minimize(log_likelihood, initial_beta, args=(yogurt_long,))\n\n# Display the estimated parameters\nresult.x\n\narray([ 1.94434698e+01,  1.97028791e+01,  1.93234092e+01,  3.98715987e-05,\n       -1.00027615e+00])"
  },
  {
    "objectID": "projects/project3/index.html#define-the-log-likelihood-function",
    "href": "projects/project3/index.html#define-the-log-likelihood-function",
    "title": "Project 3",
    "section": "Define the Log-Likelihood Function",
    "text": "Define the Log-Likelihood Function\nWe first defined a log-likelihood function for the MNL model. This function computes the probability of each consumer choosing each of the four yogurt products based on the utility derived from the product features (intercepts for products 1-3, whether the product was featured, and its price). The utility for each product was modeled as a linear function of these features, with independent parameters 𝛽. ## Negative Log-Likelihood: Since minimize seeks to minimize a function, we used the negative of the log-likelihood. This means that minimizing the negative log-likelihood is equivalent to maximizing the log-likelihood."
  },
  {
    "objectID": "projects/project3/index.html#initial-guess",
    "href": "projects/project3/index.html#initial-guess",
    "title": "Project 3",
    "section": "Initial Guess",
    "text": "Initial Guess\nWe provided an initial guess for the parameters β1=0, β2=0, β3=0, 𝛽𝑓=0 and 𝛽𝑝=−1"
  },
  {
    "objectID": "projects/project3/index.html#optimization",
    "href": "projects/project3/index.html#optimization",
    "title": "Project 3",
    "section": "Optimization",
    "text": "Optimization\nThe minimize function was then called with the negative log-likelihood function, the initial parameter estimates, and the data. It iteratively adjusted the parameters to find the set that minimizes the negative log-likelihood."
  },
  {
    "objectID": "projects/project3/index.html#result",
    "href": "projects/project3/index.html#result",
    "title": "Project 3",
    "section": "Result",
    "text": "Result\nThe output from the optimization process gave us the estimated values for the parameters. These were: β1= 19.1352 for the intercept of product 1, β2= 19.3886 for the intercept of product 2, β3= 19.0173 for the intercept of product 3, 𝛽𝑓= −0.00033 for the coefficient of the featured variable, 𝛽𝑝= −0.9996 for the coefficient of the price variable.\nAmong these, Product 2 has the highest intercept (β2) indicating it is the most preferred yogurt when other factors are equal. Product 3 has the lowest intercept (β3) making it the least preferred under the same conditions.\n\nDiscussion"
  },
  {
    "objectID": "projects/project3/index.html#dollar-per-util-conversion-and-brand-value",
    "href": "projects/project3/index.html#dollar-per-util-conversion-and-brand-value",
    "title": "Project 3",
    "section": "Dollar-Per-Util Conversion and Brand Value",
    "text": "Dollar-Per-Util Conversion and Brand Value\n\n# Calculate the dollar-per-util conversion factor\ndollar_per_util = 1 / abs(result.x[4])\n\n# Calculate the dollar benefit between the most-preferred and least-preferred yogurt\ndollar_benefit = (result.x[1] - result.x[2]) * dollar_per_util\n\ndollar_per_util, dollar_benefit\n\n(0.999723927647949, 0.37936511645444987)\n\n\nThe estimated price coefficient (βₚ = -0.9996) can be interpreted as the change in utility for a one-unit increase in price (per oz). To convert this into a dollar-per-util conversion factor, you use the inverse of this coefficient:\nDollar per util=1 / ∣𝛽𝑝∣\nUsing this conversion, we calculate the dollar benefit between the most and least preferred yogurts:\nDollar benefit = (𝛽2−𝛽3) × Dollar per util\nLet’s compute these values.\nThe dollar-per-util conversion factor is approximately 1.00041.0004. Using this factor, the monetary benefit between the most-preferred yogurt (Product 2) and the least preferred yogurt (Product 3) is approximately $0.37$0.37 per ounce. This amount represents the additional value consumers place on Product 2 over Product 3 purely based on brand preference."
  },
  {
    "objectID": "projects/project3/index.html#market-share-simulation-with-price-change",
    "href": "projects/project3/index.html#market-share-simulation-with-price-change",
    "title": "Project 3",
    "section": "Market Share Simulation with Price Change",
    "text": "Market Share Simulation with Price Change\n\ndef calculate_market_shares(data, beta):\n    # Extract parameters\n    beta1, beta2, beta3, beta_f, beta_p = beta\n    \n    # Compute the utility for each product using the original data\n    U1 = beta1 * data['product_1'] + beta_f * data['featured'] + beta_p * data['price']\n    U2 = beta2 * data['product_2'] + beta_f * data['featured'] + beta_p * data['price']\n    U3 = beta3 * data['product_3'] + beta_f * data['featured'] + beta_p * data['price']\n    U4 = 0 * data['product_1'] + beta_f * data['featured'] + beta_p * data['price']  # Reference product\n    \n    # Compute the probability for each product\n    exp_U1 = np.exp(U1)\n    exp_U2 = np.exp(U2)\n    exp_U3 = np.exp(U3)\n    exp_U4 = np.exp(U4)\n    sum_exp_U = exp_U1 + exp_U2 + exp_U3 + exp_U4\n    \n    P1 = exp_U1 / sum_exp_U\n    P2 = exp_U2 / sum_exp_U\n    P3 = exp_U3 / sum_exp_U\n    P4 = exp_U4 / sum_exp_U\n    \n    # Compute the average market shares\n    market_shares = [P1.mean(), P2.mean(), P3.mean(), P4.mean()]\n    return market_shares\n\n# Compute the initial market shares\ninitial_market_shares = calculate_market_shares(yogurt_long, result.x)\n\n# Increase the price of yogurt 1 by $0.10 and compute new market shares\nyogurt_long_adjusted = yogurt_long.copy()\nyogurt_long_adjusted.loc[yogurt_long['product'] == 1, 'price'] += 0.10\nadjusted_market_shares = calculate_market_shares(yogurt_long_adjusted, result.x)\n\ninitial_market_shares, adjusted_market_shares\n\n([0.3124999990102917,\n  0.31249999983193066,\n  0.31249999855157956,\n  0.06250000260619806],\n [0.3124999990102917,\n  0.31249999983193066,\n  0.31249999855157956,\n  0.06250000260619806])\n\n\nIt appears that the market shares did not change when increasing the price of yogurt 1 by $0.10 per ounce in the simulation. This suggests that the adjustment in price was not captured correctly in the utility calculations or that the high intercepts significantly overshadowed the price effect.\nTypically, an increase in the price of a product should lead to a decrease in its market share, especially if other products remain competitively priced. Let’s double-check and ensure that the price change is properly applied and that the sensitivities in the model reflect typical consumer behavior more clearly. ​\nIt appears there might be a deeper issue in how the model or the adjustments are being applied, as the market shares remain unchanged even after the price increase for yogurt 1. This outcome could be due to several reasons, such as the very high intercepts significantly overshadowing the price changes or potential issues in the simulation logic.\nIn a typical scenario, increasing the price of a product should decrease its market share, as consumers opt for more competitively priced alternatives. The current model results suggest that either the intercepts or other factors are dominating the choice probabilities to an extent that price changes are not impacting the outcomes as expected.\n\n# Correct the approach to calculate new market shares with price adjustment\n\n# Apply price change to yogurt 1\nyogurt_long_adjusted_price = yogurt_long.copy()\nyogurt_long_adjusted_price.loc[yogurt_long_adjusted_price['product'] == 1, 'price'] += 0.10\n\n# Recalculate the utilities and probabilities with adjusted prices\nadjusted_market_shares_corrected = calculate_market_shares(yogurt_long_adjusted_price, result.x)\n\ninitial_market_shares, adjusted_market_shares_corrected\n\n([0.3124999990102917,\n  0.31249999983193066,\n  0.31249999855157956,\n  0.06250000260619806],\n [0.3124999990102917,\n  0.31249999983193066,\n  0.31249999855157956,\n  0.06250000260619806])\n\n\n\n2. Estimating Minivan Preferences\n\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'rintro-chapter13conjoint.csv'\ndf = pd.read_csv(file_path)\n\n# Display the first few rows and summary of the dataset\ndf.head()\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n\n\n\n\n\n\n# Calculate the total number of respondents\ntotal_respondents = df['resp.id'].nunique()\nprint(\"Total number of respondents: \")\nprint(total_respondents)\n\n# Calculate the number of choice tasks each respondent completed\nchoice_tasks_per_respondent = df.groupby('resp.id')['ques'].nunique()\nprint(\"Number of Choice tasks each respondent completed: \")\nprint(choice_tasks_per_respondent.describe())\n\n# Calculate the number of alternatives presented in each choice task\nalternatives_per_task = df.groupby(['resp.id', 'ques'])['alt'].nunique()\nprint(\"Number of alternatives presented in each choice task\")\nprint(alternatives_per_task.describe())\n\n# total_respondents, choice_tasks_per_respondent.describe(), alternatives_per_task.describe()\n\nTotal number of respondents: \n200\nNumber of Choice tasks each respondent completed: \ncount    200.0\nmean      15.0\nstd        0.0\nmin       15.0\n25%       15.0\n50%       15.0\n75%       15.0\nmax       15.0\nName: ques, dtype: float64\nNumber of alternatives presented in each choice task\ncount    3000.0\nmean        3.0\nstd         0.0\nmin         3.0\n25%         3.0\n50%         3.0\n75%         3.0\nmax         3.0\nName: alt, dtype: float64"
  },
  {
    "objectID": "projects/project3/index.html#model",
    "href": "projects/project3/index.html#model",
    "title": "Project 3",
    "section": "Model",
    "text": "Model\n\nfrom statsmodels.formula.api import mnlogit\nformula = 'choice ~ carpool + C(seat) + C(cargo) + C(eng) + price'\nmodel = mnlogit(formula, data=df).fit()\n\nmodel.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.558661\n         Iterations 6\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nchoice\nNo. Observations:\n9000\n\n\nModel:\nMNLogit\nDf Residuals:\n8992\n\n\nMethod:\nMLE\nDf Model:\n7\n\n\nDate:\nSun, 19 May 2024\nPseudo R-squ.:\n0.1223\n\n\nTime:\n10:55:00\nLog-Likelihood:\n-5028.0\n\n\nconverged:\nTrue\nLL-Null:\n-5728.6\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.970e-298\n\n\n\n\n\n\nchoice=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n4.0952\n0.218\n18.809\n0.000\n3.668\n4.522\n\n\ncarpool[T.yes]\n0.0086\n0.053\n0.162\n0.872\n-0.096\n0.113\n\n\nC(seat)[T.7]\n-0.5248\n0.060\n-8.800\n0.000\n-0.642\n-0.408\n\n\nC(seat)[T.8]\n-0.2931\n0.059\n-5.009\n0.000\n-0.408\n-0.178\n\n\nC(cargo)[T.3ft]\n0.4385\n0.049\n9.004\n0.000\n0.343\n0.534\n\n\nC(eng)[T.gas]\n1.4347\n0.062\n23.217\n0.000\n1.314\n1.556\n\n\nC(eng)[T.hyb]\n0.6742\n0.063\n10.715\n0.000\n0.551\n0.798\n\n\nprice\n-0.1591\n0.006\n-25.617\n0.000\n-0.171\n-0.147\n\n\n\n\n\nIntercept (4.0952): This is the baseline utility for a minivan when all predictors are at their base levels. The positive value indicates a general preference for the default choice in the absence of other attributes.\nCarpool (yes) (0.0086): This coefficient is near zero and not statistically significant (p-value = 0.872), indicating that the carpool factor (yes vs. no) does not significantly impact the choice of minivan. Respondents’ preferences are not swayed by whether the minivan is used for carpooling.\nSeats (7) (-0.5248): This negative coefficient indicates a significant decrease in preference for minivans with 7 seats compared to the base level of 6 seats. The respondents clearly prefer fewer seats or find 7-seat configurations less appealing.\nSeats (8) (-0.2931): Similar to the 7-seat configuration, there is a negative preference for 8 seats compared to 6 seats, but the aversion is less intense than for 7 seats. This suggests a gradient in decreasing preference as the number of seats increases.\nCargo (3ft) (0.4385): The positive coefficient here shows a clear preference for minivans with 3ft of cargo space over those with 2ft. More cargo space increases the utility or attractiveness of a minivan, as indicated by this significant coefficient.\nEngine (Gas) (1.4347): A strongly positive coefficient indicating a significant preference for gas engines over the baseline (electric). This suggests that respondents favor the performance or familiarity of gas engines.\nEngine (Hybrid) (0.6742): Also positive, this coefficient indicates a preference for hybrid engines over electric ones, but not as strongly as for gas engines. It suggests an interest in more sustainable options while still valuing traditional engine performance.\nPrice (-0.1591): This negative coefficient demonstrates sensitivity to price; as the price of the minivan increases, its utility decreases, making it less likely to be chosen. The magnitude suggests a strong price sensitivity among respondents.\n\nprice_coef = 1 / model.params.iloc[-1].values[0]\nprint('Price coefficient in utils: ', price_coef)\n\nPrice coefficient in utils:  -6.2838236131875895\n\n\n\nutility_diff = 0.4386\nprint(\"The Utility difference is: \", utility_diff)\n\nThe Utility difference is:  0.4386\n\n\n\nbenefit = utility_diff * price_coef * 1000\nprint(f\"The Estimated dollar benefit of additional cargo space: ${benefit:.2f}\")\n\nThe Estimated dollar benefit of additional cargo space: $-2756.09\n\n\n\nparam_names = ['Featured', 'Price', 'Brand 1 Intercept', 'Brand 2 Intercept', 'Brand 3 Intercept']\n\n\ndata = {\n    'Minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat': [7, 6, 8, 7, 6, 7],\n    'cargo': [2, 2, 2, 3, 2, 2],\n    'eng': ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    'price': [30, 30, 30, 40, 40, 35]\n\n}\ndf_predict = pd.DataFrame(data)\ndf_predict['cargo'] = df_predict['cargo'].map({2: '2ft', 3: '3ft'})\nif 'carpool' not in df_predict.columns:\n    df_predict['carpool'] = 'no'\n\ntraining_features = model.params.index\npredict_features = pd.get_dummies(df_predict)\nprint(model.params.index)\nprint(df_predict)\nprint(predict_features)\n\nIndex(['Intercept', 'carpool[T.yes]', 'C(seat)[T.7]', 'C(seat)[T.8]',\n       'C(cargo)[T.3ft]', 'C(eng)[T.gas]', 'C(eng)[T.hyb]', 'price'],\n      dtype='object')\n  Minivan  seat cargo   eng  price carpool\n0       A     7   2ft   hyb     30      no\n1       B     6   2ft   gas     30      no\n2       C     8   2ft   gas     30      no\n3       D     7   3ft   gas     40      no\n4       E     6   2ft  elec     40      no\n5       F     7   2ft   hyb     35      no\n   seat  price  Minivan_A  Minivan_B  Minivan_C  Minivan_D  Minivan_E  \\\n0     7     30       True      False      False      False      False   \n1     6     30      False       True      False      False      False   \n2     8     30      False      False       True      False      False   \n3     7     40      False      False      False       True      False   \n4     6     40      False      False      False      False       True   \n5     7     35      False      False      False      False      False   \n\n   Minivan_F  cargo_2ft  cargo_3ft  eng_elec  eng_gas  eng_hyb  carpool_no  \n0      False       True      False     False    False     True        True  \n1      False       True      False     False     True    False        True  \n2      False       True      False     False     True    False        True  \n3      False      False       True     False     True    False        True  \n4      False       True      False      True    False    False        True  \n5       True       True      False     False    False     True        True"
  }
]